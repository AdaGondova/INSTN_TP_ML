{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TP1_features_extraction_and_visualization.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"Oi31YQKqMK7F"},"source":["Alexandre CARRE*, Marvin LEROUSSEAU, Enzo BATTISTELLA <img align=\"right\" width=\"400\" height=\"40\" src=\"images/epu_ia_logo.png\"> <br> *alexandre.carre@gustaveroussy.fr (Notebook conception) <br> marvin.lerousseau@gustaveroussy.fr (Notebook revision) <br> enzo.battistella@gustaveroussy.fr (Notebook revision)"]},{"cell_type":"markdown","metadata":{"id":"O0Fl5ZhMMK7Q"},"source":["# TP1: FEATURES EXTRACTION-VISUALIZATION"]},{"cell_type":"markdown","metadata":{"id":"JqFCacMwMK7W"},"source":["In this notebook, you will learn how to extract radiomics features and visualize the important characteristics of the Dataset. First, download necessary materials for the afternoon practical sessions."]},{"cell_type":"code","metadata":{"id":"ujZ5cuPNMrah"},"source":["!git clone https://github.com/RRouhi/EPU-IA-2021.git\n","%cd EPU-IA-2021/"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ABi5RdLPMK7b"},"source":["### Installation of dependencies"]},{"cell_type":"code","metadata":{"id":"Irz6Seg-K4NI"},"source":["!pip install pyradiomics seaborn SimpleITK numpy pandas"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"dz0iWBtJMK78"},"source":["## Step 1: Radiomics features extraction"]},{"cell_type":"markdown","metadata":{"id":"DNo9no_VMK8C"},"source":["#### import radiomics toolbox"]},{"cell_type":"code","metadata":{"id":"tP6KYNTRMK8F"},"source":["from radiomics import featureextractor  # This module is used for interaction with pyradiomics"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4RnP8vV2MK8W"},"source":["### 1) Setting up data"]},{"cell_type":"markdown","metadata":{"id":"gmW-OCRfMK8Z"},"source":["We will load 2 patients from the BratS dataset (Images and segmentations of brain tumors). The radiomics package will be used to extract a set of features, and the \"signatures\" will be compared."]},{"cell_type":"markdown","metadata":{"id":"HdWvy1TRMK8f"},"source":["In the following, the data is returned in the sitk format. Two test cases will be loaded and and saved."]},{"cell_type":"code","metadata":{"id":"8AxvfCt_MK8l"},"source":["import os\n","import glob\n","import SimpleITK as sitk\n","\n","\n","def load_nii_volume_as_array(filename, with_header=False):\n","    \"\"\"\n","    load nifty image into numpy array [z,y,x] axis order\n","    The output array shape is like [Depth, Height, Width]\n","    :param filename: the input file name, should be *.nii or *.nii.gz\n","    :param with_header: return header information\n","    :return: a numpy data array or numpy data array with header if set to True\n","    \"\"\"\n","    img = sitk.ReadImage(filename)\n","    data = sitk.GetArrayFromImage(img)\n","\n","    if with_header:\n","        origin, spacing, direction = img.GetOrigin(), img.GetSpacing(), img.GetDirection()\n","        return data, (origin, spacing, direction)\n","    else:\n","        return data\n","\n","    \n","def set_header_information(array, header):\n","    \"\"\"\n","    Function to set header information to an array.\n","    :param array: array to set the header\n","    :param header: header information will be a tuple with (origin, spacing, direction)\n","    :return: an image in sitk format\n","    \"\"\"\n","    img = sitk.GetImageFromArray(array)\n","    img.SetOrigin(header[0])\n","    img.SetSpacing(header[1])\n","    img.SetDirection(header[2])\n","    return img\n","\n","\n","# function to process one subject data and return output path\n","def process_one_subject(src, count, image_type):\n","    \"\"\"\n","    Function to read one subject data from the general input directory specifying by the count number and image_type\n","    :param src: general input directory of all data\n","    :param count: patient to be read (same order as the glob function walk in the path)\n","    :param image_type: specify the image type to be read, can be 't1ce', 't1', 'flair', 't2', 'seg'\n","    :return: a dict for the 'seg' with key as type of seg and value as nD array. For image a nD array\n","    \"\"\"\n","    # add check range of count (depend of numbers of samples)\n","    files = glob.glob(os.path.join(src, '**/*{}.nii.gz'.format(image_type)), recursive=True)\n","    k = (len(files) - count) - 1\n","    file = files[k]\n","    print('Processing---', os.path.basename(file))\n","\n","    if any(x in image_type.lower() for x in ['t1ce', 't1', 'flair', 't2']):\n","        normalized = zscore_normalize(file)\n","        return normalized\n","\n","    if 'seg' in image_type.lower():\n","        label_full, label_nec, label_core, label_et = [], [], [], []\n","        for label_num in [1, 2, 4, 5]:\n","            img, header = load_nii_volume_as_array(file, with_header=True)\n","            if label_num == 5:\n","                img[img != 0] = 1  # Region 1 => 1+2+3+4 complete tumor\n","                label_full = set_header_information(img, header)\n","            if label_num == 1:\n","                img[img != 1] = 0  # only left necrosis\n","                label_nec = set_header_information(img, header)\n","            if label_num == 2:\n","                img[img == 2] = 0  # turn edema to 0\n","                img[img != 0] = 1  # only keep necrosis, ET, NET = Tumor core\n","                label_core = set_header_information(img, header)\n","            if label_num == 4:\n","                img[img != 4] = 0  # only left ET\n","                img[img == 4] = 1\n","                label_et = set_header_information(img, header)\n","        return {'label_full': label_full, 'label_nec': label_nec,\n","                'label_core': label_core, 'label_et': label_et}\n","\n","\n","def zscore_normalize(img_path, mask=None):\n","    \"\"\"\n","    Function to Z-Score normalize an image from an image filepath. It will normalize the target \n","    image by subtracting the mean of the whole brain and dividing by the standard deviation.\n","    :param img_path: target MR brain image path\n","    :param mask: brain mask path for img\n","    :return: Normalized image in sitk format\n","    \"\"\"\n","    img_data, header = load_nii_volume_as_array(img_path, with_header=True)\n","    if mask is not None and isinstance(mask, str):\n","        mask_data = load_nii_volume_as_array(mask, with_header=False)\n","    elif mask == 'nomask':\n","        mask_data = img_data == img_data\n","    else:\n","        mask_data = img_data > img_data.mean()\n","    logical_mask = mask_data == 1  # force the mask to be logical type\n","    mean = img_data[logical_mask].mean()\n","    std = img_data[logical_mask].std()\n","    normalized_data = (img_data - mean) / std\n","    normalized = set_header_information(normalized_data, header)\n","    return normalized"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"UL_MOd6KMK8v"},"source":["**Exercice:** Replace the expression '''CompleteHere''' by the correct one to: \n","- load images from two patients with all their respective segmentations.- apply normalization to MRI images.\n","- save the normalized images."]},{"cell_type":"code","metadata":{"id":"WL6Ab3xbLMQJ"},"source":["#@title Exercise 1 {display-mode: \"form\"}\n","# %load solutions_exercices/2patients.py\n","\n","src = './data/preprocessed/'\n","patient1_t1 = process_one_subject(src, count=0, image_type='t1')\n","patient1_t1_normalize_path = 'patient1_t1_normalize.nii.gz'\n","sitk.WriteImage(patient1_t1, patient1_t1_normalize_path)\n","\n","patient1_t1ce = process_one_subject(src, count=0, image_type='t1ce')\n","patient1_tce1_normalize_path = 'patient1_t1ce_normalize.nii.gz'\n","sitk.WriteImage('''CompleteHere''', patient1_tce1_normalize_path)\n","\n","patient1_t2 = process_one_subject(src, count=0, image_type='t2')\n","patient1_t2_normalize_path = 'patient1_t2_normalize.nii.gz'\n","sitk.WriteImage(patient1_t2, '''CompleteHere''')\n","\n","patient1_flair = process_one_subject(src, count=0, image_type='flair')\n","patient1_flair_normalize_path = 'patient1_flair_normalize.nii.gz'\n","sitk.WriteImage('''CompleteHere''', '''CompleteHere''')\n","\n","patient1_segmentations = '''CompleteHere'''(src, count=0, image_type='seg') # return a dict with {label_full, label_nec, label_core, label_et}\n","patient1_seg_label_full_path = 'patient1_seg_label_full.nii.gz'\n","sitk.WriteImage(patient1_segmentations['label_full'], '''CompleteHere''')\n","patient1_seg_label_nec_path = 'patient1_seg_label_nec.nii.gz'\n","sitk.WriteImage(patient1_segmentations['label_nec'], '''CompleteHere''')\n","patient1_seg_label_core_path = 'patient1_seg_label_core.nii.gz'\n","sitk.WriteImage(patient1_segmentations['label_core'], '''CompleteHere''')\n","patient1_seg_label_et_path = 'patient1_seg_label_et.nii.gz'\n","sitk.WriteImage(patient1_segmentations['label_et'], '''CompleteHere''')\n","\n","\n","patient2_t1 = '''CompleteHere'''(src, count=1, image_type='t1')\n","patient2_t1_normalize_path = 'patient2_t1_normalize.nii.gz'\n","sitk.WriteImage('''CompleteHere''', '''CompleteHere''')\n","\n","patient2_t1ce = process_one_subject(src, count=1, image_type='t1ce')\n","patient2_tce1_normalize_path = 'patient2_t1ce_normalize.nii.gz'\n","sitk.WriteImage('''CompleteHere''', '''CompleteHere''')\n","\n","patient2_t2 = process_one_subject(src, count=1, image_type='t2')\n","patient2_t2_normalize_path = 'patient2_t2_normalize.nii.gz'\n","sitk.WriteImage('''CompleteHere''', '''CompleteHere''')\n","\n","patient2_flair = process_one_subject(src, count=1, image_type='flair')\n","patient2_flair_normalize_path = 'patient2_flair_normalize.nii.gz'\n","sitk.WriteImage(patient2_flair, '''CompleteHere''')\n","\n","patient2_segmentations = process_one_subject(src, count=1, image_type='seg')\n","patient2_seg_label_full_path = 'patient2_seg_label_full.nii.gz'\n","sitk.WriteImage(patient2_segmentations['label_full'], '''CompleteHere''')\n","patient2_seg_label_nec_path = 'patient2_seg_label_nec.nii.gz'\n","sitk.WriteImage(patient2_segmentations['label_nec'], '''CompleteHere''')\n","patient2_seg_label_core_path = 'patient2_seg_label_core.nii.gz'\n","sitk.WriteImage(patient2_segmentations['label_core'], '''CompleteHere''')\n","patient2_seg_label_et_path = 'patient2_seg_label_et.nii.gz'\n","sitk.WriteImage(patient2_segmentations['label_et'], '''CompleteHere''')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"fG84YOAkMK8y"},"source":["\n","#@title Solution 1 {display-mode: \"form\"}\n","# %load solutions_exercices/2patients.py\n","\n","src = './data/preprocessed/'\n","patient1_t1 = process_one_subject(src, count=0, image_type='t1')\n","patient1_t1_normalize_path = 'patient1_t1_normalize.nii.gz'\n","sitk.WriteImage(patient1_t1, patient1_t1_normalize_path)\n","\n","patient1_t1ce = process_one_subject(src, count=0, image_type='t1ce')\n","patient1_tce1_normalize_path = 'patient1_t1ce_normalize.nii.gz'\n","sitk.WriteImage(patient1_t1ce, patient1_tce1_normalize_path)\n","\n","patient1_t2 = process_one_subject(src, count=0, image_type='t2')\n","patient1_t2_normalize_path = 'patient1_t2_normalize.nii.gz'\n","sitk.WriteImage(patient1_t2, patient1_t2_normalize_path)\n","\n","patient1_flair = process_one_subject(src, count=0, image_type='flair')\n","patient1_flair_normalize_path = 'patient1_flair_normalize.nii.gz'\n","sitk.WriteImage(patient1_flair, patient1_flair_normalize_path)\n","\n","patient1_segmentations = process_one_subject(src, count=0, image_type='seg') # return a dict with {label_full, label_nec, label_core, label_et}\n","patient1_seg_label_full_path = 'patient1_seg_label_full.nii.gz'\n","sitk.WriteImage(patient1_segmentations['label_full'], patient1_seg_label_full_path)\n","patient1_seg_label_nec_path = 'patient1_seg_label_nec.nii.gz'\n","sitk.WriteImage(patient1_segmentations['label_nec'], patient1_seg_label_nec_path)\n","patient1_seg_label_core_path = 'patient1_seg_label_core.nii.gz'\n","sitk.WriteImage(patient1_segmentations['label_core'], patient1_seg_label_core_path)\n","patient1_seg_label_et_path = 'patient1_seg_label_et.nii.gz'\n","sitk.WriteImage(patient1_segmentations['label_et'], patient1_seg_label_et_path)\n","\n","\n","patient2_t1 = process_one_subject(src, count=1, image_type='t1')\n","patient2_t1_normalize_path = 'patient2_t1_normalize.nii.gz'\n","sitk.WriteImage(patient2_t1, patient2_t1_normalize_path)\n","\n","patient2_t1ce = process_one_subject(src, count=1, image_type='t1ce')\n","patient2_tce1_normalize_path = 'patient2_t1ce_normalize.nii.gz'\n","sitk.WriteImage(patient2_t1ce, patient2_tce1_normalize_path)\n","\n","patient2_t2 = process_one_subject(src, count=1, image_type='t2')\n","patient2_t2_normalize_path = 'patient2_t2_normalize.nii.gz'\n","sitk.WriteImage(patient2_t2, patient2_t2_normalize_path)\n","\n","patient2_flair = process_one_subject(src, count=1, image_type='flair')\n","patient2_flair_normalize_path = 'patient2_flair_normalize.nii.gz'\n","sitk.WriteImage(patient2_flair, patient2_flair_normalize_path)\n","\n","patient2_segmentations = process_one_subject(src, count=1, image_type='seg')\n","patient2_seg_label_full_path = 'patient2_seg_label_full.nii.gz'\n","sitk.WriteImage(patient2_segmentations['label_full'], patient2_seg_label_full_path)\n","patient2_seg_label_nec_path = 'patient2_seg_label_nec.nii.gz'\n","sitk.WriteImage(patient2_segmentations['label_nec'], patient2_seg_label_nec_path)\n","patient2_seg_label_core_path = 'patient2_seg_label_core.nii.gz'\n","sitk.WriteImage(patient2_segmentations['label_core'], patient2_seg_label_core_path)\n","patient2_seg_label_et_path = 'patient2_seg_label_et.nii.gz'\n","sitk.WriteImage(patient2_segmentations['label_et'], patient2_seg_label_et_path)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gczujo3tMK86"},"source":["### 2) Start the extraction"]},{"cell_type":"markdown","metadata":{"id":"pBMYxxFAMK9B"},"source":["Now that we have our input, we need to define the parameters and instantiate the extractor."]},{"cell_type":"markdown","metadata":{"id":"FxIDADgvMK9J"},"source":["#### We use the default setting of the extractor."]},{"cell_type":"code","metadata":{"id":"qSIoiLJjMK9P"},"source":["\n","# Instantiate the extractor\n","extractor = featureextractor.RadiomicsFeatureExtractor()\n","\n","print('Extraction parameters:\\n\\t', extractor.settings)\n","print('Enabled filters:\\n\\t', extractor.enabledImagetypes)\n","print('Enabled features:\\n\\t', extractor.enabledFeatures)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Lm0n0CnmMK-w"},"source":["###  Extract features¶"]},{"cell_type":"markdown","metadata":{"id":"OqLL9oYeMK-1"},"source":["Now that we have our extractor set up with the correct parameters, we can start extracting features:"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"a8S3xxmEMK-6","executionInfo":{"elapsed":2735,"status":"ok","timestamp":1636136361500,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"},"user_tz":-60},"outputId":"9daf4eb7-8d51-4307-a98b-0d79988e7ad1"},"source":["imagePath = patient1_t1_normalize_path\n","maskPath = patient1_seg_label_full_path\n","\n","result = extractor.execute(imagePath, maskPath)\n","print(result)"],"execution_count":null,"outputs":[{"name":"stderr","output_type":"stream","text":["GLCM is symmetrical, therefore Sum Average = 2 * Joint Average, only 1 needs to be calculated\n"]},{"name":"stdout","output_type":"stream","text":["OrderedDict([('diagnostics_Versions_PyRadiomics', 'v3.0.1'), ('diagnostics_Versions_Numpy', '1.19.5'), ('diagnostics_Versions_SimpleITK', '2.1.1'), ('diagnostics_Versions_PyWavelet', '1.1.1'), ('diagnostics_Versions_Python', '3.7.12'), ('diagnostics_Configuration_Settings', {'minimumROIDimensions': 2, 'minimumROISize': None, 'normalize': False, 'normalizeScale': 1, 'removeOutliers': None, 'resampledPixelSpacing': None, 'interpolator': 'sitkBSpline', 'preCrop': False, 'padDistance': 5, 'distances': [1], 'force2D': False, 'force2Ddimension': 0, 'resegmentRange': None, 'label': 1, 'additionalInfo': True}), ('diagnostics_Configuration_EnabledImageTypes', {'Original': {}}), ('diagnostics_Image-original_Hash', '9b254c88bd7c5dce806b5c3b096441635b242bdc'), ('diagnostics_Image-original_Dimensionality', '3D'), ('diagnostics_Image-original_Spacing', (1.0, 1.0, 1.0)), ('diagnostics_Image-original_Size', (240, 240, 155)), ('diagnostics_Image-original_Mean', -3.2388073481968083), ('diagnostics_Image-original_Minimum', -3.9469088572376285), ('diagnostics_Image-original_Maximum', 10.879673454083735), ('diagnostics_Mask-original_Hash', '854e13e7b95928c8268a22517a7a8b3d1eb2d7c5'), ('diagnostics_Mask-original_Spacing', (1.0, 1.0, 1.0)), ('diagnostics_Mask-original_Size', (240, 240, 155)), ('diagnostics_Mask-original_BoundingBox', (134, 148, 40, 52, 68, 70)), ('diagnostics_Mask-original_VoxelNum', 30073), ('diagnostics_Mask-original_VolumeNum', 21), ('diagnostics_Mask-original_CenterOfMassIndex', (163.4712865360955, 182.95750340837296, 75.82732018754365)), ('diagnostics_Mask-original_CenterOfMass', (163.4712865360955, -56.04249659162704, 75.82732018754365)), ('original_shape_Elongation', 0.8088851113146096), ('original_shape_Flatness', 0.37795771530853056), ('original_shape_LeastAxisLength', 21.667608179505134), ('original_shape_MajorAxisLength', 57.32812772936161), ('original_shape_Maximum2DDiameterColumn', array(69.72087205)), ('original_shape_Maximum2DDiameterRow', array(65.12296062)), ('original_shape_Maximum2DDiameterSlice', array(79.64923101)), ('original_shape_Maximum3DDiameter', array(84.58132182)), ('original_shape_MeshVolume', array(29775.79166667)), ('original_shape_MinorAxisLength', 46.37186897982283), ('original_shape_Sphericity', array(0.40144207)), ('original_shape_SurfaceArea', array(11572.75572445)), ('original_shape_SurfaceVolumeRatio', array(0.38866324)), ('original_shape_VoxelVolume', 30073.0), ('original_firstorder_10Percentile', array(-0.00712028)), ('original_firstorder_90Percentile', array(1.25341297)), ('original_firstorder_Energy', array(33031.50618197)), ('original_firstorder_Entropy', array(0.47295691)), ('original_firstorder_InterquartileRange', array(0.63619418)), ('original_firstorder_Kurtosis', array(17.6452626)), ('original_firstorder_Maximum', array(1.99512669)), ('original_firstorder_MeanAbsoluteDeviation', array(0.50723195)), ('original_firstorder_Mean', array(0.53070123)), ('original_firstorder_Median', array(0.66753514)), ('original_firstorder_Minimum', array(-3.90211282)), ('original_firstorder_Range', array(5.89723952)), ('original_firstorder_RobustMeanAbsoluteDeviation', array(0.26389262)), ('original_firstorder_RootMeanSquared', array(1.04803506)), ('original_firstorder_Skewness', array(-3.48108196)), ('original_firstorder_TotalEnergy', array(33031.50618197)), ('original_firstorder_Uniformity', array(0.81799736)), ('original_firstorder_Variance', array(0.8167337)), ('original_glcm_Autocorrelation', array(3.72548187)), ('original_glcm_ClusterProminence', array(0.62439552)), ('original_glcm_ClusterShade', array(-0.35467558)), ('original_glcm_ClusterTendency', array(0.24159497)), ('original_glcm_Contrast', array(0.05855857)), ('original_glcm_Correlation', array(0.60625393)), ('original_glcm_DifferenceAverage', array(0.05855857)), ('original_glcm_DifferenceEntropy', array(0.32015172)), ('original_glcm_DifferenceVariance', array(0.05501543)), ('original_glcm_Id', array(0.97072071)), ('original_glcm_Idm', array(0.97072071)), ('original_glcm_Idmn', array(0.98828829)), ('original_glcm_Idn', array(0.98048048)), ('original_glcm_Imc1', array(-0.35606981)), ('original_glcm_Imc2', array(0.49748703)), ('original_glcm_InverseVariance', array(0.05855857)), ('original_glcm_JointAverage', array(1.91825372)), ('original_glcm_JointEnergy', array(0.7949078)), ('original_glcm_JointEntropy', array(0.66969096)), ('original_glcm_MCC', array(0.60625393)), ('original_glcm_MaximumProbability', array(0.88897443)), ('original_glcm_SumAverage', array(3.83650744)), ('original_glcm_SumEntropy', array(0.61113239)), ('original_glcm_SumSquares', array(0.07503839)), ('original_gldm_DependenceEntropy', array(3.58569832)), ('original_gldm_DependenceNonUniformity', array(6907.15615336)), ('original_gldm_DependenceNonUniformityNormalized', array(0.22967965)), ('original_gldm_DependenceVariance', array(34.77514004)), ('original_gldm_GrayLevelNonUniformity', array(24599.63452266)), ('original_gldm_GrayLevelVariance', array(0.09100132)), ('original_gldm_HighGrayLevelEmphasis', array(3.69623915)), ('original_gldm_LargeDependenceEmphasis', array(538.4404948)), ('original_gldm_LargeDependenceHighGrayLevelEmphasis', array(2095.47750474)), ('original_gldm_LargeDependenceLowGrayLevelEmphasis', array(149.18124231)), ('original_gldm_LowGrayLevelEmphasis', array(0.32594021)), ('original_gldm_SmallDependenceEmphasis', array(0.00404579)), ('original_gldm_SmallDependenceHighGrayLevelEmphasis', array(0.01126746)), ('original_gldm_SmallDependenceLowGrayLevelEmphasis', array(0.00224037)), ('original_glrlm_GrayLevelNonUniformity', array(3000.81588306)), ('original_glrlm_GrayLevelNonUniformityNormalized', array(0.56843411)), ('original_glrlm_GrayLevelVariance', array(0.21578295)), ('original_glrlm_HighGrayLevelRunEmphasis', array(3.0541676)), ('original_glrlm_LongRunEmphasis', array(79.8779344)), ('original_glrlm_LongRunHighGrayLevelEmphasis', array(314.33951917)), ('original_glrlm_LongRunLowGrayLevelEmphasis', array(21.2625382)), ('original_glrlm_LowGrayLevelRunEmphasis', array(0.4864581)), ('original_glrlm_RunEntropy', array(4.3242074)), ('original_glrlm_RunLengthNonUniformity', array(695.44606867)), ('original_glrlm_RunLengthNonUniformityNormalized', array(0.12764085)), ('original_glrlm_RunPercentage', array(0.17528885)), ('original_glrlm_RunVariance', array(42.9107552)), ('original_glrlm_ShortRunEmphasis', array(0.32741217)), ('original_glrlm_ShortRunHighGrayLevelEmphasis', array(0.74376469)), ('original_glrlm_ShortRunLowGrayLevelEmphasis', array(0.22332404)), ('original_glszm_GrayLevelNonUniformity', array(34.89473684)), ('original_glszm_GrayLevelNonUniformityNormalized', array(0.61218837)), ('original_glszm_GrayLevelVariance', array(0.19390582)), ('original_glszm_HighGrayLevelZoneEmphasis', array(1.78947368)), ('original_glszm_LargeAreaEmphasis', array(12713757.94736842)), ('original_glszm_LargeAreaHighGrayLevelEmphasis', array(50702562.36842106)), ('original_glszm_LargeAreaLowGrayLevelEmphasis', array(3216556.84210526)), ('original_glszm_LowGrayLevelZoneEmphasis', array(0.80263158)), ('original_glszm_SizeZoneNonUniformity', array(5.38596491)), ('original_glszm_SizeZoneNonUniformityNormalized', array(0.09449061)), ('original_glszm_SmallAreaEmphasis', array(0.29051882)), ('original_glszm_SmallAreaHighGrayLevelEmphasis', array(0.58761696)), ('original_glszm_SmallAreaLowGrayLevelEmphasis', array(0.21624428)), ('original_glszm_ZoneEntropy', array(4.65291842)), ('original_glszm_ZonePercentage', array(0.00189539)), ('original_glszm_ZoneVariance', array(12435399.88981225)), ('original_ngtdm_Busyness', array(265.95651029)), ('original_ngtdm_Coarseness', array(0.00110834)), ('original_ngtdm_Complexity', array(0.0600041)), ('original_ngtdm_Contrast', array(0.00561115)), ('original_ngtdm_Strength', array(0.00107857))])\n"]}]},{"cell_type":"markdown","metadata":{"id":"_KWgzJT3MK_M"},"source":["As you may have already seen, Pyradiomics returns an ordered dict that is visually not very pleasant. We can then turn this result into a pandas format. The keys in the dictionary will be used as the index (labels for the rows), with the values of the features as the values in the rows."]},{"cell_type":"code","metadata":{"id":"82AK29bsMK_N"},"source":["import pandas as pd \n","\n","df = pd.Series(result)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"Uo5V6ZETMK_V"},"source":["pd.set_option('display.max_rows', None)\n","print(df)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"HPo7haYZMK_e"},"source":["Now we will use a loop to extract a set of features for our two patients with all their segmentations"]},{"cell_type":"code","metadata":{"id":"0T4T84RJMK_g"},"source":["# The idea is to loop over image and associated segmentations\n","# We choose to create a dataframe who associated both\n","\n","from IPython.display import display\n","import pandas as pd\n","\n","patient1_segmentations_path = [patient1_seg_label_full_path, patient1_seg_label_nec_path, patient1_seg_label_core_path, patient1_seg_label_et_path]\n","patient2_segmentations_path = [patient2_seg_label_full_path, patient2_seg_label_nec_path, patient2_seg_label_core_path, patient2_seg_label_et_path]\n","patient1_images_path = [patient1_t1_normalize_path, patient1_tce1_normalize_path, patient1_t2_normalize_path, patient1_flair_normalize_path]\n","patient2_images_path = [patient2_t1_normalize_path, patient2_tce1_normalize_path, patient2_t2_normalize_path, patient2_flair_normalize_path]\n","\n","# For a segmentation, we have 4 MRI sequences\n","\n","segmentations  = [item for item in patient1_segmentations_path for i in range(4)] + [item for item in patient2_segmentations_path for i in range(4)]\n","images = patient1_images_path * 4 + patient2_images_path * 4\n","\n","# Now we construct a DataFrame from our two list\n","data = {'Image':  images,\n","        'Segmentation': segmentations,\n","        }\n","\n","database_df = pd.DataFrame (data, columns = data.keys())\n","display(database_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"hLEMvYXnMK_s"},"source":["# We construct the loop to extract features from our Database generated DataFrame\n","from radiomics import featureextractor\n","\n","paramPath = os.path.join('exampleSettings', 'Params.yaml')\n","extractor = featureextractor.RadiomicsFeatureExtractor(paramPath)\n","\n","results_extraction = pd.DataFrame()\n","for row_idx, row in database_df.iterrows():\n","    print('process:'  '   ----   ' 'Image: ', row['Image'], ' Segmentation: ', row['Segmentation'])\n","    result_extraction = pd.Series(extractor.execute(row['Image'], row['Segmentation']))\n","    feature_vector = (pd.Series(row)).append(result_extraction)\n","    feature_vector.name = row_idx\n","    results_extraction = results_extraction.join(feature_vector, how='outer')\n","    \n","# Transpose DataFrame to have one row by analysis\n","results_extraction = results_extraction.T"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"ynx9K8h8MK_1"},"source":["# One row per image/segmentation, features ares columns\n","from IPython.display import display\n","\n","display(results_extraction)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ORQ0hB4SMK__"},"source":["Now, you have an example of the extracted features corresponding with the two patients and their respective modalities and also segmentations."]},{"cell_type":"markdown","metadata":{"id":"3VuBHlZ7MLAB"},"source":["### Signatures comparison"]},{"cell_type":"markdown","metadata":{"id":"azuQJXfAMLAH"},"source":["Now, let's compare the signatures for our two patients..."]},{"cell_type":"markdown","metadata":{"id":"n0KYRhhyMLAJ"},"source":["#### Prepare for plotting"]},{"cell_type":"markdown","metadata":{"id":"RB2uqMMRMLAK"},"source":["Because we'd like to plot the feature vectors, create numpy arrays for features starting with original_ (excluding meta-features)."]},{"cell_type":"code","metadata":{"id":"0H6MofSDLTZo"},"source":["# Adding some cleaning in the generated dataframe\n","patient = results_extraction['Image'].str.split('_').str[0]\n","sequence = results_extraction['Image'].str.split('_').str[1]\n","segtype = results_extraction['Segmentation'].str.split('_').str[-1].str.split('.').str[0]\n","features_columns = [col for col in results_extraction if col.startswith('original')]\n","dataframe_cleaned = results_extraction[features_columns] \n","dataframe_cleaned.insert(0, 'patient', patient)\n","dataframe_cleaned.insert(1, 'sequence', sequence)\n","dataframe_cleaned.insert(2, 'segmentation', segtype)\n","\n","display(dataframe_cleaned)\n","\n","# Now you can easily select a line of set of features for a patient sequence for a specific segmentations: ie\n","\n","features_p1_t1_full =  dataframe_cleaned.iloc[0, 3:].to_numpy()\n","features_p2_t1_full =  dataframe_cleaned.iloc[16, 3:].to_numpy()\n","\n","\n","# Help iloc[row_idx, columns_idx]"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Tehgz6HpMLAT"},"source":["**Exercice:** Complete the folowing script to:\n","- plot the two feature vectors and the their difference.\n","\n","Notes: \n","- Feature values have a wide range of magnitudes and will be plotted on a log scale. You can have fun and change the input modality or the segmentation type. \n","- plot will be composed of three subplot, first is features from patient1 from a modality_type1 for segtype1. Second is features from patient2 from a modality_type2 for segtype2. Third subplot is the difference between the two features vectors."]},{"cell_type":"code","metadata":{"id":"P_jOGAHfMLAV"},"source":["#@title Exercise 2 {display-mode: \"form\"}\n","# %load solutions_exercices/plot_features_vector.py\n","\n","\n","import matplotlib.pyplot as plt\n","\n","fig, axes = plt.subplots(2, 1, sharex=True, squeeze=True, figsize=(20, 10))\n","\n","axes[0].plot('''CompleteHere''', label='features from patient 1 for the full segmentation in T1 modality', color='red')\n","axes[0].'''CompleteHere'''(features_p2_t1_full, label='features from patient 2 for the full segmentation in T1 modality', color='green')\n","axes[0].set_yscale('symlog')\n","axes[0].set_ylabel('symlog')\n","axes[0].set_xlabel('feature id')\n","axes[0].set_xticklabels(features_columns, rotation='vertical')\n","axes[0].set_title(\"Features from Patient 1 for the full segmentation in T1 modality\")\n","\n","axes[1].plot('''CompleteHere''' - features_p2_t1_full)\n","axes[1].set_yscale('symlog')\n","axes[1].set_ylabel('symlog')\n","axes[1].set_xlabel('feature id')\n","axes[1].set_xticks(range(len(features_p1_t1_full)))\n","axes[1].set_xticklabels('''CompleteHere''', rotation=90)\n","axes[1].'''CompleteHere'''(\"Difference\")\n","plt.subplots_adjust(wspace=0, hspace=0)\n","plt.show();\n","\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iukwQbsCMLAc"},"source":["#@title Solution 2 {display-mode: \"form\"}\n","# %load solutions_exercices/plot_features_vector.py\n","\n","\n","import matplotlib.pyplot as plt\n","\n","fig, axes = plt.subplots(2, 1, sharex=True, squeeze=True, figsize=(20, 10))\n","\n","axes[0].plot(features_p1_t1_full, label='features from patient 1 for the full segmentation in T1 modality', color='red')\n","axes[0].plot(features_p2_t1_full, label='features from patient 2 for the full segmentation in T1 modality', color='green')\n","axes[0].set_yscale('symlog')\n","axes[0].set_ylabel('symlog')\n","axes[0].set_xlabel('feature id')\n","axes[0].set_xticklabels(features_columns, rotation='vertical')\n","axes[0].set_title(\"Features from Patient 1 for the full segmentation in T1 modality\")\n","\n","axes[1].plot(features_p1_t1_full - features_p2_t1_full)\n","axes[1].set_yscale('symlog')\n","axes[1].set_ylabel('symlog')\n","axes[1].set_xlabel('feature id')\n","axes[1].set_xticks(range(len(features_p1_t1_full)))\n","axes[1].set_xticklabels(features_columns, rotation=90)\n","axes[1].set_title(\"Difference\")\n","plt.subplots_adjust(wspace=0, hspace=0)\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"LHTfHZ_8MLAo"},"source":["## Step 2: Exploring and Visualizing Data"]},{"cell_type":"markdown","metadata":{"id":"p2kRLi2qMLAr"},"source":["The first step in machine learning is to understand your data, so that you can identify a good target to learn and choose an appropriate learning technique among many possible families of candidates.\n","\n","The previous work of this notebook has been applied on up to two patients, with already significant time to process all of the data into features. Since our time in this practical session is limited, the features on the entire BraTS dataset has been extracted beforehand and saved into a csv file which should be available locally in _./data/radiomics_analysis_cleaned.csv_. This file contains one row per patient per input modality (T1, T1ce, T2, flair) per type of tumor ROI (4*4=16 rows per patient).\n","\n","Let's explore the data !"]},{"cell_type":"markdown","metadata":{"id":"KD2nQxUSMLAv"},"source":["### Load datas"]},{"cell_type":"code","metadata":{"colab":{"background_save":true},"id":"v_l2SfxwMLAx"},"source":["import pandas as pd\n","from IPython.display import display\n","\n","path_dataset = './data/radiomics_analysis_cleaned.csv'\n","\n","data = pd.read_csv(path_dataset)\n","display(data)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"pUaUlXE7MLA3"},"source":["We can generate a full dataframe which representes all the features for a patient "]},{"cell_type":"code","metadata":{"id":"vljErfNlLjvG"},"source":["full_features_df = data.pivot_table(index=['patient', 'label'],\n","                                    columns=['sequence', 'segmentation'],\n","                                    values=data.columns[4:])\n","full_features_df.columns = ['_'.join(col).strip() for col in full_features_df.columns.values]\n","full_features_df.reset_index(level=1, inplace=True)\n","display(full_features_df)"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"HV6f83jVMLBC"},"source":["print('repartition by class:\\n',  full_features_df['label'].value_counts())\n","print('numbers of total features: ', len(full_features_df.columns) -1)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rPvaNgEtMLBJ"},"source":["### Pairwise Join Distributions"]},{"cell_type":"markdown","metadata":{"id":"qQGY7An_MLBM"},"source":["We can see the join distribution of any pair of columns/attributes/variables/features by using the pairplot function offered by Seaborn, which is based on Matplotlib. We're only gonna plot the first 5 features."]},{"cell_type":"code","metadata":{"id":"sYJ61n3WMLBO"},"source":["import matplotlib.pyplot as plt\n","import seaborn as sns\n","\n","# filter one sequence type and a segmentation type (you can change the sequence type and segmentation type)\n","d = data[(data['sequence']=='t1ce')  & (data['segmentation']=='full')].iloc[:, 3:9] \n","\n","sns.pairplot(d, hue=\"label\", height=2.5, )\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"44IbRa5jZBUH"},"source":["#### Interpretation\n","When observing the produced plots, it can be seen that some pairs of features seem te bring high value towards discrimintating between LGG (in orange) and HGG (in blue). For instance, the plot at the second line and first column between the features 10Percentile (10th percentile of image intensities) and 90Percentile (idem with 90th) seem to cluster both cancer subtypes into fuzzy clusters. Multiple lines could be drawn on the graph and would yield performance which is expected to outperform randomness. In these 2D plots, a line is equivalent to a hyperplane (i.e. a plane whose dimension is one less than the dimension of the vectorial space); in other words, any line draw on a 2D plot between two features can be seen as a linear function solution of the classification problem. A decision system can be extracted from a line itself obtained by interrogating the data such as in this plot, and would essentially classify an MRI based on a hyperplance of the projections of the images into a two-dimensional space of features (the space {10Percentile, 90Percentile}). Multi-dimensional machine learning such as linear functions on high dimensional feature spaces with more than 2 features essentially perform the same method, by trying to fit a hyperplance based on a training set such that the classes are separated as much as possible. It is notworthy that rigourous machine learning should involve a hold-out testing set such as to challenge any extracted rule, would it be a line between a space of 2 vectors or any parametrized algorithm such as artificial neural networks, on data samples which have not been used to extract such decision system."]},{"cell_type":"markdown","metadata":{"id":"8e2cFiE-MLBU"},"source":["### Plot features as a heatmap¶"]},{"cell_type":"markdown","metadata":{"id":"hrrqnTRYMLBW"},"source":["Showing the pairwise join distributions may still be mind-boggling when we have a lot of variables in the dataset. Sometimes, we can just plot the correlation matrix to quantify the linear relationship between variables.\n","\n","A heat map gives the correlation of features to each other, red indicating positive correlation, and blue negative one. A heatmap may be clustered to show groupings of features that are similar."]},{"cell_type":"code","metadata":{"id":"1xEdc4yjMLBY"},"source":["import pandas as pd\n","import seaborn as sns\n","\n","# filter one sequence type and a segmentation type (you can change the sequence type and segmentation type)\n","d = data[(data['sequence']=='t1ce')  & (data['segmentation']=='full')].iloc[:, 3:] # the full feature one is too big\n","\n","# create the correlation matrix\n","corr = d.corr()\n","\n","# Set up the matplotlib figure, make it big!\n","f, ax = plt.subplots(figsize=(15, 10))\n","\n","# Draw the heatmap using seaborn\n","sns.heatmap(corr, vmax=.8, square=True)\n","plt.show();"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"NcqK1vLyMLBg"},"source":["### Cluster the heatmap"]},{"cell_type":"markdown","metadata":{"id":"vd-i5DqtMLBk"},"source":["Though useful, heatmaps tell a much better story if the features are clustered. Here we will take a smaller subset of the features and cluster them. In this dendrogram, there are 2 major groups, and many smaller groupings."]},{"cell_type":"code","metadata":{"id":"EC3ygNEfL4To"},"source":["# Choose a subset of features for clustering\n","dd = d.iloc[:,3:50]\n","\n","pp = sns.clustermap(dd.corr(), linewidths=.5, figsize=(13,13))\n","_ = plt.setp(pp.ax_heatmap.get_yticklabels(), rotation=0)\n","\n","plt.show();"],"execution_count":null,"outputs":[]}]}