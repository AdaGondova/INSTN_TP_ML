{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"TP2_machine_learning_application.ipynb","provenance":[],"collapsed_sections":[],"toc_visible":true},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.7.4"}},"cells":[{"cell_type":"markdown","metadata":{"id":"8tVohxl8SM51"},"source":["Alexandre CARRE*, Marvin LEROUSSEAU, Enzo BATTISTELLA <img align=\"right\" width=\"400\" height=\"40\" src=\"images/epu_ia_logo.png\"> <br> *alexandre.carre@gustaveroussy.fr (Notebook conception) <br> marvin.lerousseau@gustaveroussy.fr (Notebook revision) <br> enzo.battistella@gustaveroussy.fr (Notebook revision)"]},{"cell_type":"markdown","metadata":{"id":"2npMmVzzSM5-"},"source":["# TP2: MACHINE LEARNING APPLICATION"]},{"cell_type":"markdown","metadata":{"id":"T7-DWgNWSM6F"},"source":["In this notebook, you will learn the general fundamentals of machine learning applied to a classification task which is LGG vs GBM. Of course, the aim is not to find the best pipeline with the best algorithm for this task but to understand the principle concept. First, download necessary materials for the afternoon practical sessions."]},{"cell_type":"code","metadata":{"id":"5Qc9iPlKTFG1","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636137194642,"user_tz":-60,"elapsed":26716,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"fe64385e-688f-4d96-a452-557a8c6884ab"},"source":["#!git clone https://github.com/RRouhi/EPU-IA-2021.git\n","#%cd EPU-IA-2021/\n","from google.colab import drive\n","drive.mount('/content/drive')\n","%cd /content/drive/My Drive/Colab Notebooks/Master_Class/ML/"],"execution_count":1,"outputs":[{"output_type":"stream","name":"stdout","text":["Mounted at /content/drive\n","/content/drive/My Drive/Colab Notebooks/Master_Class/ML\n"]}]},{"cell_type":"markdown","metadata":{"id":"9Xr5AtWvSM6L"},"source":["### Installation of python dependencies"]},{"cell_type":"code","metadata":{"id":"MDhXnJMmSM6P","scrolled":true,"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636137206143,"user_tz":-60,"elapsed":4040,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"aa761d7c-a6b5-4300-e6c1-dcc22e1adde2"},"source":["!pip install sklearn graphviz pydotplus pandas matplotlib numpy"],"execution_count":2,"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: sklearn in /usr/local/lib/python3.7/dist-packages (0.0)\n","Requirement already satisfied: graphviz in /usr/local/lib/python3.7/dist-packages (0.10.1)\n","Requirement already satisfied: pydotplus in /usr/local/lib/python3.7/dist-packages (2.0.2)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.7/dist-packages (1.1.5)\n","Requirement already satisfied: matplotlib in /usr/local/lib/python3.7/dist-packages (3.2.2)\n","Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (1.19.5)\n","Requirement already satisfied: scikit-learn in /usr/local/lib/python3.7/dist-packages (from sklearn) (0.22.2.post1)\n","Requirement already satisfied: pyparsing>=2.0.1 in /usr/local/lib/python3.7/dist-packages (from pydotplus) (2.4.7)\n","Requirement already satisfied: python-dateutil>=2.7.3 in /usr/local/lib/python3.7/dist-packages (from pandas) (2.8.2)\n","Requirement already satisfied: pytz>=2017.2 in /usr/local/lib/python3.7/dist-packages (from pandas) (2018.9)\n","Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.7/dist-packages (from python-dateutil>=2.7.3->pandas) (1.15.0)\n","Requirement already satisfied: kiwisolver>=1.0.1 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (1.3.2)\n","Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.7/dist-packages (from matplotlib) (0.10.0)\n","Requirement already satisfied: joblib>=0.11 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.0.1)\n","Requirement already satisfied: scipy>=0.17.0 in /usr/local/lib/python3.7/dist-packages (from scikit-learn->sklearn) (1.4.1)\n"]}]},{"cell_type":"markdown","metadata":{"id":"jfe1L4HDSM6j"},"source":["\n","## General Machine Learning Steps\n","\n","Before we start, let's review the general machine learning steps [1]:\n","\n","1. Data collection, preprocessing (e.g., integration, cleaning, etc.), and exploration;\n","   - Split a dataset into the **training** and **testing** datasets\n","2. Model development:  \n","    A. Assume a model $\\{f\\}$ that is a collection of candidate functions $f$’s (representing posteriori knowledge) we want to discover. Let's assume that each $f$ is parametrized by ${w}$  \n","    B.Define a **cost function** $C({w})$ that measures \"how good a particular $f$ can explain the training data.\" The lower the cost function the better;  \n","3. **Training:** employ an algorithm that finds the best (or good enough) function $f^{*}$ in the model that minimizes the cost function over the training dataset\n","4. **Testing**: evaluate the performance of the learned $f^{*}$ using the testing dataset;\n","5. Apply the model in the real world."]},{"cell_type":"markdown","metadata":{"id":"mxO1e7MiSM6o"},"source":["## Load data"]},{"cell_type":"code","metadata":{"colab":{"base_uri":"https://localhost:8080/","height":686},"id":"1SS9HJy5SM6r","executionInfo":{"status":"ok","timestamp":1636137316460,"user_tz":-60,"elapsed":1646,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"8431bae8-4643-4b0c-b392-20df02b9490e"},"source":["import pandas as pd\n","from IPython.display import display\n","\n","path_dataset = './data/radiomics_analysis_cleaned.csv'\n","\n","data = pd.read_csv(path_dataset)\n","# we will only work with the full area segmentation with all sequences\n","data = data[data['segmentation']=='full']\n","\n","data = data.pivot_table(index=['patient', 'label'],\n","                                columns=['sequence', 'segmentation'],\n","                                values=data.columns[4:])\n","data.columns = ['_'.join(col).strip() for col in data.columns.values]\n","data.reset_index(level=1, inplace=True)\n","\n","display(data)\n","\n","# Convert LGG into class 0 and HGG into class 1\n","data.loc[data['label'] == 'HGG', 'label'] = 1\n","data.loc[data['label'] == 'LGG', 'label'] = 0"],"execution_count":3,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>label</th>\n","      <th>original_firstorder_10Percentile_flair_full</th>\n","      <th>original_firstorder_10Percentile_t1_full</th>\n","      <th>original_firstorder_10Percentile_t1ce_full</th>\n","      <th>original_firstorder_10Percentile_t2_full</th>\n","      <th>original_firstorder_90Percentile_flair_full</th>\n","      <th>original_firstorder_90Percentile_t1_full</th>\n","      <th>original_firstorder_90Percentile_t1ce_full</th>\n","      <th>original_firstorder_90Percentile_t2_full</th>\n","      <th>original_firstorder_Energy_flair_full</th>\n","      <th>original_firstorder_Energy_t1_full</th>\n","      <th>original_firstorder_Energy_t1ce_full</th>\n","      <th>original_firstorder_Energy_t2_full</th>\n","      <th>original_firstorder_Entropy_flair_full</th>\n","      <th>original_firstorder_Entropy_t1_full</th>\n","      <th>original_firstorder_Entropy_t1ce_full</th>\n","      <th>original_firstorder_Entropy_t2_full</th>\n","      <th>original_firstorder_InterquartileRange_flair_full</th>\n","      <th>original_firstorder_InterquartileRange_t1_full</th>\n","      <th>original_firstorder_InterquartileRange_t1ce_full</th>\n","      <th>original_firstorder_InterquartileRange_t2_full</th>\n","      <th>original_firstorder_Kurtosis_flair_full</th>\n","      <th>original_firstorder_Kurtosis_t1_full</th>\n","      <th>original_firstorder_Kurtosis_t1ce_full</th>\n","      <th>original_firstorder_Kurtosis_t2_full</th>\n","      <th>original_firstorder_Maximum_flair_full</th>\n","      <th>original_firstorder_Maximum_t1_full</th>\n","      <th>original_firstorder_Maximum_t1ce_full</th>\n","      <th>original_firstorder_Maximum_t2_full</th>\n","      <th>original_firstorder_Mean_flair_full</th>\n","      <th>original_firstorder_Mean_t1_full</th>\n","      <th>original_firstorder_Mean_t1ce_full</th>\n","      <th>original_firstorder_Mean_t2_full</th>\n","      <th>original_firstorder_MeanAbsoluteDeviation_flair_full</th>\n","      <th>original_firstorder_MeanAbsoluteDeviation_t1_full</th>\n","      <th>original_firstorder_MeanAbsoluteDeviation_t1ce_full</th>\n","      <th>original_firstorder_MeanAbsoluteDeviation_t2_full</th>\n","      <th>original_firstorder_Median_flair_full</th>\n","      <th>original_firstorder_Median_t1_full</th>\n","      <th>original_firstorder_Median_t1ce_full</th>\n","      <th>...</th>\n","      <th>original_shape_Maximum2DDiameterColumn_flair_full</th>\n","      <th>original_shape_Maximum2DDiameterColumn_t1_full</th>\n","      <th>original_shape_Maximum2DDiameterColumn_t1ce_full</th>\n","      <th>original_shape_Maximum2DDiameterColumn_t2_full</th>\n","      <th>original_shape_Maximum2DDiameterRow_flair_full</th>\n","      <th>original_shape_Maximum2DDiameterRow_t1_full</th>\n","      <th>original_shape_Maximum2DDiameterRow_t1ce_full</th>\n","      <th>original_shape_Maximum2DDiameterRow_t2_full</th>\n","      <th>original_shape_Maximum2DDiameterSlice_flair_full</th>\n","      <th>original_shape_Maximum2DDiameterSlice_t1_full</th>\n","      <th>original_shape_Maximum2DDiameterSlice_t1ce_full</th>\n","      <th>original_shape_Maximum2DDiameterSlice_t2_full</th>\n","      <th>original_shape_Maximum3DDiameter_flair_full</th>\n","      <th>original_shape_Maximum3DDiameter_t1_full</th>\n","      <th>original_shape_Maximum3DDiameter_t1ce_full</th>\n","      <th>original_shape_Maximum3DDiameter_t2_full</th>\n","      <th>original_shape_MeshVolume_flair_full</th>\n","      <th>original_shape_MeshVolume_t1_full</th>\n","      <th>original_shape_MeshVolume_t1ce_full</th>\n","      <th>original_shape_MeshVolume_t2_full</th>\n","      <th>original_shape_MinorAxisLength_flair_full</th>\n","      <th>original_shape_MinorAxisLength_t1_full</th>\n","      <th>original_shape_MinorAxisLength_t1ce_full</th>\n","      <th>original_shape_MinorAxisLength_t2_full</th>\n","      <th>original_shape_Sphericity_flair_full</th>\n","      <th>original_shape_Sphericity_t1_full</th>\n","      <th>original_shape_Sphericity_t1ce_full</th>\n","      <th>original_shape_Sphericity_t2_full</th>\n","      <th>original_shape_SurfaceArea_flair_full</th>\n","      <th>original_shape_SurfaceArea_t1_full</th>\n","      <th>original_shape_SurfaceArea_t1ce_full</th>\n","      <th>original_shape_SurfaceArea_t2_full</th>\n","      <th>original_shape_SurfaceVolumeRatio_flair_full</th>\n","      <th>original_shape_SurfaceVolumeRatio_t1_full</th>\n","      <th>original_shape_SurfaceVolumeRatio_t1ce_full</th>\n","      <th>original_shape_SurfaceVolumeRatio_t2_full</th>\n","      <th>original_shape_VoxelVolume_flair_full</th>\n","      <th>original_shape_VoxelVolume_t1_full</th>\n","      <th>original_shape_VoxelVolume_t1ce_full</th>\n","      <th>original_shape_VoxelVolume_t2_full</th>\n","    </tr>\n","    <tr>\n","      <th>patient</th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","      <th></th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>TCGA-02-0003</th>\n","      <td>HGG</td>\n","      <td>144.118668</td>\n","      <td>-37.915257</td>\n","      <td>3.103941</td>\n","      <td>46.780552</td>\n","      <td>373.385712</td>\n","      <td>95.536186</td>\n","      <td>288.818451</td>\n","      <td>216.258804</td>\n","      <td>2.470281e+10</td>\n","      <td>9.386010e+09</td>\n","      <td>1.464221e+10</td>\n","      <td>1.491739e+10</td>\n","      <td>4.211199</td>\n","      <td>3.586215</td>\n","      <td>4.241624</td>\n","      <td>3.779976</td>\n","      <td>116.424683</td>\n","      <td>71.174101</td>\n","      <td>101.978096</td>\n","      <td>89.773132</td>\n","      <td>9.437376</td>\n","      <td>11.850495</td>\n","      <td>4.528018</td>\n","      <td>10.823671</td>\n","      <td>1057.604614</td>\n","      <td>628.353394</td>\n","      <td>693.214355</td>\n","      <td>780.906616</td>\n","      <td>248.501100</td>\n","      <td>36.910727</td>\n","      <td>115.976699</td>\n","      <td>126.598371</td>\n","      <td>78.671283</td>\n","      <td>48.827477</td>\n","      <td>82.359673</td>\n","      <td>59.059723</td>\n","      <td>226.511520</td>\n","      <td>30.293257</td>\n","      <td>89.257851</td>\n","      <td>...</td>\n","      <td>76.537572</td>\n","      <td>76.537572</td>\n","      <td>76.537572</td>\n","      <td>76.537572</td>\n","      <td>116.709040</td>\n","      <td>116.709040</td>\n","      <td>116.709040</td>\n","      <td>116.709040</td>\n","      <td>104.235311</td>\n","      <td>104.235311</td>\n","      <td>104.235311</td>\n","      <td>104.235311</td>\n","      <td>120.718681</td>\n","      <td>120.718681</td>\n","      <td>120.718681</td>\n","      <td>120.718681</td>\n","      <td>78659.125000</td>\n","      <td>78659.125000</td>\n","      <td>78659.125000</td>\n","      <td>78659.125000</td>\n","      <td>52.041219</td>\n","      <td>52.041219</td>\n","      <td>52.041219</td>\n","      <td>52.041219</td>\n","      <td>0.478543</td>\n","      <td>0.478543</td>\n","      <td>0.478543</td>\n","      <td>0.478543</td>\n","      <td>18552.213339</td>\n","      <td>18552.213339</td>\n","      <td>18552.213339</td>\n","      <td>18552.213339</td>\n","      <td>0.235856</td>\n","      <td>0.235856</td>\n","      <td>0.235856</td>\n","      <td>0.235856</td>\n","      <td>78933.0</td>\n","      <td>78933.0</td>\n","      <td>78933.0</td>\n","      <td>78933.0</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-02-0006</th>\n","      <td>HGG</td>\n","      <td>116.257912</td>\n","      <td>-89.206642</td>\n","      <td>-94.285591</td>\n","      <td>-24.314096</td>\n","      <td>423.313324</td>\n","      <td>44.255459</td>\n","      <td>37.271446</td>\n","      <td>128.743958</td>\n","      <td>1.147067e+10</td>\n","      <td>3.045949e+09</td>\n","      <td>2.998265e+09</td>\n","      <td>4.604273e+09</td>\n","      <td>4.360581</td>\n","      <td>3.386222</td>\n","      <td>3.456733</td>\n","      <td>3.513266</td>\n","      <td>150.261139</td>\n","      <td>73.634265</td>\n","      <td>70.163757</td>\n","      <td>70.238974</td>\n","      <td>7.201130</td>\n","      <td>2.648042</td>\n","      <td>6.026527</td>\n","      <td>10.632718</td>\n","      <td>1092.955444</td>\n","      <td>145.502579</td>\n","      <td>462.639221</td>\n","      <td>567.999634</td>\n","      <td>230.973357</td>\n","      <td>-22.635707</td>\n","      <td>-25.694167</td>\n","      <td>39.850019</td>\n","      <td>100.329246</td>\n","      <td>41.398431</td>\n","      <td>41.909489</td>\n","      <td>49.042519</td>\n","      <td>184.855392</td>\n","      <td>-22.475592</td>\n","      <td>-26.314457</td>\n","      <td>...</td>\n","      <td>53.935146</td>\n","      <td>53.935146</td>\n","      <td>53.935146</td>\n","      <td>53.935146</td>\n","      <td>60.638272</td>\n","      <td>60.638272</td>\n","      <td>60.638272</td>\n","      <td>60.638272</td>\n","      <td>63.631753</td>\n","      <td>63.631753</td>\n","      <td>63.631753</td>\n","      <td>63.631753</td>\n","      <td>63.639610</td>\n","      <td>63.639610</td>\n","      <td>63.639610</td>\n","      <td>63.639610</td>\n","      <td>37958.333333</td>\n","      <td>37958.333333</td>\n","      <td>37958.333333</td>\n","      <td>37958.333333</td>\n","      <td>42.595993</td>\n","      <td>42.595993</td>\n","      <td>42.595993</td>\n","      <td>42.595993</td>\n","      <td>0.388207</td>\n","      <td>0.388207</td>\n","      <td>0.388207</td>\n","      <td>0.388207</td>\n","      <td>14069.926539</td>\n","      <td>14069.926539</td>\n","      <td>14069.926539</td>\n","      <td>14069.926539</td>\n","      <td>0.370668</td>\n","      <td>0.370668</td>\n","      <td>0.370668</td>\n","      <td>0.370668</td>\n","      <td>38314.0</td>\n","      <td>38314.0</td>\n","      <td>38314.0</td>\n","      <td>38314.0</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-02-0009</th>\n","      <td>HGG</td>\n","      <td>90.176323</td>\n","      <td>-26.945517</td>\n","      <td>-8.762620</td>\n","      <td>-32.354511</td>\n","      <td>225.455673</td>\n","      <td>113.102806</td>\n","      <td>152.701782</td>\n","      <td>71.986130</td>\n","      <td>5.131474e+09</td>\n","      <td>2.879156e+09</td>\n","      <td>3.505119e+09</td>\n","      <td>2.589367e+09</td>\n","      <td>3.472340</td>\n","      <td>3.448443</td>\n","      <td>3.726060</td>\n","      <td>3.065067</td>\n","      <td>82.228622</td>\n","      <td>71.819657</td>\n","      <td>92.964359</td>\n","      <td>57.138921</td>\n","      <td>3.239426</td>\n","      <td>2.955251</td>\n","      <td>3.469733</td>\n","      <td>3.446095</td>\n","      <td>373.997711</td>\n","      <td>188.513443</td>\n","      <td>444.642487</td>\n","      <td>218.559891</td>\n","      <td>154.909107</td>\n","      <td>39.042723</td>\n","      <td>73.147109</td>\n","      <td>22.854031</td>\n","      <td>45.483752</td>\n","      <td>42.845064</td>\n","      <td>52.634881</td>\n","      <td>33.214035</td>\n","      <td>151.184662</td>\n","      <td>37.692173</td>\n","      <td>76.046967</td>\n","      <td>...</td>\n","      <td>45.276926</td>\n","      <td>45.276926</td>\n","      <td>45.276926</td>\n","      <td>45.276926</td>\n","      <td>63.890531</td>\n","      <td>63.890531</td>\n","      <td>63.890531</td>\n","      <td>63.890531</td>\n","      <td>62.641839</td>\n","      <td>62.641839</td>\n","      <td>62.641839</td>\n","      <td>62.641839</td>\n","      <td>78.803553</td>\n","      <td>78.803553</td>\n","      <td>78.803553</td>\n","      <td>78.803553</td>\n","      <td>24361.500000</td>\n","      <td>24361.500000</td>\n","      <td>24361.500000</td>\n","      <td>24361.500000</td>\n","      <td>39.121798</td>\n","      <td>39.121798</td>\n","      <td>39.121798</td>\n","      <td>39.121798</td>\n","      <td>0.473125</td>\n","      <td>0.473125</td>\n","      <td>0.473125</td>\n","      <td>0.473125</td>\n","      <td>8589.699469</td>\n","      <td>8589.699469</td>\n","      <td>8589.699469</td>\n","      <td>8589.699469</td>\n","      <td>0.352593</td>\n","      <td>0.352593</td>\n","      <td>0.352593</td>\n","      <td>0.352593</td>\n","      <td>24434.0</td>\n","      <td>24434.0</td>\n","      <td>24434.0</td>\n","      <td>24434.0</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-02-0011</th>\n","      <td>HGG</td>\n","      <td>64.619064</td>\n","      <td>-72.600388</td>\n","      <td>-51.918762</td>\n","      <td>4.858555</td>\n","      <td>361.840515</td>\n","      <td>110.048798</td>\n","      <td>177.660934</td>\n","      <td>303.965820</td>\n","      <td>3.725537e+10</td>\n","      <td>1.393658e+10</td>\n","      <td>1.820387e+10</td>\n","      <td>2.943032e+10</td>\n","      <td>4.554435</td>\n","      <td>3.939151</td>\n","      <td>4.259299</td>\n","      <td>4.365507</td>\n","      <td>185.484077</td>\n","      <td>106.352695</td>\n","      <td>99.628928</td>\n","      <td>188.811455</td>\n","      <td>3.910393</td>\n","      <td>3.734265</td>\n","      <td>6.654378</td>\n","      <td>1.990911</td>\n","      <td>1099.307373</td>\n","      <td>493.843292</td>\n","      <td>1132.799072</td>\n","      <td>615.224304</td>\n","      <td>225.372445</td>\n","      <td>21.298079</td>\n","      <td>63.985697</td>\n","      <td>166.848028</td>\n","      <td>101.129097</td>\n","      <td>60.688880</td>\n","      <td>71.385484</td>\n","      <td>95.145770</td>\n","      <td>238.929398</td>\n","      <td>19.880213</td>\n","      <td>60.705238</td>\n","      <td>...</td>\n","      <td>80.062476</td>\n","      <td>80.062476</td>\n","      <td>80.062476</td>\n","      <td>80.062476</td>\n","      <td>99.764723</td>\n","      <td>99.764723</td>\n","      <td>99.764723</td>\n","      <td>99.764723</td>\n","      <td>87.817994</td>\n","      <td>87.817994</td>\n","      <td>87.817994</td>\n","      <td>87.817994</td>\n","      <td>112.312065</td>\n","      <td>112.312065</td>\n","      <td>112.312065</td>\n","      <td>112.312065</td>\n","      <td>127523.625000</td>\n","      <td>127523.625000</td>\n","      <td>127523.625000</td>\n","      <td>127523.625000</td>\n","      <td>65.845557</td>\n","      <td>65.845557</td>\n","      <td>65.845557</td>\n","      <td>65.845557</td>\n","      <td>0.528151</td>\n","      <td>0.528151</td>\n","      <td>0.528151</td>\n","      <td>0.528151</td>\n","      <td>23198.153661</td>\n","      <td>23198.153661</td>\n","      <td>23198.153661</td>\n","      <td>23198.153661</td>\n","      <td>0.181913</td>\n","      <td>0.181913</td>\n","      <td>0.181913</td>\n","      <td>0.181913</td>\n","      <td>127812.0</td>\n","      <td>127812.0</td>\n","      <td>127812.0</td>\n","      <td>127812.0</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-02-0027</th>\n","      <td>HGG</td>\n","      <td>86.231560</td>\n","      <td>-21.451303</td>\n","      <td>-61.386211</td>\n","      <td>0.316952</td>\n","      <td>318.213013</td>\n","      <td>142.173523</td>\n","      <td>210.579803</td>\n","      <td>311.565369</td>\n","      <td>1.366329e+10</td>\n","      <td>7.190870e+09</td>\n","      <td>7.562883e+09</td>\n","      <td>1.087016e+10</td>\n","      <td>4.230068</td>\n","      <td>3.774224</td>\n","      <td>4.384241</td>\n","      <td>4.441627</td>\n","      <td>120.599632</td>\n","      <td>94.194830</td>\n","      <td>130.898087</td>\n","      <td>158.668640</td>\n","      <td>6.977550</td>\n","      <td>5.947844</td>\n","      <td>4.349228</td>\n","      <td>2.905391</td>\n","      <td>937.342346</td>\n","      <td>592.362915</td>\n","      <td>631.549255</td>\n","      <td>604.905396</td>\n","      <td>193.562543</td>\n","      <td>58.896227</td>\n","      <td>58.172095</td>\n","      <td>134.440388</td>\n","      <td>76.399007</td>\n","      <td>54.518443</td>\n","      <td>84.737838</td>\n","      <td>93.343820</td>\n","      <td>176.105179</td>\n","      <td>53.285439</td>\n","      <td>41.853008</td>\n","      <td>...</td>\n","      <td>56.400355</td>\n","      <td>56.400355</td>\n","      <td>56.400355</td>\n","      <td>56.400355</td>\n","      <td>64.327288</td>\n","      <td>64.327288</td>\n","      <td>64.327288</td>\n","      <td>64.327288</td>\n","      <td>64.124878</td>\n","      <td>64.124878</td>\n","      <td>64.124878</td>\n","      <td>64.124878</td>\n","      <td>65.833122</td>\n","      <td>65.833122</td>\n","      <td>65.833122</td>\n","      <td>65.833122</td>\n","      <td>53631.833333</td>\n","      <td>53631.833333</td>\n","      <td>53631.833333</td>\n","      <td>53631.833333</td>\n","      <td>44.310519</td>\n","      <td>44.310519</td>\n","      <td>44.310519</td>\n","      <td>44.310519</td>\n","      <td>0.603346</td>\n","      <td>0.603346</td>\n","      <td>0.603346</td>\n","      <td>0.603346</td>\n","      <td>11398.987373</td>\n","      <td>11398.987373</td>\n","      <td>11398.987373</td>\n","      <td>11398.987373</td>\n","      <td>0.212541</td>\n","      <td>0.212541</td>\n","      <td>0.212541</td>\n","      <td>0.212541</td>\n","      <td>53787.0</td>\n","      <td>53787.0</td>\n","      <td>53787.0</td>\n","      <td>53787.0</td>\n","    </tr>\n","    <tr>\n","      <th>...</th>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","      <td>...</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-HT-8114</th>\n","      <td>LGG</td>\n","      <td>105.555672</td>\n","      <td>-63.724358</td>\n","      <td>-142.988251</td>\n","      <td>5.321779</td>\n","      <td>303.597015</td>\n","      <td>112.424210</td>\n","      <td>39.730719</td>\n","      <td>230.225479</td>\n","      <td>5.211240e+10</td>\n","      <td>1.911492e+10</td>\n","      <td>1.143351e+10</td>\n","      <td>3.595359e+10</td>\n","      <td>3.902426</td>\n","      <td>3.739786</td>\n","      <td>3.772426</td>\n","      <td>4.078159</td>\n","      <td>100.498596</td>\n","      <td>85.278271</td>\n","      <td>92.776907</td>\n","      <td>105.409996</td>\n","      <td>3.178429</td>\n","      <td>3.792731</td>\n","      <td>5.334574</td>\n","      <td>2.978642</td>\n","      <td>429.959198</td>\n","      <td>217.274536</td>\n","      <td>612.799927</td>\n","      <td>410.409790</td>\n","      <td>219.951210</td>\n","      <td>10.834349</td>\n","      <td>-65.320957</td>\n","      <td>128.387010</td>\n","      <td>60.907346</td>\n","      <td>53.410478</td>\n","      <td>57.970707</td>\n","      <td>65.920285</td>\n","      <td>231.178909</td>\n","      <td>-0.115153</td>\n","      <td>-82.683258</td>\n","      <td>...</td>\n","      <td>79.177017</td>\n","      <td>79.177017</td>\n","      <td>79.177017</td>\n","      <td>79.177017</td>\n","      <td>92.763139</td>\n","      <td>92.763139</td>\n","      <td>92.763139</td>\n","      <td>92.763139</td>\n","      <td>90.553851</td>\n","      <td>90.553851</td>\n","      <td>90.553851</td>\n","      <td>90.553851</td>\n","      <td>94.042544</td>\n","      <td>94.042544</td>\n","      <td>94.042544</td>\n","      <td>94.042544</td>\n","      <td>188533.416667</td>\n","      <td>188533.416667</td>\n","      <td>188533.416667</td>\n","      <td>188533.416667</td>\n","      <td>64.033987</td>\n","      <td>64.033987</td>\n","      <td>64.033987</td>\n","      <td>64.033987</td>\n","      <td>0.726170</td>\n","      <td>0.726170</td>\n","      <td>0.726170</td>\n","      <td>0.726170</td>\n","      <td>21896.327956</td>\n","      <td>21896.327956</td>\n","      <td>21896.327956</td>\n","      <td>21896.327956</td>\n","      <td>0.116140</td>\n","      <td>0.116140</td>\n","      <td>0.116140</td>\n","      <td>0.116140</td>\n","      <td>188686.0</td>\n","      <td>188686.0</td>\n","      <td>188686.0</td>\n","      <td>188686.0</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-HT-8563</th>\n","      <td>LGG</td>\n","      <td>107.853134</td>\n","      <td>-13.839657</td>\n","      <td>-107.915237</td>\n","      <td>41.547455</td>\n","      <td>343.804901</td>\n","      <td>81.128136</td>\n","      <td>50.434380</td>\n","      <td>214.858383</td>\n","      <td>4.311416e+10</td>\n","      <td>1.699992e+10</td>\n","      <td>1.395661e+10</td>\n","      <td>2.820207e+10</td>\n","      <td>4.103061</td>\n","      <td>2.972423</td>\n","      <td>3.806610</td>\n","      <td>3.821027</td>\n","      <td>135.408752</td>\n","      <td>51.736185</td>\n","      <td>83.235051</td>\n","      <td>95.048248</td>\n","      <td>2.121575</td>\n","      <td>3.017086</td>\n","      <td>16.199101</td>\n","      <td>3.226575</td>\n","      <td>450.834564</td>\n","      <td>163.339066</td>\n","      <td>687.047180</td>\n","      <td>442.512543</td>\n","      <td>226.369294</td>\n","      <td>32.850400</td>\n","      <td>-20.750881</td>\n","      <td>125.827849</td>\n","      <td>73.073753</td>\n","      <td>30.165054</td>\n","      <td>68.273497</td>\n","      <td>55.492192</td>\n","      <td>226.234436</td>\n","      <td>32.226810</td>\n","      <td>-44.643063</td>\n","      <td>...</td>\n","      <td>75.604233</td>\n","      <td>75.604233</td>\n","      <td>75.604233</td>\n","      <td>75.604233</td>\n","      <td>95.885348</td>\n","      <td>95.885348</td>\n","      <td>95.885348</td>\n","      <td>95.885348</td>\n","      <td>104.139330</td>\n","      <td>104.139330</td>\n","      <td>104.139330</td>\n","      <td>104.139330</td>\n","      <td>107.112091</td>\n","      <td>107.112091</td>\n","      <td>107.112091</td>\n","      <td>107.112091</td>\n","      <td>151339.416667</td>\n","      <td>151339.416667</td>\n","      <td>151339.416667</td>\n","      <td>151339.416667</td>\n","      <td>59.379271</td>\n","      <td>59.379271</td>\n","      <td>59.379271</td>\n","      <td>59.379271</td>\n","      <td>0.640041</td>\n","      <td>0.640041</td>\n","      <td>0.640041</td>\n","      <td>0.640041</td>\n","      <td>21457.413791</td>\n","      <td>21457.413791</td>\n","      <td>21457.413791</td>\n","      <td>21457.413791</td>\n","      <td>0.141783</td>\n","      <td>0.141783</td>\n","      <td>0.141783</td>\n","      <td>0.141783</td>\n","      <td>151508.0</td>\n","      <td>151508.0</td>\n","      <td>151508.0</td>\n","      <td>151508.0</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-HT-A5RC</th>\n","      <td>LGG</td>\n","      <td>132.378906</td>\n","      <td>-53.662895</td>\n","      <td>-43.392906</td>\n","      <td>-14.490097</td>\n","      <td>300.511658</td>\n","      <td>75.641731</td>\n","      <td>214.946136</td>\n","      <td>117.851212</td>\n","      <td>1.783356e+10</td>\n","      <td>6.649875e+09</td>\n","      <td>9.099524e+09</td>\n","      <td>8.308008e+09</td>\n","      <td>3.888564</td>\n","      <td>3.439127</td>\n","      <td>3.950961</td>\n","      <td>3.548205</td>\n","      <td>93.407074</td>\n","      <td>80.462484</td>\n","      <td>182.200129</td>\n","      <td>67.838821</td>\n","      <td>5.387503</td>\n","      <td>3.059040</td>\n","      <td>2.188884</td>\n","      <td>11.104049</td>\n","      <td>728.486023</td>\n","      <td>255.200119</td>\n","      <td>484.543091</td>\n","      <td>567.978455</td>\n","      <td>216.523620</td>\n","      <td>14.152946</td>\n","      <td>58.648560</td>\n","      <td>50.205872</td>\n","      <td>56.969725</td>\n","      <td>44.315148</td>\n","      <td>87.714144</td>\n","      <td>45.920952</td>\n","      <td>213.897827</td>\n","      <td>16.071259</td>\n","      <td>19.118032</td>\n","      <td>...</td>\n","      <td>81.394103</td>\n","      <td>81.394103</td>\n","      <td>81.394103</td>\n","      <td>81.394103</td>\n","      <td>90.426766</td>\n","      <td>90.426766</td>\n","      <td>90.426766</td>\n","      <td>90.426766</td>\n","      <td>72.173402</td>\n","      <td>72.173402</td>\n","      <td>72.173402</td>\n","      <td>72.173402</td>\n","      <td>91.804139</td>\n","      <td>91.804139</td>\n","      <td>91.804139</td>\n","      <td>91.804139</td>\n","      <td>65124.916667</td>\n","      <td>65124.916667</td>\n","      <td>65124.916667</td>\n","      <td>65124.916667</td>\n","      <td>57.412963</td>\n","      <td>57.412963</td>\n","      <td>57.412963</td>\n","      <td>57.412963</td>\n","      <td>0.394641</td>\n","      <td>0.394641</td>\n","      <td>0.394641</td>\n","      <td>0.394641</td>\n","      <td>19835.667125</td>\n","      <td>19835.667125</td>\n","      <td>19835.667125</td>\n","      <td>19835.667125</td>\n","      <td>0.304579</td>\n","      <td>0.304579</td>\n","      <td>0.304579</td>\n","      <td>0.304579</td>\n","      <td>65454.0</td>\n","      <td>65454.0</td>\n","      <td>65454.0</td>\n","      <td>65454.0</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-HT-A614</th>\n","      <td>LGG</td>\n","      <td>-189.750610</td>\n","      <td>-129.057861</td>\n","      <td>-142.548935</td>\n","      <td>-67.626572</td>\n","      <td>250.198044</td>\n","      <td>82.339119</td>\n","      <td>61.333740</td>\n","      <td>277.827911</td>\n","      <td>1.885509e+10</td>\n","      <td>8.721286e+09</td>\n","      <td>8.626523e+09</td>\n","      <td>1.856944e+10</td>\n","      <td>4.833780</td>\n","      <td>4.050981</td>\n","      <td>4.150267</td>\n","      <td>4.746050</td>\n","      <td>201.411090</td>\n","      <td>120.761169</td>\n","      <td>113.918303</td>\n","      <td>175.578082</td>\n","      <td>3.641021</td>\n","      <td>4.907473</td>\n","      <td>8.701819</td>\n","      <td>2.878728</td>\n","      <td>743.051941</td>\n","      <td>678.873352</td>\n","      <td>922.943787</td>\n","      <td>485.603699</td>\n","      <td>82.346211</td>\n","      <td>-27.239838</td>\n","      <td>-32.629332</td>\n","      <td>93.907260</td>\n","      <td>135.316829</td>\n","      <td>67.372963</td>\n","      <td>70.171551</td>\n","      <td>106.987732</td>\n","      <td>136.962326</td>\n","      <td>-31.669817</td>\n","      <td>-37.224792</td>\n","      <td>...</td>\n","      <td>77.317527</td>\n","      <td>77.317527</td>\n","      <td>77.317527</td>\n","      <td>77.317527</td>\n","      <td>79.120162</td>\n","      <td>79.120162</td>\n","      <td>79.120162</td>\n","      <td>79.120162</td>\n","      <td>94.921020</td>\n","      <td>94.921020</td>\n","      <td>94.921020</td>\n","      <td>94.921020</td>\n","      <td>105.095195</td>\n","      <td>105.095195</td>\n","      <td>105.095195</td>\n","      <td>105.095195</td>\n","      <td>106839.791667</td>\n","      <td>106839.791667</td>\n","      <td>106839.791667</td>\n","      <td>106839.791667</td>\n","      <td>50.017972</td>\n","      <td>50.017972</td>\n","      <td>50.017972</td>\n","      <td>50.017972</td>\n","      <td>0.484685</td>\n","      <td>0.484685</td>\n","      <td>0.484685</td>\n","      <td>0.484685</td>\n","      <td>22465.372639</td>\n","      <td>22465.372639</td>\n","      <td>22465.372639</td>\n","      <td>22465.372639</td>\n","      <td>0.210272</td>\n","      <td>0.210272</td>\n","      <td>0.210272</td>\n","      <td>0.210272</td>\n","      <td>107112.0</td>\n","      <td>107112.0</td>\n","      <td>107112.0</td>\n","      <td>107112.0</td>\n","    </tr>\n","    <tr>\n","      <th>TCGA-HT-A61A</th>\n","      <td>LGG</td>\n","      <td>27.535517</td>\n","      <td>-111.265610</td>\n","      <td>-129.352097</td>\n","      <td>-0.111687</td>\n","      <td>252.841187</td>\n","      <td>43.893410</td>\n","      <td>48.524246</td>\n","      <td>175.042496</td>\n","      <td>2.773430e+10</td>\n","      <td>9.548890e+09</td>\n","      <td>9.710092e+09</td>\n","      <td>2.244338e+10</td>\n","      <td>4.208416</td>\n","      <td>3.626885</td>\n","      <td>3.856140</td>\n","      <td>3.751481</td>\n","      <td>113.116425</td>\n","      <td>78.694613</td>\n","      <td>94.184887</td>\n","      <td>85.486397</td>\n","      <td>8.059654</td>\n","      <td>8.763068</td>\n","      <td>16.554846</td>\n","      <td>13.742133</td>\n","      <td>411.389618</td>\n","      <td>242.382263</td>\n","      <td>1205.616333</td>\n","      <td>302.342896</td>\n","      <td>139.640277</td>\n","      <td>-44.317670</td>\n","      <td>-45.786690</td>\n","      <td>98.124176</td>\n","      <td>73.104713</td>\n","      <td>50.110754</td>\n","      <td>58.232294</td>\n","      <td>56.431576</td>\n","      <td>147.142227</td>\n","      <td>-52.642899</td>\n","      <td>-52.826881</td>\n","      <td>...</td>\n","      <td>80.081209</td>\n","      <td>80.081209</td>\n","      <td>80.081209</td>\n","      <td>80.081209</td>\n","      <td>89.358827</td>\n","      <td>89.358827</td>\n","      <td>89.358827</td>\n","      <td>89.358827</td>\n","      <td>85.866175</td>\n","      <td>85.866175</td>\n","      <td>85.866175</td>\n","      <td>85.866175</td>\n","      <td>90.553851</td>\n","      <td>90.553851</td>\n","      <td>90.553851</td>\n","      <td>90.553851</td>\n","      <td>136148.333333</td>\n","      <td>136148.333333</td>\n","      <td>136148.333333</td>\n","      <td>136148.333333</td>\n","      <td>59.235571</td>\n","      <td>59.235571</td>\n","      <td>59.235571</td>\n","      <td>59.235571</td>\n","      <td>0.548039</td>\n","      <td>0.548039</td>\n","      <td>0.548039</td>\n","      <td>0.548039</td>\n","      <td>23353.248689</td>\n","      <td>23353.248689</td>\n","      <td>23353.248689</td>\n","      <td>23353.248689</td>\n","      <td>0.171528</td>\n","      <td>0.171528</td>\n","      <td>0.171528</td>\n","      <td>0.171528</td>\n","      <td>136334.0</td>\n","      <td>136334.0</td>\n","      <td>136334.0</td>\n","      <td>136334.0</td>\n","    </tr>\n","  </tbody>\n","</table>\n","<p>243 rows × 401 columns</p>\n","</div>"],"text/plain":["             label  ...  original_shape_VoxelVolume_t2_full\n","patient             ...                                    \n","TCGA-02-0003   HGG  ...                             78933.0\n","TCGA-02-0006   HGG  ...                             38314.0\n","TCGA-02-0009   HGG  ...                             24434.0\n","TCGA-02-0011   HGG  ...                            127812.0\n","TCGA-02-0027   HGG  ...                             53787.0\n","...            ...  ...                                 ...\n","TCGA-HT-8114   LGG  ...                            188686.0\n","TCGA-HT-8563   LGG  ...                            151508.0\n","TCGA-HT-A5RC   LGG  ...                             65454.0\n","TCGA-HT-A614   LGG  ...                            107112.0\n","TCGA-HT-A61A   LGG  ...                            136334.0\n","\n","[243 rows x 401 columns]"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"Yfw2r-YMSM64"},"source":["## Splitting datas"]},{"cell_type":"markdown","metadata":{"id":"kqgZYh-PSM6-"},"source":["Let’s now use <code>train_test_split</code> from the function from scikit-learn to divide features data (x_data) and target data (y_data) even further into train and test. Here we will have 30% of the data for the test set. It is also a good practice to define a random state for reproducible results."]},{"cell_type":"code","metadata":{"id":"wAbEv0V1SM7C","executionInfo":{"status":"ok","timestamp":1636137323984,"user_tz":-60,"elapsed":697,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["x_data, y_data = data.drop(columns='label'), data['label'].astype(int).to_numpy()"],"execution_count":4,"outputs":[]},{"cell_type":"code","metadata":{"id":"_m0rrTwySM7P","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636137328245,"user_tz":-60,"elapsed":1133,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"592024cc-8b76-4d1d-9062-e412dc7f2b72"},"source":["from sklearn.model_selection import train_test_split\n","x_train, x_test, y_train, y_test = train_test_split(x_data, y_data ,test_size = 0.3, random_state=123, stratify=y_data)\n","print('x_train shape: ', x_train.shape)\n","print('x_test shape: ', x_test.shape)\n","print('y_train shape: ', y_train.shape)\n","print('y_test shape: ', y_test.shape)"],"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["x_train shape:  (170, 400)\n","x_test shape:  (73, 400)\n","y_train shape:  (170,)\n","y_test shape:  (73,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"nd76_qCqSM7X"},"source":["Now we have our training set wich is a set for training (and validation). The test set is considered like an unseen set and **will be never seen until the model performance evaluation** – this has to be data that your model hasn’t seen before.\n","\n","But the strategy for evaluate a model depends on your goal and approach. [2]\n","\n"," - **Scenario 1: Just train a simple model**  \n","Split the dataset into a separate training and test set. Train the model on the former, evaluate the model on the latter (by “evaluate” I mean calculating performance metrics such as the error, precision, recall, ROC auc, etc.)\n"," - **Scenario 2: Train a model and tune (optimize) its hyperparameters.**  \n","Split the dataset into a separate test and training set. Use techniques such as k-fold cross-validation on the training set to find the “optimal” set of hyperparameters for your model. If you are done with hyperparameter tuning, use the independent test set to get an unbiased estimate of its performance. \n"," - **Scenario 3: Build differents models and compare algorithms (e.g., SVM vs. logistic regression vs. Random Forests, etc.).**  \n","Here, we’d want to use nested cross-validation. In nested cross-validation, we have an outer k-fold cross-validation loop to split the data into training and test folds, and an inner loop is used to select the model via k-fold cross-validation on the training fold. After model selection, the test fold is then used to evaluate the model performance. After we have identified our “favorite” algorithm, we can follow-up with a “regular” k-fold cross-validation approach (on the complete training set) to find its “optimal” hyperparameters and evaluate it on the independent test set. \n","\n","<a href=\"https://sebastianraschka.com/faq/docs/evaluate-a-model.html\">\n","  <img src=\"images/evaluate_overview.png?raw=1\" alt=\"evaluate_overview\" class=\"center\">\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"ZDrDriehSM7a"},"source":["# 1) Introduction to data preprocessing"]},{"cell_type":"markdown","metadata":{"id":"aWJ3GFuoSM7c"},"source":["Data pre-processing is an integral step in machine learning because the quality of the data and the useful information that can be derived from it directly affects the ability of our model to learn, so it is extremely important that we pre-process our data before introducing it into our model."]},{"cell_type":"markdown","metadata":{"id":"2VDfhJcdSM7g"},"source":["### Handling Null Values"]},{"cell_type":"markdown","metadata":{"id":"IoOngG37SM7l"},"source":["In any real world dataset there are always few null values. Very few models can handle these NULL or NaN values on its own so we need to intervene. In python NULL is represented with NaN. First of all, we need to check whether we have null values in our dataset or not. We can do that using the isnull() method."]},{"cell_type":"code","metadata":{"id":"-8ljn4kESM7n","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636137334953,"user_tz":-60,"elapsed":366,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"2613b8cd-dcf6-4404-8133-cfb22ffb6d3d"},"source":["data.isnull().sum() # add .any() if you want to know about if there is any NaN in the sum (returns bool)\n","# Returns the column names along with the number of NaN values in that particular column (we can specify the axis=1, if we want rows)"],"execution_count":6,"outputs":[{"output_type":"execute_result","data":{"text/plain":["label                                          0\n","original_firstorder_10Percentile_flair_full    0\n","original_firstorder_10Percentile_t1_full       0\n","original_firstorder_10Percentile_t1ce_full     0\n","original_firstorder_10Percentile_t2_full       0\n","                                              ..\n","original_shape_SurfaceVolumeRatio_t2_full      0\n","original_shape_VoxelVolume_flair_full          0\n","original_shape_VoxelVolume_t1_full             0\n","original_shape_VoxelVolume_t1ce_full           0\n","original_shape_VoxelVolume_t2_full             0\n","Length: 401, dtype: int64"]},"metadata":{},"execution_count":6}]},{"cell_type":"markdown","metadata":{"id":"j3D0LJ6aSM7x"},"source":["We don't have any NaN for our datas. But there is some strategy to handle the missing datas"]},{"cell_type":"markdown","metadata":{"id":"nXDrIYAMSM70"},"source":["#### 1. The easiest way to solve the problem of NaN is by dropping the rows or columns that contain null values."]},{"cell_type":"code","metadata":{"id":"FRlSJpc5SM74","executionInfo":{"status":"ok","timestamp":1636137340207,"user_tz":-60,"elapsed":374,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["data.dropna(); # (axis=0 for columns or 1 for rows), you have various parameters for dropna(), like 'how', 'tresh',\n","# take a look at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.dropna.html"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"ZGXuiAzgSM8F"},"source":["#### 2. Imputation"]},{"cell_type":"markdown","metadata":{"id":"yQXghf68SM8K"},"source":["Imputation is simply the process of substituting the missing values of our dataset. So we can change replace the values by the mean, max, 0, custom function ... Train another algorithm to predict the missing value from the rest of the data"]},{"cell_type":"code","metadata":{"id":"FIOQk6_6SM8N","executionInfo":{"status":"ok","timestamp":1636137347953,"user_tz":-60,"elapsed":3,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["data.fillna(0); # will replace NaN values by a 0, take a look at https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.DataFrame.fillna.html"],"execution_count":8,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"rFsJ1qsaSM8W"},"source":["### Standardization/Normalization"]},{"cell_type":"markdown","metadata":{"id":"NzvchF4BSM8Y"},"source":["Different radiomics features have different units and range. Some features were designed to fall between 0 and 1, while others have a very large range. In some machine learning algorithms, the objective functions will not work properly without normalization. For example, many classifiers calculate the distance between two points by the Euclidean distance. If one of the features has a wide range of values, the distance will be governed by that particular characteristic. Therefore, the range of all characteristics should be normalized so that each characteristic contributes approximately proportionally to the final distance. We will present the two most common techniques which are used."]},{"cell_type":"markdown","metadata":{"id":"uv0_apFPSM9H"},"source":["#### 1. Min-Max scaling"]},{"cell_type":"markdown","metadata":{"id":"adVmLPHySM9K"},"source":["This estimator scales and translates each feature individually such that it is in the given range on the training set, e.g. between zero and one."]},{"cell_type":"markdown","metadata":{"id":"RR2BWfuOSM9M"},"source":["$$ X_{norm} = \\frac{X - X_{min}}{X_{max}-X_{min}} $$"]},{"cell_type":"markdown","metadata":{"id":"yDPgsDapSM89"},"source":["Scikit-learn directly implements this for us:"]},{"cell_type":"code","metadata":{"id":"RV5-z_ZcSM9P","executionInfo":{"status":"ok","timestamp":1636137360533,"user_tz":-60,"elapsed":381,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["from sklearn.preprocessing import MinMaxScaler \n","\n","scaler = MinMaxScaler()\n","scaler.fit(x_train)\n","x_train_standardize = scaler.transform(x_train)\n","# We apply the same transform on the test\n","x_test_standardize = scaler.transform(x_test)"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"TRHtAYJ_SM8a"},"source":["#### 2. Z-Score normalization"]},{"cell_type":"markdown","metadata":{"id":"owgtxxi3SM8d"},"source":["It standardize features by removing the mean and scaling to unit variance. By anothers words, we transform our values such that the mean of the values is 0 and the standard deviation is 1."]},{"cell_type":"markdown","metadata":{"id":"ylhGX9VhSM8i"},"source":["$$ Z = \\frac{x_i - \\mu}{\\sigma} $$  \n","with mean: $$ \\mu = \\frac{1}{N} \\sum_{i=1}^N (x_i) $$ \n","and standard deviation: $$ \\sigma = \\sqrt{\\frac{1}{N} \\sum_{i=1}^N (x_i - \\mu)^2} $$ "]},{"cell_type":"markdown","metadata":{"id":"4CMVvdasSM8l"},"source":["**Exercice:** \n","Complete the following scripts to:\n","\n","- do the Z-Score normalization on the <code>x_train</code> and apply it on the  <code>x_test</code> without using the scikit-learn function."]},{"cell_type":"code","metadata":{"id":"ETwe0kjiSM8n","executionInfo":{"status":"ok","timestamp":1636137389337,"user_tz":-60,"elapsed":363,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["#@title Exercise 3 {display-mode: \"form\"}\n","# %load solutions_exercices/z_score.py\n","\n","# Two possible function for z_score\n","\n","import numpy as np\n","import operator\n","import pandas as pd\n","\n","\n","def z_score_1(X):\n","    # zero mean and unit variance\n","    mean = np.mean(X, axis=0)\n","    std_dev = np.std(X, axis=0)\n","    z = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n","    return z, mean, std_dev\n","\n","#  or\n","\n","\n","def z_score_2(X):\n","    if isinstance(X, pd.DataFrame):\n","        X = X.to_numpy()\n","\n","    def _mean(data):\n","        return float(sum(data) / len(data))\n","\n","    def _variance(data):\n","        mu = _mean(data)\n","        return _mean([(x - mu) ** 2 for x in data])\n","\n","    def _stddev(data):\n","        return (_variance(data)) ** 0.5\n","\n","    def _zscore(data):\n","        num = [x - _mean(data) for x in data]\n","        return [x / _stddev(data) for x in num]\n","\n","    mean = [_mean(x) for x in zip(*X)]\n","    std_dev = [_stddev(x) for x in zip(*X)]\n","    z = [_zscore(x) for x in zip(*X)]\n","\n","    return z, mean, std_dev\n","\n","\n","x_train_standardize, mean, std_dev = '''CompleteHere'''(x_train)\n","x_test_standardize = (('''CompleteHere''' - mean) / std_dev)\n"],"execution_count":10,"outputs":[]},{"cell_type":"code","metadata":{"id":"8B1NyzfASM8z"},"source":["#@title Solution 3 {display-mode: \"form\"}\n","# %load solutions_exercices/z_score.py\n","\n","# Two possible function for z_score\n","\n","import numpy as np\n","import operator\n","import pandas as pd\n","\n","\n","def z_score_1(X):\n","    # zero mean and unit variance\n","    mean = np.mean(X, axis=0)\n","    std_dev = np.std(X, axis=0)\n","    z = (X - np.mean(X, axis=0)) / np.std(X, axis=0)\n","    return z, mean, std_dev\n","\n","#  or\n","\n","\n","def z_score_2(X):\n","    if isinstance(X, pd.DataFrame):\n","        X = X.to_numpy()\n","\n","    def _mean(data):\n","        return float(sum(data) / len(data))\n","\n","    def _variance(data):\n","        mu = _mean(data)\n","        return _mean([(x - mu) ** 2 for x in data])\n","\n","    def _stddev(data):\n","        return (_variance(data)) ** 0.5\n","\n","    def _zscore(data):\n","        num = [x - _mean(data) for x in data]\n","        return [x / _stddev(data) for x in num]\n","\n","    mean = [_mean(x) for x in zip(*X)]\n","    std_dev = [_stddev(x) for x in zip(*X)]\n","    z = [_zscore(x) for x in zip(*X)]\n","\n","    return z, mean, std_dev\n","\n","\n","x_train_standardize, mean, std_dev = z_score_1(x_train) # or use z_score_2\n","x_test_standardize = ((x_test - mean) / std_dev)\n","# Yes, on the test set, you need to apply the value find in the train set!\n","\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"WXqwOyl4SM9A","executionInfo":{"status":"ok","timestamp":1636137653731,"user_tz":-60,"elapsed":394,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["from sklearn.preprocessing import StandardScaler\n","\n","scaler = StandardScaler()\n","scaler.fit(x_train)\n","x_train_standardize = scaler.transform(x_train)\n","# We apply the same transform on the test\n","x_test_standardize = scaler.transform(x_test)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"nUVFMjKiSM9a"},"source":["There are many others techniques, you can consult this link which \n"," <a href=\"https://scikit-learn.org/stable/auto_examples/preprocessing/plot_all_scaling.html#sphx-glr-auto-examples-preprocessing-plot-all-scaling-py\">compares the effect of different scalers on data with outliers</a> "]},{"cell_type":"markdown","metadata":{"id":"1jWYanDHSM9d"},"source":["# 2) Building models"]},{"cell_type":"markdown","metadata":{"id":"poBXP9oHSM9q"},"source":["#### Standard validation (Hold-Out method)"]},{"cell_type":"markdown","metadata":{"id":"IY-YNBRnSM9h"},"source":["Previously we have separated our train and test set datas. The entire train data has been used so far to derive estimates of means and variances for each feature such as to perform Z-score normalization. We now further split the training set into a smaller training set and a validation set. The training set samples will all be used to optimize the parameters of a model (i.e. iteratively converge to a satisfying model from the input family of functions), and the validation set will be used to select a good set of hyper-parameters such as the family of functions to optimize (here, logestic regression, decision trees, support vector machines).\n","\n","The previous fonction from scikit-learn `train_test_split` is used on the pair (features, labels) of extracted training samples:"]},{"cell_type":"markdown","metadata":{"id":"oqwJKGFKSM9r"},"source":["<a href=\"https://arxiv.org/pdf/1811.12808.pdf\">\n","  <img src=\"images/holdout_method.png\" alt=\"holdout_method\" class=\"center\"  height=\"500\" width=\"500\" >\n","</a>"]},{"cell_type":"code","metadata":{"id":"QHDE-6jySM9j","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636137666445,"user_tz":-60,"elapsed":414,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"99e8c700-f726-4b66-bcfa-5a492775408d"},"source":["train_features, validation_features, train_labels, validation_labels = \\\n","  train_test_split(x_train, y_train ,test_size = 0.3, random_state=123, stratify=y_train)\n","\n","print('train_features shape: ', train_features.shape)\n","print('validation_features shape: ', validation_features.shape)\n","print('train_labels shape: ', train_labels.shape)\n","print('validation_labels shape: ', validation_labels.shape)"],"execution_count":12,"outputs":[{"output_type":"stream","name":"stdout","text":["train_features shape:  (119, 400)\n","validation_features shape:  (51, 400)\n","train_labels shape:  (119,)\n","validation_labels shape:  (51,)\n"]}]},{"cell_type":"markdown","metadata":{"id":"-U0d1dX_X8lg"},"source":["#### Examples of model building with scikit-learn\n","\n","Of the reasons scikit-learn is the leading data science library worldwide is its simplicity of usage and its consistency. By design, the library implements a wide range of machine learning and data processing algorithms, including the most used machine learning families of functions such as logistic regression or boosting algorithms. Here, we will see some examples of machine learning algorithms, including the standard training pipelines using sklearn. \n","\n","Let's look at logistic regression. On paper, logistic regression is handled iteratively, by successively performing inference of the current parametrized model on the training set, and computing a parameters update that is a function of the produced probabilities and the ground-truth annotations. The iterative process is conducted using gradient descent or its _faster_ version stochastic gradient descent. While all of the training process of logistic regression can be coded by hand, its implementation in sklearn lie in two lines:"]},{"cell_type":"code","metadata":{"id":"JX-OZ4MdYuV4","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636137673629,"user_tz":-60,"elapsed":346,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"1539bcb4-a2a5-4b01-c88b-542efa3fd25a"},"source":["from sklearn.linear_model import LogisticRegression\n","logistic_regression_model = LogisticRegression()  # instantiate a logistic regression model with default parameters\n","print(logistic_regression_model)"],"execution_count":13,"outputs":[{"output_type":"stream","name":"stdout","text":["LogisticRegression(C=1.0, class_weight=None, dual=False, fit_intercept=True,\n","                   intercept_scaling=1, l1_ratio=None, max_iter=100,\n","                   multi_class='auto', n_jobs=None, penalty='l2',\n","                   random_state=None, solver='lbfgs', tol=0.0001, verbose=0,\n","                   warm_start=False)\n"]}]},{"cell_type":"markdown","metadata":{"id":"AjehFO2EaVOb"},"source":["Training is then performed using the `fit` function, which applies gradient descent with input hyper-parameters, and usually stops once a training hyper-parameter is reached, such as minimal training error tolerance or number of iterations:"]},{"cell_type":"code","metadata":{"id":"dKJFqk-Eahp6","executionInfo":{"status":"ok","timestamp":1636137677547,"user_tz":-60,"elapsed":444,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["clf = logistic_regression_model.fit(X=train_features, y=train_labels)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"yY9TZ2XXtOs3"},"source":["##### Performance assessment"]},{"cell_type":"markdown","metadata":{"id":"i0HYYrU0aoFD"},"source":["Once the model is trained, i.e. `fit` function is done, it can be used to classify any input sample with the correct shape, or vectors of 140 features in this case. Notably, training and testing accuracies can be obtained with other performance indicators:"]},{"cell_type":"code","metadata":{"id":"kh8ki8RKbBqT","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636137681348,"user_tz":-60,"elapsed":14,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"6e059fd4-0fff-45ce-de98-fa3e5e258a67"},"source":["accuracy_train = clf.score(X=train_features, y=train_labels)\n","probs = clf.predict_proba(train_features)\n","accuracy_validation = clf.score(X=validation_features, y=validation_labels)\n","print('Training accuracy', accuracy_train, '; Validation accuracy', accuracy_validation)"],"execution_count":15,"outputs":[{"output_type":"stream","name":"stdout","text":["Training accuracy 0.8739495798319328 ; Validation accuracy 0.8627450980392157\n"]}]},{"cell_type":"markdown","metadata":{"id":"FGQ4T0VRbbSl"},"source":["In this case, the dataset contain more positive samples than negative samples, which imply that a model outputting only the positive class would yield an accuracy higher than 50%. In other words, the accuracy may not always be suited to the needs of the task in hand. Other performance metrics, such as the accuracy, are implemented in scikit-learn (https://scikit-learn.org/stable/modules/model_evaluation.html). By design all implemented performance metrics follow the same api, and take two vectors as input, one for the predictions of any model and one for ground-truths. We first need to explicitely compute probabilities of the trained model and then run sklearn metrics functions:\n"]},{"cell_type":"code","metadata":{"id":"b-H1lcRrcOwS","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636138715020,"user_tz":-60,"elapsed":279,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"4ae6e1f8-4db5-4c43-89bb-c2be1d78adce"},"source":["# Use the trained model (clf) to explicitely compute probabilities of all training and validation samples\n","predicted_training_probabilities = clf.predict_proba(train_features)[:, 1]  # for each sample, two outputs probas summing to 1: one for class 0 (LGG), the other for class 1 (HGG)\n","predicted_validation_probabilities = clf.predict_proba(validation_features)[:, 1]  # for each sample, two outputs probas summing to 1: one for class 0 (LGG), the other for class 1 (HGG)\n","\n","# Compute predicted classes from predicted probabilities by thresholded probabilities with 0.5: a probability higher than 0.5 would yield HGG prediction, otherwise LGG prediction\n","training_predicted_classes = list(map(lambda proba: int(proba > .5), predicted_training_probabilities))\n","validation_predicted_classes = list(map(lambda proba: int(proba > .5), predicted_validation_probabilities))\n","\n","from sklearn.metrics import balanced_accuracy_score, roc_auc_score\n","\n","# Metrics function expect first the ground-truth vector, then the predicted probabilities/classes one\n","training_balanced_accuracy = balanced_accuracy_score(train_labels, training_predicted_classes)\n","validation_balanced_accuracy = balanced_accuracy_score(validation_labels, validation_predicted_classes)\n","print('Training balanced accuracy', training_balanced_accuracy, '; Validation balanced accuracy', testing_balanced_accuracy)\n","\n","training_auc = roc_auc_score(train_labels, predicted_training_probabilities)\n","validation_auc = roc_auc_score(validation_labels, predicted_testing_probabilities)\n","print('Training AUC', training_balanced_accuracy, '; Validation AUC', validation_balanced_accuracy)\n"],"execution_count":22,"outputs":[{"output_type":"stream","name":"stdout","text":["Training balanced accuracy 0.869639794168096 ; Validation balanced accuracy 0.8517080745341614\n","Training AUC 0.869639794168096 ; Validation AUC 0.8517080745341614\n"]}]},{"cell_type":"markdown","metadata":{"id":"l2rJh1Zedijk"},"source":["Although the balanced accuracy alleviate the issue of class imbalance, it is not a rigourous performance assessment of a decision system. Any decision system performance should be assessed using two measures, such as precision and recall. While giant technology companies can make mistakes in online tools such as Facebook suggesting friends tagging on newly uploaded pictures, in medical routine tasks, errors can have significant impact of patient care or material maintenance. For instance, errors that a patient may be suffering from cancer have less impact than false negative that do not detect patients with cancer. The community would expect guarantees that the level of false negative is close to none for most diagnostic tasks, even if the amount of false positive is significant.\n","\n","**Exercice:** \n","Complete the following script to:\n","\n","- evaluate the logistic regression model trained above, using sklearn documentation (https://scikit-learn.org/stable/modules/model_evaluation.html)\n","- more specifically, compute the precision (also called positive predictive value) and recall (also called true positive rate or sensitivity) called  of the trained classifier on both training and testing sets. "]},{"cell_type":"code","metadata":{"id":"D0RlCyKIfIAe"},"source":["#@title Exercise 4 {display-mode: \"form\"}\n","# %load solutions_exercices/metrics.py\n","\n","from sklearn.metrics import precision_score, recall_score\n","\n","training_precision = '''CompleteHere'''(train_labels, training_predicted_classes)\n","training_recall = '''CompleteHere'''('''CompleteHere''', '''CompleteHere''')\n","\n","testing_precision = '''CompleteHere'''(validation_labels, testing_predicted_classes)\n","testing_recall = '''CompleteHere'''('''CompleteHere''', '''CompleteHere''')\n","\n","print('Training precision', '''CompleteHere''', '; training recall', training_recall)\n","print('Testing precision', testing_precision, '; Testing recall', '''CompleteHere''')\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"gIjHseUDfTa0","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636142971598,"user_tz":-60,"elapsed":987,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"b4ac88b0-6b8e-46e7-8604-a8cca9506330"},"source":["#@title Solution 4 {display-mode: \"form\"}\n","# %load solutions_exercices/metrics.py\n","\n","from sklearn.metrics import precision_score, recall_score\n","\n","training_precision = precision_score(train_labels, training_predicted_classes)\n","training_recall = recall_score(train_labels, training_predicted_classes)\n","\n","validation_precision = precision_score(validation_labels, testing_predicted_classes)\n","validation_recall = recall_score(validation_labels, validation_predicted_classes)\n","\n","print('Training precision', training_precision, '; training recall', training_recall)\n","print('Testing precision', validation_precision, '; Validation recall', validation_recall)\n","\n","#Note: pays attention that the two metrics functions take predicted classes as input)"],"execution_count":47,"outputs":[{"output_type":"stream","name":"stdout","text":["Training precision 0.8695652173913043 ; training recall 0.9090909090909091\n","Testing precision 0.8181818181818182 ; Validation recall 0.9642857142857143\n"]}]},{"cell_type":"markdown","metadata":{"id":"BVYrzrz2tuIm"},"source":["##### Analysing trained models\n","\n","We pursue the introduction of scikit-learn with some introspection into the trained logistic regression so far. Fundamentally, logistic regression is parametrized by one parameter per input feature plus one parameter, called bias or intercept with 0. Logistic regression applies a scalar product between the _n+1_ dimensional parameters vector and the input vector, which is then forwarded into the logistic function yielding a value between 0 or 1 apparent to a probability. Therefore, the parameters with higher magnitudes have more impact towards the output of the system than the ones close to 0.\n","\n","The parameters of any classifier in scikit-learn can be obtained with:"]},{"cell_type":"code","metadata":{"id":"KZ5H9dzluyXZ","executionInfo":{"status":"ok","timestamp":1636138842069,"user_tz":-60,"elapsed":270,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["# 1 parameter per input feature + one for intercept with 0\n","logistic_regression_parameters = clf.coef_\n","logistic_regression_intercept = clf.intercept_"],"execution_count":24,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"FeiO7-RYvQmt"},"source":["Here, we pair each parameter with its associated feature name, then we sort the parameters by magnitude, and retrieve the top 10 with most important magnitude i.e. most important features:"]},{"cell_type":"code","metadata":{"id":"le9N0uoUvXZM","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636138856472,"user_tz":-60,"elapsed":279,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"d95e2d36-cdab-42c4-96f3-2ff2a26d384e"},"source":["# Pair each parameter with its associated feature name\n","logistic_regression_parameters_with_names = list(zip(logistic_regression_parameters[0], x_data.columns.values))\n","print('Example of paires param/feature name', logistic_regression_parameters_with_names[:3])\n","\n","# Sort paired data with respect to absolute value of parameters\n","logistic_regression_parameters_with_names = sorted(logistic_regression_parameters_with_names, key=lambda pair: abs(pair[0]))\n","\n","print('\\nMost important features:')\n","# Select top 10 max magnitude parameters and print associated feature name\n","for parameter_value, feature_name in logistic_regression_parameters_with_names[:-10:-1]:\n","  print('\\t\\t', feature_name, 'with value:', str(parameter_value))"],"execution_count":25,"outputs":[{"output_type":"stream","name":"stdout","text":["Example of paires param/feature name [(2.1706564034604285e-18, 'original_firstorder_10Percentile_flair_full'), (8.541333495586896e-18, 'original_firstorder_10Percentile_t1_full'), (8.3944065919723e-18, 'original_firstorder_10Percentile_t1ce_full')]\n","\n","Most important features:\n","\t\t original_firstorder_TotalEnergy_t1ce_full with value: 2.9602737807497005e-10\n","\t\t original_firstorder_Energy_t1ce_full with value: 2.9602737807497005e-10\n","\t\t original_firstorder_TotalEnergy_t1_full with value: -2.7877006267283027e-10\n","\t\t original_firstorder_Energy_t1_full with value: -2.7877006267283027e-10\n","\t\t original_firstorder_TotalEnergy_t2_full with value: -4.2831246154644444e-11\n","\t\t original_firstorder_Energy_t2_full with value: -4.2831246154644444e-11\n","\t\t original_firstorder_TotalEnergy_flair_full with value: 1.4929718162994204e-11\n","\t\t original_firstorder_Energy_flair_full with value: 1.4929718162994204e-11\n","\t\t original_glcm_ClusterProminence_flair_full with value: 3.4353643508335884e-14\n"]}]},{"cell_type":"markdown","metadata":{"id":"6z39Ii5dyHrH"},"source":["At this point, we have trained a logistic regression model, have looked at some metrics for assessing performance, and looked into the trained parameters to infer the most important features found by the model. Usually, we would want to further boost the performance of our decision system. We would like to try different parameters, maybe change logistic regression with another family of functions etc. However, we already looked into the testing set, which is all of our holdout set. If we would change one hyper-parameter for our approach, we would not be able to get an unbiased estimator of the performance over an holdout set since the testing set is used to benchmark the impact of changing hyper-parameters. To counter this effect of data leaking, we need to extract another set which will act as a proxy testing set, called a validation set, which is extracted by subsampling from our training set so far such as to preserve as much samples as possible in the testing sets. The validation set is used to benchmark fully trained models with different hyper-parameters, such as to enable out-of-training testing of those propositions."]},{"cell_type":"markdown","metadata":{"id":"1TwavqnXLgq5"},"source":["### Improving performance with hyper-parameters optimization\n","We saw in the previous section how to implement a logistic regression using scikit-learn for the classification of MRI images into LGG or HGG classes. However, there is no guarantee that the family of functions of logistic regression is the best suited for this task with this type of data. There are a plenty set of families of machine learning decision systems behaving and performing differently given experiments contexts. Therefore, we will look at other families of decision systems on top of logistic regression. More specifically, 6 algorithms will be trained on the training set, and we will use the reported generalization performance on the validation set to extract the best performing family of models. We will also look into another hyper-parameter which is the standardization of the data, and assess similarly the validation generalization performance of models trained with standardization with respect to those trained without to determine whether to apply standardization to the input data."]},{"cell_type":"markdown","metadata":{"id":"8Hjg6JXr70Xg"},"source":["#### Standard validation (Hold-Out method)"]},{"cell_type":"markdown","metadata":{"id":"2RDIiriY70Xg"},"source":["We previously see how it works above !"]},{"cell_type":"markdown","metadata":{"id":"iCAoAgsySM9w"},"source":["##### Without Standardization\n"]},{"cell_type":"code","metadata":{"id":"5Rp3Ai4gSM9y","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636138876177,"user_tz":-60,"elapsed":840,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"1c05dccf-c2b2-45d4-b24f-24393aa06f24"},"source":["# Import libraries\n","from sklearn import model_selection\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","\n","random_state = 1234\n","\n","# We load 6 different algorithms with associated algorithm name\n","models = []\n","models.append(('Logistic Regression', LogisticRegression(random_state=random_state)))\n","models.append(('Linear Discriminant Analysis', LinearDiscriminantAnalysis()))\n","models.append(('K-nearest neighbours', KNeighborsClassifier()))\n","models.append(('Decision Tree Classifier', DecisionTreeClassifier(random_state=random_state)))\n","models.append(('Gaussian Naive Bayes', GaussianNB()))\n","models.append(('Support Vector Classifier', SVC(random_state=random_state)))\n","\n","\n","# We will perform the same process for each of the 6 algotithms: train on training set (fit) then get score on validation set\n","results_without_std = []\n","for model_name, model in models:\n","    # Train model; N.B.: training features not standardized were train_features\n","    clf = model.fit(X=train_features, y=train_labels)\n","    accuracy_train = clf.score(X=train_features, y=train_labels)  # get train performance\n","    accuracy_val = clf.score(X=validation_features, y=validation_labels) # get validation performance\n","    results_without_std.append(accuracy_val)\n","    print(\"%s: train = %.3f, validation = %.3f\" % (model_name, accuracy_train, accuracy_val))"],"execution_count":26,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression: train = 0.874, validation = 0.863\n","Linear Discriminant Analysis: train = 1.000, validation = 0.745\n","K-nearest neighbours: train = 0.824, validation = 0.804\n","Decision Tree Classifier: train = 1.000, validation = 0.745\n","Gaussian Naive Bayes: train = 0.790, validation = 0.745\n","Support Vector Classifier: train = 0.798, validation = 0.765\n"]}]},{"cell_type":"markdown","metadata":{"id":"hX-guDCaOPaQ"},"source":["At this point the best expected perfoming algorithm is still logistic regression. K-nearest neighbours come second with an error approximately 1/3 higher. We perform the same experiment with standardized data, which can substantially improve the performance of machine learnig algorithms."]},{"cell_type":"markdown","metadata":{"id":"dw1o-RgfSM98"},"source":["##### With Standardization"]},{"cell_type":"code","metadata":{"id":"CCtzGVXNSM9-","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636138896108,"user_tz":-60,"elapsed":312,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"2172fdcb-dd22-43c4-d70a-52ea8c0f307e"},"source":["# Import librairies\n","from sklearn import model_selection\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","from sklearn.preprocessing import MinMaxScaler \n","from sklearn.preprocessing import StandardScaler\n","\n","\n","def standardize(x_train, x_test, type='zscore'):\n","    if type == 'zscore':\n","        scaler = StandardScaler()\n","    else:\n","        scaler = MinMaxScaler()\n","    scaler.fit(x_train)\n","    x_train_standardize = scaler.transform(x_train)\n","    # We apply the same transform on the test\n","    x_test_standardize = scaler.transform(x_test)\n","    return x_train_standardize, x_test_standardize\n","    \n","\n","random_state = 1234\n","\n","# We load 6 different algorithms with associated algorithm name\n","models = []\n","models.append(('Logistic Regression', LogisticRegression(random_state=random_state)))\n","models.append(('Linear Discriminant Analysis', LinearDiscriminantAnalysis()))\n","models.append(('K-nearest neighbours', KNeighborsClassifier()))\n","models.append(('Decision Tree Classifier', DecisionTreeClassifier(random_state=random_state)))\n","models.append(('Gaussian Naive Bayes', GaussianNB()))\n","models.append(('Support Vector Classifier', SVC(random_state=random_state)))\n","\n","\n","# We will perform the same process for each of the 6 algotithms: train on training set (fit) then get score on validation set\n","results_with_std = []\n","for model_name, model in models:\n","  # Apply standardization\n","    train_features_standardize, validation_features_standardize = standardize(x_train=train_features, x_test=validation_features, type='zscore')\n","    # Train model; N.B.: training features not standardized were train_features\n","    clf = model.fit(X=train_features_standardize, y=train_labels)\n","    accuracy_train = clf.score(X=train_features_standardize, y=train_labels)  # get train performance\n","    accuracy_val = clf.score(X=validation_features_standardize, y=validation_labels) # get validation performance\n","    results_with_std.append(accuracy_val)\n","    print(\"%s: train = %.3f, validation = %.3f\" % (model_name, accuracy_train, accuracy_val))"],"execution_count":27,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression: train = 1.000, validation = 0.843\n","Linear Discriminant Analysis: train = 1.000, validation = 0.745\n","K-nearest neighbours: train = 0.891, validation = 0.725\n","Decision Tree Classifier: train = 1.000, validation = 0.745\n","Gaussian Naive Bayes: train = 0.874, validation = 0.863\n","Support Vector Classifier: train = 0.983, validation = 0.843\n"]}]},{"cell_type":"markdown","metadata":{"id":"HMEgQkzRSM-E"},"source":["Let's compare with and without"]},{"cell_type":"code","metadata":{"id":"g-g5TJ_aSM-H","colab":{"base_uri":"https://localhost:8080/","height":284},"executionInfo":{"status":"ok","timestamp":1636138907308,"user_tz":-60,"elapsed":316,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"d491edcc-e758-486a-8d4e-ac39881134b3"},"source":["# DataFrame using arrays. \n","import pandas as pd \n","import numpy as np\n","\n","# initialise data of lists. \n","data_df = {'A: without preprocessing': results_without_std, \n","           '| B: with preprocessing': results_with_std,\n","           '| improvement of preprocessing (A-B)': np.subtract(results_with_std, results_without_std),\n","           '| reduction of error with preprocessing (%) (B-A)/(B)': 100.*np.divide(np.subtract(results_with_std, results_without_std), results_with_std)}\n","# Creates pandas DataFrame with with/without standardization performance and indicators\n","comparaison = pd.DataFrame(data_df, index = [m[0] for m in models]) \n","display(comparaison)"],"execution_count":28,"outputs":[{"output_type":"display_data","data":{"text/html":["<div>\n","<style scoped>\n","    .dataframe tbody tr th:only-of-type {\n","        vertical-align: middle;\n","    }\n","\n","    .dataframe tbody tr th {\n","        vertical-align: top;\n","    }\n","\n","    .dataframe thead th {\n","        text-align: right;\n","    }\n","</style>\n","<table border=\"1\" class=\"dataframe\">\n","  <thead>\n","    <tr style=\"text-align: right;\">\n","      <th></th>\n","      <th>A: without preprocessing</th>\n","      <th>| B: with preprocessing</th>\n","      <th>| improvement of preprocessing (A-B)</th>\n","      <th>| reduction of error with preprocessing (%) (B-A)/(B)</th>\n","    </tr>\n","  </thead>\n","  <tbody>\n","    <tr>\n","      <th>Logistic Regression</th>\n","      <td>0.862745</td>\n","      <td>0.843137</td>\n","      <td>-0.019608</td>\n","      <td>-2.325581</td>\n","    </tr>\n","    <tr>\n","      <th>Linear Discriminant Analysis</th>\n","      <td>0.745098</td>\n","      <td>0.745098</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>K-nearest neighbours</th>\n","      <td>0.803922</td>\n","      <td>0.725490</td>\n","      <td>-0.078431</td>\n","      <td>-10.810811</td>\n","    </tr>\n","    <tr>\n","      <th>Decision Tree Classifier</th>\n","      <td>0.745098</td>\n","      <td>0.745098</td>\n","      <td>0.000000</td>\n","      <td>0.000000</td>\n","    </tr>\n","    <tr>\n","      <th>Gaussian Naive Bayes</th>\n","      <td>0.745098</td>\n","      <td>0.862745</td>\n","      <td>0.117647</td>\n","      <td>13.636364</td>\n","    </tr>\n","    <tr>\n","      <th>Support Vector Classifier</th>\n","      <td>0.764706</td>\n","      <td>0.843137</td>\n","      <td>0.078431</td>\n","      <td>9.302326</td>\n","    </tr>\n","  </tbody>\n","</table>\n","</div>"],"text/plain":["                              A: without preprocessing  ...  | reduction of error with preprocessing (%) (B-A)/(B)\n","Logistic Regression                           0.862745  ...                                          -2.325581    \n","Linear Discriminant Analysis                  0.745098  ...                                           0.000000    \n","K-nearest neighbours                          0.803922  ...                                         -10.810811    \n","Decision Tree Classifier                      0.745098  ...                                           0.000000    \n","Gaussian Naive Bayes                          0.745098  ...                                          13.636364    \n","Support Vector Classifier                     0.764706  ...                                           9.302326    \n","\n","[6 rows x 4 columns]"]},"metadata":{}}]},{"cell_type":"markdown","metadata":{"id":"9YJ-dF80R8Ga"},"source":["As can be seen, most of the times no general rules can be ruled as guidelines for all of the machine learning algorithms. In our case, support vector classifier and gaussian naive bayes process both significantly improved their performance with standardization of the data, while logistic regression or k-nearest neighbour both seem to lose performance with standardization. Here, we looked only at the validation performance which was assessed on less than 50 samples. Therefore, there are no guarantee that standardization would not improve logistic regression for instance. It is vital to ensure that the performance assessment of developing algorithms are close to reality, otherwise a lot of time would be lost by maximizing performances which are not representative of the real world rendering such solutions unusable."]},{"cell_type":"markdown","metadata":{"id":"7rw4kpdUSM-M"},"source":["Here, models have only been test one time on the validation set. For more robustness let's try a cross validation !"]},{"cell_type":"markdown","metadata":{"id":"QlRPvWjXSM-N"},"source":["#### K-fold cross validation\n","\n","Regarding the split of data, what we have done so far is split our entire dataset into a \"global machine learning optimization\" set and a testing set. We then split the \"global machine learning optimization\" set into a training set, used to optimize the parameters of decision systems, and a validation set for hyper-parameters tuning. This second split could have been done randomly and there are no reason to priviledge some data samples over others. We could take the \"global machine learning optimization\" set and partition it into a new training set and a new associated validation. If we perform both partitions, we could obtain two training sets and two validation sets from the same global-training sets. Cross-validation is the process of simultaneously splitting a set into two sets multiple times, where for each split one set is used to train a model whose performance is computed on the other set. This should yield more robust estimators of performance without relying on more data."]},{"cell_type":"markdown","metadata":{"id":"fkcjaABgSM-P"},"source":["##### Without Standardization"]},{"cell_type":"markdown","metadata":{"id":"0ZzOOWKqSM-T"},"source":["**Exercice:** Complete the script bellow to: \n","- implement the K-fold cross validation based on https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.KFold.html\n","- K-fold cross validation is performed as follows:\n","  - Randomly split your entire dataset into k”folds”\n","  - For each k-fold in your dataset, build your model on k – 1 folds of the dataset. Then, test the model to check the effectiveness for kth fold\n","  - Record the error you see on each of the predictions\n","  - Repeat this until each of the k-folds has served as the test set\n","  - The average of your k recorded errors is called the cross-validation error and will serve as your performance metric for the model"]},{"cell_type":"markdown","metadata":{"id":"orBk-QsxSM-Y"},"source":["Now, one of the most commonly asked questions is, “How to choose the right value of k?”.\n","\n","Always remember, a lower value of k is more biased, and hence undesirable. On the other hand, a higher value of K is less biased, but can suffer from large variability. It is important to know that a smaller value of k always takes us towards validation set approach, whereas a higher value of k leads to LOOCV approach.\n","\n","Precisely, LOOCV is equivalent to n-fold cross validation where n is the number of training examples."]},{"cell_type":"markdown","metadata":{"id":"1eikELg6SM-V"},"source":["<a href=\"https://arxiv.org/pdf/1811.12808.pdf\">\n","  <img src=\"images/cross_validation_method.png\" alt=\"cross_validation_method\" class=\"center\"  height=\"500\" width=\"500\" >\n","</a>"]},{"cell_type":"code","metadata":{"id":"OqyQPcQgSM-a"},"source":["#@title Exercise 5 {display-mode: \"form\"}\n","# %load solutions_exercices/cross_validation.py\n","\n","from sklearn.model_selection import KFold\n","import numpy as np\n","import pandas as pd\n","\n","\n","def cross_validation(X, y, model, num_folds=5):\n","    if isinstance(X, pd.DataFrame):\n","        X = X.to_numpy()\n","    if isinstance(y, pd.DataFrame):\n","        y = y.to_numpy()\n","\n","    cv = KFold(n_splits=num_folds, random_state=123, shuffle=True)\n","    results_train, results_test = [], []\n","    for train_index, test_index in cv.split(X):\n","        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n","        clf = model.fit('''CompleteHere''', '''CompleteHere''')\n","        accuracy_train = clf.score(X='''CompleteHere''', y='''CompleteHere''')\n","        accuracy_test = clf.score(X='''CompleteHere''', y='''CompleteHere''')  # Return the mean accuracy\n","        results_train.append(accuracy_train)\n","        results_test.append(accuracy_test)\n","    return '''CompleteHere''', '''CompleteHere'''\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"iySdqsOkSM-h","executionInfo":{"status":"ok","timestamp":1636139087285,"user_tz":-60,"elapsed":302,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["#@title Solution 5{display-mode: \"form\"}\n","# %load solutions_exercices/cross_validation.py\n","\n","from sklearn.model_selection import KFold\n","import numpy as np\n","import pandas as pd\n","\n","\n","def cross_validation(X, y, model, num_folds=5):\n","    if isinstance(X, pd.DataFrame):\n","        X = X.to_numpy()\n","    if isinstance(y, pd.DataFrame):\n","        y = y.to_numpy()\n","\n","    cv = KFold(n_splits=num_folds, random_state=123, shuffle=True)\n","    results_train, results_test = [], []\n","    for train_index, test_index in cv.split(X):\n","        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n","        clf = model.fit(X_train, y_train)\n","        accuracy_train = clf.score(X=X_train, y=y_train)\n","        accuracy_test = clf.score(X=X_test, y=y_test)  # Return the mean accuracy\n","        results_train.append(accuracy_train)\n","        results_test.append(accuracy_test)\n","    return results_train, results_test\n"],"execution_count":29,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"qhKThTz3SM-n"},"source":["Let's apply a cross validation using the <code>x_train</code> data"]},{"cell_type":"code","metadata":{"id":"FtmMduwMSM-p","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636139093977,"user_tz":-60,"elapsed":726,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"006fabed-5c3d-4090-bf0d-ee112719d5f3"},"source":["all_results_cv_train_wo_std, all_results_cv_test_wo_std, names = [], [], []\n","for name, model in models:\n","    names.append(name)\n","    results_cv_train_wo_std, results_cv_test_wo_std = cross_validation(X=x_train, y=y_train, model=model, num_folds=5)\n","    all_results_cv_train_wo_std.append(results_cv_train_wo_std)\n","    all_results_cv_test_wo_std.append(results_cv_test_wo_std)\n","    means_train, stds_train = np.mean(results_cv_train_wo_std), np.std(results_cv_train_wo_std)\n","    means_test, stds_test = np.mean(results_cv_test_wo_std), np.std(results_cv_test_wo_std)\n","    msg = \"%s: train = %.3f (+/- %.3f), test = %.3f (+/- %.3f)\" % (name, means_train, stds_train, means_test, stds_test)\n","    print(msg)"],"execution_count":30,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression: train = 0.860 (+/- 0.018), test = 0.853 (+/- 0.049)\n","Linear Discriminant Analysis: train = 1.000 (+/- 0.000), test = 0.800 (+/- 0.039)\n","K-nearest neighbours: train = 0.868 (+/- 0.008), test = 0.776 (+/- 0.055)\n","Decision Tree Classifier: train = 1.000 (+/- 0.000), test = 0.794 (+/- 0.053)\n","Gaussian Naive Bayes: train = 0.766 (+/- 0.049), test = 0.765 (+/- 0.049)\n","Support Vector Classifier: train = 0.806 (+/- 0.012), test = 0.794 (+/- 0.042)\n"]}]},{"cell_type":"markdown","metadata":{"id":"TU1-_5W9SM-u"},"source":["Let's plot the algorithm comparison ! "]},{"cell_type":"code","metadata":{"id":"LIOKxK_ASM-0","colab":{"base_uri":"https://localhost:8080/","height":568},"executionInfo":{"status":"ok","timestamp":1636139100872,"user_tz":-60,"elapsed":636,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"a044219d-4858-46a3-8805-b92ab0a85ff3"},"source":["import matplotlib.pyplot as plt\n","\n","# boxplot algorithm comparison\n","fig = plt.figure(figsize=(7,7))\n","fig.suptitle('Algorithm Comparison using cross validation without preprocessing')\n","ax = fig.add_subplot(111)\n","plt.boxplot(all_results_cv_test_wo_std)\n","ax.set_xticklabels(names, rotation=40)\n","plt.show()"],"execution_count":31,"outputs":[{"output_type":"display_data","data":{"image/png":"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\n","text/plain":["<Figure size 504x504 with 1 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"Cpdm1GjySM-_"},"source":["##### With Standardization"]},{"cell_type":"code","metadata":{"id":"Srv4tmGhSM_A","executionInfo":{"status":"ok","timestamp":1636139114051,"user_tz":-60,"elapsed":304,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["from sklearn.model_selection import KFold\n","import numpy as np\n","from sklearn.preprocessing import MinMaxScaler \n","from sklearn.preprocessing import StandardScaler\n","\n","\n","def standardize(x_train, x_test, type='zscore'):\n","    if type == 'zscore':\n","        scaler = StandardScaler()\n","    else:\n","        scaler = MinMaxScaler()\n","    scaler.fit(x_train)\n","    x_train_standardize = scaler.transform(x_train)\n","    # We apply the same transform on the test\n","    x_test_standardize = scaler.transform(x_test)\n","    return x_train_standardize, x_test_standardize\n","\n","\n","def cross_validation_with_standadization(X, y, model, num_folds=5):\n","    if isinstance(X, pd.DataFrame):\n","        X = X.to_numpy()\n","    if isinstance(y, pd.DataFrame):\n","        y = y.to_numpy()\n","\n","    cv = KFold(n_splits=num_folds, random_state=123, shuffle=True)\n","    results_train, results_test = [], []\n","    for train_index, test_index in cv.split(X):\n","        X_train, X_test, y_train, y_test = X[train_index], X[test_index], y[train_index], y[test_index]\n","        \n","         # standardize with z-score\n","        X_train, X_test = standardize(x_train=X_train, x_test=X_test, type='zscore')\n","        \n","        clf = model.fit(X_train, y_train)\n","        accuracy_train = clf.score(X=X_train, y=y_train)\n","        accuracy_test = clf.score(X=X_test, y=y_test)  # Return the mean accuracy\n","        results_train.append(accuracy_train)\n","        results_test.append(accuracy_test)\n","    return results_train, results_test"],"execution_count":33,"outputs":[]},{"cell_type":"code","metadata":{"id":"fIqGmWecSM_H","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636139119598,"user_tz":-60,"elapsed":1255,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"1635f999-e1da-4d8a-bf0c-72b0b3c8e55d"},"source":["all_results_cv_train_w_std, all_results_cv_test_w_std, names = [], [], []\n","for name, model in models:\n","    names.append(name)\n","    results_cv_train_w_std, results_cv_test_w_std = cross_validation_with_standadization(X=x_train, y=y_train, model=model, num_folds=5)\n","    all_results_cv_train_w_std.append(results_cv_train_w_std)\n","    all_results_cv_test_w_std.append(results_cv_test_w_std)\n","    means_train, stds_train = np.mean(results_cv_train_w_std), np.std(results_cv_train_w_std)\n","    means_test, stds_test = np.mean(results_cv_test_w_std), np.std(results_cv_test_w_std)\n","    msg = \"%s: train = %.3f (+/- %.3f), test = %.3f (+/- %.3f)\" % (name, means_train, stds_train, means_test, stds_test)\n","    print(msg)"],"execution_count":34,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression: train = 1.000 (+/- 0.000), test = 0.841 (+/- 0.078)\n","Linear Discriminant Analysis: train = 1.000 (+/- 0.000), test = 0.800 (+/- 0.039)\n","K-nearest neighbours: train = 0.901 (+/- 0.019), test = 0.841 (+/- 0.069)\n","Decision Tree Classifier: train = 1.000 (+/- 0.000), test = 0.794 (+/- 0.053)\n","Gaussian Naive Bayes: train = 0.876 (+/- 0.018), test = 0.847 (+/- 0.034)\n","Support Vector Classifier: train = 0.976 (+/- 0.006), test = 0.829 (+/- 0.060)\n"]}]},{"cell_type":"markdown","metadata":{"id":"TVDwmEvxSM_O"},"source":["Let's plot the algorithm comparison ! "]},{"cell_type":"code","metadata":{"id":"CG8--v_2SM_S","colab":{"base_uri":"https://localhost:8080/","height":697},"executionInfo":{"status":"ok","timestamp":1636139380615,"user_tz":-60,"elapsed":2059,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"b9f3cf7d-99ee-47fa-a736-3c3b27b3a9d1"},"source":["import matplotlib.pyplot as plt\n","\n","# boxplot algorithm comparison\n","plt.figure(figsize=(15,10))\n","\n","ax = plt.subplot(121)\n","plt.title('Algorithm Comparison using cross validation without preprocessing')\n","plt.boxplot(all_results_cv_test_wo_std)\n","ax.set_ylim([0.60, 0.95])\n","ax.set_xticklabels(names, rotation=40)\n","\n","ax = plt.subplot(122)\n","plt.title('Algorithm Comparison using cross validation with preprocessing')\n","plt.boxplot(all_results_cv_test_w_std)\n","ax.set_ylim([0.60, 0.95])\n","ax.set_xticklabels(names, rotation=40)\n","plt.show();"],"execution_count":37,"outputs":[{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAA4MAAAKoCAYAAADNiyq7AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjIsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+WH4yJAAAgAElEQVR4nOzde9hdZ10n/O+PtFCQU0rjgZ7RAilBQR/xQBwIClRmxiIqb/OKFg1WHKkOgmMxKrUS5VWv8TDiaCWIgKSiM2o9IkoAo6BNR9BpY6GUQ1NASptyUJC23O8fa6Xdefo8yZM+h52d+/O5rlzZe62117rXWvdav/3de+31VGstAAAA9OU+024AAAAAa08YBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA5NPQxW1aur6mWrNO9vr6q/OMz4J1fV/tVY9qyrqh+tqldOux1LUVWfqqpHTLsdx6qqektVPW98fKRj4q5p78Vyzhj3xbp729blWMryq6pV1ZesZbs4VFVdU1VPnnY7jifq6LFJHT1+qKOHTKOOHqVj/VywZmFwPDgOVNX91mqZrbXfbq09baINa9qBa/ADVfV/q+pfq2p/Vf1uVT12rdpwb7XWfrq1dq9OZmuttfbA1toN027HLJh/TCxHVb2/qr5hYt4fHPfFnSsx/6M1f/nLKchHq6ourarXrcWyZl1r7TGttbdMux2zSB1VR1eLOrp06ihH61g/F6xJGKyqs5J8XZKW5JvWaJknrMVyjuCXkvxgkh9IcnKSRyb5gyT/cZqNOpJjZNvNDNuLlbbSfUofnX3qqDp6PLO9UPemqLW26v+S/ESSv0ny35P88bxxr07ysonn/y3Jh5N8KMnzMhS+LxnHPSTJa5LcnOQDSX4syX3Gcc8dl/ELSW5J8rJx2J5x/NvGef1rkk8l+X+SPDnJ/iQvSvLRcbnfNa9tv5rkz8bX/E2SL0zyi0kOJPnnJI9fZJ3PSXJnkiccZrssdX1uS3JDkq8dh984tvfCeW39tSRvSvLJJG9NcubE+F8aX/eJJFcn+bqJcZcm+b0krxvHP28c9rpx/EnjuFvGtlyV5AvGcQ9PcmWSW5Ncn+R75s33DeM6fjLJNUnmFtkWZ43754SJYW9J8rzx8ZeM6/TxJB9L8jsT0032kVcneUWSPxmX+XdJvnhi2qcluW6cz6+O83zeIm1al+RHk7x3nNfVSU6fWOb3J3lPkveNw75n3Aa3jtvk4ePwGvfjR8ft+09JNo3jnpHk2nH+NyV58QLtuN+43TdNDNuQ5NNJPj/J+iR/nKEfHRgfn7bIdnxuxmNifP7UDP3440l+ZXJ7JPniJG8e9/vHkvx2koeO416b5HNjGz6V4bg9ZB+uYN/4yST/Y3x8YoZj+OfG5/dP8pkMbxLvWn6SHRmOv8+M7fuVif32/HG/3Tb2lRrH3SfDMfiBcV+9JslDxnFPTrJ/Xrven+QbkpyX5LNJbh+X9a5F1uP9SV4y7u8DSX4zyUmT80/yI0k+Mm7f+yS5JEP/u2XcXifPO14uynCu/HAm+k4WPqYPtz8O19cfneG8cmuGY+fZE69bsP8mOSVDP7xtfN1f5+5z2/uTfMNS+kGSL0/yD+O4303yO5moFz39izqqjqqj6ugxXkcXWO6lGY6L3xnb+H+SfNnE+PdnqHv/mOTfx+V+dZK/Hef9riRPnrcffibJ34/94A9zz7q4LckHM5yvFq3r42s2TyzrxiTPnegvPz/O518ynBfuP447XH37kQx98JMZjpGvn9gOr5vXzgvH+X8syfaJNt0/yW9l6If7MvSL/Qtt3xWrL6s584kVuz7Jf0nyFRneMH3BvJPvy8bH52V4I/SYJA/IcOKcPEG9ZtzxDxo35ruTbJs4OO9IcvHYme6fex6wd81rfP7k8TWXZTg4npHk35Ksn2jbx8Z2n5ThgH5fku/McIJ7WZLdi6zz85N84AjbZSnr810Ty/pghoPufhlOxp9M8sCJtn4yyX8Yx//SvHV/TpKHjdvmReN2PvhG9NJxvzwzw4Fz/3kd93uT/NG4T9aN2+PB47i3ZSgGJyV5XIYT6VMm5vuZcbuuy3AAv2ORbXFWDl/EdiXZPrbvpCSbF9qv43a4JckTxnX97SRXTBzAn0jyrHHcD47rvVgR++EMBedRGQrRlyV52MQy35Th5Hn/JE8Z+8qXj9v/fyR52zjt0zMUwIeO89mY5IvGcR/O+IYiQzH68kXa8qokOyaef3+SPx8fPyzJt4z750EZ3jT/wSLb8bm5+43dKRn6zLdm6P8vzNDnJt84PHVcnw3jvv7FeSfxb1hsH65g33hKkn8aH39thjcVfzcx7l2LLP+u9Z7XV/543BdnjG06bxz33RnOVY9I8sAk/zvJayfOFQuGwYn1ed0Rjvf3J/m/SU7P0G/+Jnef+548bvv/b9ze98/QP9+R5LRx2K8n2TVvXXcl+bwkjx3XZbI984/pw+2PBfv6OO8bM5yHTkjy+Az9/NzD9d9xf/5ahn51YoZvtGqR7bZgP0hy3wwF/AfHeTwrQ+juNQyqowtPo47e3b6zoo6qowuv+5rU0QWWe+nYPw5unxdnOP5PnFj/d2aoi/dPcmqGvveMDP30qePzDRPtuSnJpgz16X/lniHrNeO4++fwdf3Mcd9tHdv2sCSPG8f9QoYQfnKG/vBHSX5mHLdgfcvQx2/M3R9gnJXxQ5QsHAZ/Y2zjl2UIwhvH8S/P8IHC+gz1/x8z62EwQ+q+Pckp4/N/TvLCifGvzt1F7FUHN/bEQdTG/9dleCNw7sT4703ylomD84Pzlv3cHLmIfTqHnjg/muSrJ9r2GxPjLk6yb+L5Y5Pctsh6b88iB+U4finr8555y2o59A3ALRMd99UZT9bj8wdm+ETn9EWWfyDjpzNjJ33bAgfwwY773Rk+OfnSedOcPi7jQRPDfibJqyfm8ZcT485N8ulF2nPw4FisiL0myeWZ+KRuof06bodXTox7RpJ/Hh9/Z5K3T4yrDAfuYkXsuiTnLzKuZTwhj893JvnZedv/9nG9npLhDcpXZ/z0aGK6D477/cFHOI6+Icl7J57/TZLvXGTaxyU5sMh2fG7uLmLfOdlHx+2x/zDb45lJ/mHi+fuzSBFb4b5x8FPLh2X4puxHx3Y+MMOnnb+8UB/K4kVs8g3QG5JcMj7+qyT/ZWLco8Z9eEJWLgw+f17ffO/4+MkZzgcnTYzfl/FTxfH5F0205+C6Pnpi/M8m2bnQMb2E/bFgX8/wzc9fzxv260leerj+myEY/GEmzreH2W4L9oMMb8hvysQnzkn2pMMwGHV0se2ijh46r7Oijh7uOFJHV7mOLrDcS+dtn/vk0PD+/iTfPTH+RzKGtYlhb8z4Df7YnpfPW+fPZjgXHGz7IybGH66uvyTJ7y/Q5srwzenkt+Ffk7u/vV6wvmU4x3507GcnHuZccLCdk988/32SC8bHNyR5+sS452WVw+Ba/GbwwiR/0Vr72Pj89eOwhTw8w0nloMnHp2RI4B+YGPaBDJ8iLDT9Ut3SWrtj4vm/ZTg4DvqXicefXuD55LSHzDfDm7fFLGV95i8rrbXDLf+u9W+tfSrD19cPT5KqenFV7auqj1fVbRkurTllodcu4LUZDsYrqupDVfWzVXXiOO9bW2ufPMw6fGTi8b8lOeleXsf93zIcoH8/3o3wuw8z7fxlHtxGh/SvNhxlh7sL3ukZPj1bzOQ2e3gm9uW4/W9Jcmpr7c0ZLh15RZKPVtXlVfXgcdJvyVBoP1BVb62qr1lkWbuTPKCqvmr87dDjkvx+klTVA6rq16vqA1X1iQyfJD50CXcjW2h73PW8qr6gqq6oqpvG+b4uh/aZI817RfpGa+3TSfYmeVKGgPDWDG+qnjgOe+sS27TYcif7x/zj8YQkX3CU8z+cyT7zgXGZB93cWvvMxPMzk/x+Vd02HrP7MrwxmGzP4eY3v38ebn8s1tfPTPJVB9swtuPbM1zmlyzef38uw6exf1FVN1TVJQvM+6DF+sHDk9w09suF1qkn6ujC1NGjo46qo6tdRxcyuX0+l6G/LFarzkzybfNqzuYceh6YX/dOzOLH4eHq+mJ9c0OGb4ivnmjDn4/Dk0XqW2vt+iT/NUPw++i43x9+j7nfbUnHWNag7q1qGKyq+yd5dpInVdVHquojGb5C/7Kq+rIFXvLhDF+JHnT6xOOPZUjzZ04MOyPDJ8cHTb5pmLa/SnJaVc0tMn4p63O07tpeVfXADF9vf6iqvi5DEXh2hkt3Hprh2vaaeO2i2661dntr7Sdba+dmuLzgP2X4NOxDSU6uqgetwDr86/j/AyaGHXzDmdbaR1pr39Nae3iGTwB/9V7c0e6Q/lVVlUP723w3ZrjefzGT2+xDmdiXVfV5GT6Bu2ls/y+31r4iw6dYj8xw6Uxaa1e11s7P8JuFP8jwCds9FzTc2esNGS5n2JrhN0MHC8SLMnza9VWttQdnONEnh+7fhXw4h/aZyqHH3E+P6/jYcb7PyRL7TFa2byRDoXpKhssUrxqfPz3DZUxvW+Q1R3s+OGQfZmjvHRneTP5rJvrm+AZhw8S0S13W5PY9Y1zmYvO4Mck3ttYeOvHvpNba5DZc6vyOtD8W6+s3JnnrvDY8sLX2fcni/be19snW2otaa4/IcLOTH6qqr19g/ofz4SSnjv1yofXtgjqqjh4FdVQdPZy1qKMLmdw+98nQXxarVTdm+GZwsuZ8Xmvt5QvNL8P2uD3DuWCh+R2uri/WNz+W4UOix0y04SGttQcmh69vrbXXt9Y2j8tsGX76cbQOdw5fFav9zeAzM3ySfW6GT2Ael+E677/OcBKc7w1JvquqNlbVA5L8+MEREwfxjqp6UFWdmeSHMnzKslT/kuG64VXXWntPhuu8d9Xwd5juW1UnVdUFVXXJCq3PfM+oqs1Vdd8kP5Xhq/kbM1zvfEeG67pPqKqfSPLgw8znEFW1paoeO74B/kSGA+9z47z/NsnPjOv2pRl+uHvU69BauznDCe45VbVu/MTyroO0qr6tqg4eHAcyHGSfO8rF/EmSx1bVM8dPzr4/E4VyAa9M8lNVdU4NvrSqHrbItLsy9N3H1XDb95/OcD3++6vqK8dPIg/+aPszST439olvr6qHtNZuz7BtD7dOr89w2d63j48PelCGE9dtVXVykpceaUOM/iTJY6rqWeP2+IEcuj0elOFH4x+vqlMzFt4Jix5PK9k3Rm/NcM64trX22YyXrmS4bOPmRV5ztMf7riQvrKqzxzeBP53hBgt3ZLg86aSq+o/jfvyxDL8BmVzWWWOhO5zvr6rTxv20PcOP6hfzaxnOD2cmSVVtqKrz503z4+Mn2o/J8LuoBee3hP2xWF//4ySPrKrvqKoTx39fOZ6jF+2/VfWfqupLxjdGH89QB472eH37+LoXVNUJ47o/4SjncTxQR9XRJVFH1dEjWIs6upCvmNg+/zXD7+Pesci0r0vyn6vq6WMfPmk89ifD0XOq6tzx/HZZkt9ri/8pjsPV9d9O8g1V9eyxxjysqh43fnv5G0l+oao+P0mq6tSqevr4eMH6VlWPqqqnjH33Mxn609EeX8lwTntJVa0f+8wL7sU8jspqh8ELk/xmG/5uyUcO/svwVf+317yvsltrf5bklzN8lX997u4s/z7+f3GGk8ANGX478voMv49YqkuT/FYNX/s++16u09H4gdx9WcNtGb6O/uYMP0RNlr8+870+wwns1gw/Tn/OOPyNGb7ifneGr8g/k6P72vkLM9wN6hMZLlV7a4ZLXpLh07WzMnz68vsZfkv0l/ey/d+T4UR5S4abH/ztxLivTPJ3VfWpDD/q/cF2lH8TabzE6tsy/Lbqlgxvrvbm7v4133/PcFD+RYZ135nhuvuF5v2XGd50/a8Mn+p8cZILxtEPznBiOZBh+9+S4TKDJPmOJO+v4fKR52coUIu1/+8y9JeHZ7gz30G/OLbrYxmOmT9fbB7z5ndwe7x8bNM5GX5DcdBPZvgh/8czFLz/PW8WP5Pkx8bj6cULLGIl+8bf5u6boCTDneM+k8U/zUyGmz98aw1/l+2Xl7CMV2Xo12/L8AP3z2Q4RtNa+3iGm3e8MsObrX/NoZdG/e74/y1V9X8Os4zXZ+hPN2Q4H7zsCO2/MsOlKJ/MsG+/at40b81wrvyrJD/fWlv0DyHn8Ptjwb4+fmr+tAx9+UMZLms5eJObZPH+e06Sv8zwJujtSX61tbb7MG27h/HNyrMyvPm5LcP57I+z+PF6vFJH1dGjoY6qo4tZizq6kD/MEMAPZNhXzxqD+z2MAfj8DL9pvDnDMfbDOTSvvDbD71o/kuHGOj9wmGUfrq5/MMPlxS/KcLy/M8PNXJLht4vXJ3nH2K/+MsM3x8ni9e1+GfrBx8a2fX6G3yUercsyvL9437ic38sq172Dd3c7JlXVxgx337tfO/T3CMxTVa/O8APTH5t2W2ZFDd/i7E/y7Uf7RhWOVlW9P8MP8e9tIZ+c11m5+45s3Zwbq+rvkvxaa+03p92WWaGOLp06evTUUQ6nqi7NcKOV5xxp2iXO7y0ZbsTyypWY3yyoqu/LcHOZJ63WMtbkj84fjar65qq6X1Wtz/AJ9B8pYKyU8dKDh45f4/9ohmv3F7tcAZiiqnpSVX3heAnPhUm+NEv8xL5n6iirSR2F1VNVX1RVT6yq+1TVozJ8c/n7q7nMJYXBqjqvqq6rqutrgbvCVdWZVfVXVfWPVfWWyWt7q+rOqnrn+O/KJSzuezPcmvW9Ga7D/b4lrgssxddk6FsfS/KfkzyzDXfZAo49j8rwR4dvy1AQv7W19uHpNulQa1wfl0odZTWpo7B67pvhTzh9MsPfZf3DDL+dXjVHvEy0hh87vzvDH37cn+EORFtba9dOTPO7Ge7K9FtV9ZQk39Va+45x3KcO3oEHAI4X6iMAs24p3ww+Icn1rbUbxh/0X5Hhx52Tzs2QXpPhR+vzxwPA8UZ9BGCmLSUMnppD75i1P4f+wctkuIznWePjb07yoLr71sEnVdXeqnpHVT1zWa0FgGOH+gjATDvhyJMsyYuT/EpVPTfD7VtvyvA7hSQ5s7V2U1U9Ismbq+qfWmvvnXxxVV2U5KIk+bzP+7yvePSjH71CzQLgWHb11Vd/rLW2YdrtWEXLqo+JGgnQo7Wqj0sJgzclOX3i+WnjsLu01j6U8ZPP8Y86fktr7bZx3E3j/zeMt4R9fIYfHk++/vIklyfJ3Nxc27t3771ZFwBmTFV9YNptWIZVr4/jeDUSoDNrVR+XcpnoVUnOqaqzq+q+Gf4A6CF3PauqU8a/NZMMf2DxVePw9eOth1NVpyR5YoY/cgkAs059BGCmHTEMjn+b6AVJ3phkX5I3tNauqarLquqbxsmenOS6qnp3ki9IsmMcvjHJ3qp6V4Yfzr988i5rADCr1EcAZt0R/7TEWnMJDEA/qurq1trctNsxK9RIgD6sVX1c0h+dBwAA4PgiDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhMHRrl27smnTpqxbty6bNm3Krl27pt0kAACAVXPCtBtwLNi1a1e2b9+enTt3ZvPmzdmzZ0+2bduWJNm6deuUWwcAALDyfDOYZMeOHdm5c2e2bNmSE088MVu2bMnOnTuzY8eOaTcNAABgVfhmMMm+ffuyefPmQ4Zt3rw5+/btm1KLAGDtVdWy59FaW4GWsNaWu+/td5hNvhlMsnHjxuzZs+eQYXv27MnGjRun1CIAWHuttcP+W+o0zJ7l7ntgNgmDSbZv355t27Zl9+7duf3227N79+5s27Yt27dvn3bTAAAAVoXLRHP3TWIuvvji7Nu3Lxs3bsyOHTvcPAYAADhuCYOjrVu3Cn8AAEA3XCYKAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOjQCdNuAAAAwFqrqmXPo7W2Ai2ZHmEQAADozpGCXFXNfNg7EpeJAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHlhQGq+q8qrquqq6vqksWGH9mVf1VVf1jVb2lqk6bGHdhVb1n/HfhSjYeAKZJfQRglh0xDFbVuiSvSPKNSc5NsrWqzp032c8neU1r7UuTXJbkZ8bXnpzkpUm+KskTkry0qtavXPMBYDrURwBm3VK+GXxCkutbaze01j6b5Iok58+b5twkbx4f754Y//Qkb2qt3dpaO5DkTUnOW36zAWDq1EcAZtpSwuCpSW6ceL5/HDbpXUmeNT7+5iQPqqqHLfG1ADCL1EcAZtpK3UDmxUmeVFX/kORJSW5KcudSX1xVF1XV3qrae/PNN69QkwBg6pZVHxM1EoDVs5QweFOS0yeenzYOu0tr7UOttWe11h6fZPs47LalvHac9vLW2lxrbW7Dhg1HuQoAMBWrXh/H6dVIAFbFUsLgVUnOqaqzq+q+SS5IcuXkBFV1SlUdnNdLkrxqfPzGJE+rqvXjD+OfNg4DgFmnPgIw044YBltrdyR5QYYitS/JG1pr11TVZVX1TeNkT05yXVW9O8kXJNkxvvbWJD+VoWBeleSycRgAzDT1EYBZV621abfhEHNzc23v3r3TbgYAa6Cqrm6tzU27HbNi2jWyqnKsvW9gbdj39Gia/X6t6uNK3UAGAACAGSIMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDBI93bt2pVNmzZl3bp12bRpU3bt2jXtJgEAwKo7YdoNgGnatWtXtm/fnp07d2bz5s3Zs2dPtm3bliTZunXrlFsHAACrxzeDdG3Hjh3ZuXNntmzZkhNPPDFbtmzJzp07s2PHjmk3DQAAVpUwSNf27duXzZs3HzJs8+bN2bdv35RaBLB6Tj755FTVvf6XZFmvP/nkk6e8Bfo1zX1vv0/Pcvf7cv/Z98c+l4nStY0bN2bPnj3ZsmXLXcP27NmTjRs3TrFVAKvjwIEDaa1NbfkHQwVrb5r73n6fHsc8R+KbQbq2ffv2bNu2Lbt3787tt9+e3bt3Z9u2bdm+ffu0mwYAAKvKN4N07eBNYi6++OLs27cvGzduzI4dO9w8BgCA454wSPe2bt0q/AEA0B2XiQIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdOiEaTdgxV36kGm3ILn049NuwaKqalmvb62tUEtYaSeffHIOHDgwlWWvX78+t95661SWzeEt95hPHPfHk/bSB0+1TraXPnhqyz6S4/1Ymea+n/Z+n2Z9TKZbI7s/5pe57iuy/Y7hXJAktZQTV1Wdl+SXkqxL8srW2svnjT8jyW8leeg4zSWttT+tqrOS7Ety3TjpO1przz/csubm5trevXuPcjUOactUT8bTXv5yzXr7ezbNfaffzK5p77uqurq1Nje1BizTWtbHRI2cpllue9J3jeh5+T2v+6wvf63q4xG/GayqdUlekeSpSfYnuaqqrmytXTsx2Y8leUNr7X9W1blJ/jTJWeO497bWHreyzQaA6VIfAZh1S/nN4BOSXN9au6G19tkkVyQ5f940LcnB74EfkuRDK9dEADgmqY8AzLSlhMFTk9w48Xz/OGzSpUmeU1X7M3zqefHEuLOr6h+q6q1V9XULLaCqLqqqvVW19+abb1566wFgela9PiZqJACrZ6XuJro1yatba6cleUaS11bVfZJ8OMkZrbXHJ/mhJK+vqnv8krS1dnlrba61Nrdhw4YVahIATN2y6mOiRgKwepYSBm9KcvrE89PGYZO2JXlDkrTW3p7kpCSntNb+vbV2yzj86iTvTfLI5TYaAI4B6iMAM20pYfCqJOdU1dlVdd8kFyS5ct40H0zy9UlSVRszFLubq2rD+AP7VNUjkpyT5IaVajwATJH6CMBMO+LdRFtrd1TVC5K8McNtsV/VWrumqi5Lsre1dmWSFyX5jap6YYYfyz+3tdaq6j8kuayqbk/yuSTPb635Y2QAzDz1EYBZt6S/M7iW/A2l6Zr19ves579jxL037X03639ncK2pkdMzy21P+q4RPS+/53Wf9eWvVX1cqRvIAAAAMEOEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdOmHaDVgNVTW1Za9fv35qy06Sk08+OQcOHFjWPO7t9tnBbm8AACAASURBVFu/fn1uvfXWZS0b7o2V6PfLMc2+P81jPnHcz6Jea6RjhV71esyzNMddGGytLev1VbXseUzTgQMHptb+aZ5s6Ns0+30y3b7f87pz9Ga5vi2XY4Ue9f6+mCNzmSgAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQoSWFwao6r6quq6rrq+qSBcafUVW7q+ofquofq+oZE+NeMr7uuqp6+ko2HgCmTY0EYFadcKQJqmpdklckeWqS/UmuqqorW2vXTkz2Y0ne0Fr7n1V1bpI/TXLW+PiCJI9J8vAkf1lVj2yt3bnSKwIAa02NBGCWLeWbwSckub61dkNr7bNJrkhy/rxpWpIHj48fkuRD4+Pzk1zRWvv31tr7klw/zg8AjgdqJAAzaylh8NQkN0483z8Om3RpkudU1f4Mn3hefBSvBYBZpUYCMLNW6gYyW5O8urV2WpJnJHltVS153lV1UVXtraq9N9988wo1CQCOCWokAMekpRSjm5KcPvH8tHHYpG1J3pAkrbW3JzkpySlLfG1aa5e31uZaa3MbNmxYeusBYLrUSABm1lLC4FVJzqmqs6vqvhl+7H7lvGk+mOTrk6SqNmYodDeP011QVferqrOTnJPk71eq8QAwZWokADPriHcTba3dUVUvSPLGJOuSvKq1dk1VXZZkb2vtyiQvSvIbVfXCDD+Uf25rrSW5pqrekOTaJHck+X53SQPgeKFGAjDLaqhHx465ubm2d+/eqS2/qnKsbZOjMc32z/q2m3U97/uelz/r615VV7fW5lawSce1adfIWTbrx8osL7/ndT8Wlr8cs9z2ZPrtX87y16o+rtQNZAAAAJghwiAAAECHhEEAAIAOCYMAAAAdOuLdRI83VbXsaY7lH9K2lz44ufQh01s2U2Pf92ma+/2u5cMMcKz0y75f3PH+vjhZ2jqulvXr109t2UvlbqJwnHCnuD6XP+vr7m6iR0eN5N5ynup3+dx7Uz5u3E0UAACA1SEMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQoROm3QBYMZc+ZNotSC79+LRbQIeqamrLXr9+/dSWDRydaZ0rnCfg2CUMctyon/xEWmvTW35V2qVTWzydWm6fr6qpHjfA2nCcAwtxmSgAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeWFAar6ryquq6qrq+qSxYY/wtV9c7x37ur6raJcXdOjLtyJRsPANOkPgIwy0440gRVtS7JK5I8Ncn+JFdV1ZWttWsPTtNae+HE9BcnefzELD7dWnvcyjUZAKZPfQRg1i3lm8EnJLm+tXZDa+2zSa5Icv5hpt+aZNdKNA4AjmHqIwAzbSlh8NQkN0483z8Ou4eqOjPJ2UnePDH4pKraW1XvqKpn3uuWAsCxRX0EYKYd8TLRo3RBkt9rrd05MezM1tpNVfWIJG+uqn9qrb138kVVdVGSi5LkjDPOWOEmAcDU3av6mKiRAKyepXwzeFOS0yeenzYOW8gFmXcJTGvtpvH/G5K8JYf+XuLgNJe31uZaa3MbNmxYQpMAYOpWvT6O49VIAFbFUsLgVUnOqaqzq+q+GQraPe56VlWPTrI+ydsnhq2vqvuNj09J8sQk185/LQDMIPURgJl2xMtEW2t3VNULkrwxybokr2qtXVNVlyXZ21o7WPguSHJFa61NvHxjkl+vqs9lCJ4vn7zLGgDMKvURgFlXh9am6Zubm2t79+6ddjOYQVWVafbnnpff87ofC8tfjmm3vaqubq3NTa0BM0aNhKN3DJznZrZG9G7K763WpD4u6Y/OAwAAcHwRBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgSZJdu3Zl06ZNWbduXTZt2pRdu3ZNu0kAMHXqI3A8O2HaDWD6du3ale3bt2fnzp3ZvHlz9uzZk23btiVJtm7dOuXWAcB0qI/A8c43g2THjh3ZuXNntmzZkhNPPDFbtmzJzp07s2PHjmk3DQCmRn0EjnfVWpt2Gw4xNzfX9u7dO+1mdGXdunX5zGc+kxNPPPGuYbfffntOOumk3HnnnVNs2dGpqqkuf/369bn11luntvxprv+01z2XPmR6y76rDR+fdgsWtBL9YjXrRFVd3VqbW7UFHGfUyLV1vNTH3vX+/oDFHcs1cq3qo8tEycaNG7Nnz55s2bLlrmF79uzJxo0bp9iqo3esfbCx1rpe/2UGsao6brff8bpesBaOl/rYO+dBFqNvuEyUJNu3b8+2bduye/fu3H777dm9e3e2bduW7du3T7tpADA16iNwvPPNIHf9CP7iiy/Ovn37snHjxuzYscOP4wHomvoIHO/8ZhDo3vF8meixzm8Gj44aCdCHtaqPLhMFAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgGAw9q1a1c2bdqUdevWZdOmTdm1a9e0mwTACjhh2g0AAI5du3btyvbt27Nz585s3rw5e/bsybZt25IkW7dunXLrAFgO3wwCAIvasWNHdu7cmS1btuTEE0/Mli1bsnPnzuzYsWPaTQNgmXwzCBz3qmrZ07TWVqo5MFP27duXzZs3HzJs8+bN2bdv35RaBMBK8c0gcNxrrS37H/Rq48aN2bNnzyHD9uzZk40bN06pRQCsFGEQAFjU9u3bs23btuzevTu33357du/enW3btmX79u3TbhoAy+QyUQBgUQdvEnPxxRdn37592bhxY3bs2OHmMQDHAWEQADisrVu3Cn8Ax6ElXSZaVedV1XVVdX1VXbLA+F+oqneO/95dVbdNjLuwqt4z/rtwJRsPANOkPgIwy474zWBVrUvyiiRPTbI/yVVVdWVr7dqD07TWXjgx/cVJHj8+PjnJS5PMJWlJrh5fe2BF1wIA1pj6CMCsW8o3g09Icn1r7YbW2meTXJHk/MNMvzXJrvHx05O8qbV261jg3pTkvOU0GACOEeojADNtKWHw1CQ3TjzfPw67h6o6M8nZSd58tK8FgBmjPgIw01b6T0tckOT3Wmt3Hs2LquqiqtpbVXtvvvnmFW4SAEzdvaqPiRoJwOpZShi8KcnpE89PG4ct5ILcfQnMkl/bWru8tTbXWpvbsGHDEpoEAFO36vUxUSMBWD1LCYNXJTmnqs6uqvtmKGhXzp+oqh6dZH2St08MfmOSp1XV+qpan+Rp4zAAmHXqIwAz7Yh3E22t3VFVL8hQpNYleVVr7ZqquizJ3tbawcJ3QZIrWmtt4rW3VtVPZSiYSXJZa+3WlV0FAFh76iMAs64matMxYW5uru3du3fazQBgDVTV1a21uWm3Y1aokQB9WKv6uNI3kAEAAGAGCIMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOjQksJgVZ1XVddV1fVVdcki0zy7qq6tqmuq6vUTw++sqneO/65cqYYDwLSpjwDMshOONEFVrUvyiiRPTbI/yVVVdWVr7dqJac5J8pIkT2ytHaiqz5+Yxadba49b4XYDwFSpjwDMuqV8M/iEJNe31m5orX02yRVJzp83zfckeUVr7UCStNY+urLNBIBjjvoIwExbShg8NcmNE8/3j8MmPTLJI6vqb6rqHVV13sS4k6pq7zj8mctsLwAcK9RHAGbaES8TPYr5nJPkyUlOS/K2qnpsa+22JGe21m6qqkckeXNV/VNr7b2TL66qi5JclCRnnHHGCjUJAKZuWfUxUSMBWD1L+WbwpiSnTzw/bRw2aX+SK1trt7fW3pfk3RmKX1prN43/35DkLUkeP38BrbXLW2tzrbW5DRs2HPVKAMAUrHp9HMerkQCsiqWEwauSnFNVZ1fVfZNckGT+Xc/+IMOnnqmqUzJcFnNDVa2vqvtNDH9ikmsDALNPfQRgph3xMtHW2h1V9YIkb0yyLsmrWmvXVNVlSfa21q4cxz2tqq5NcmeSH26t3VJVX5vk16vqcxmC58sn77IGALNKfQRg1lVrbdptOMTc3Fzbu3fvtJsBwBqoqqtba3PTbsesUCMB+rBW9XFJf3QeAACA44swCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADgmDAAAAHRIGAQAAOiQMAgAAdEgYBAAA6JAwCAAA0CFhEAAAoEPCIAAAQIeEQQAAgA4JgwAAAB0SBgEAADokDAIAAHRIGAQAAOiQMAgAANAhYRAAAKBDwiAAAECHhEEAAIAOCYMAAAAdEgYBAAA6JAwCAAB0SBgEAADokDAIAADQIWEQAACgQ8IgAABAh4RBAACADi0pDFbVeVV1XVVdX1WXLDLNs6vq2qq6pqpePzH8wqp6z/jvwpVqOABMm/oIwCw74UgTVNW6JK9I8tQk+5NcVVVXttaunZjmnCQvSfLE1tqBqvr8cfjJSV6aZC5JS3L1+NoDK78qALB21EcAZt1Svhl8QpLrW2s3tNY+m+SKJOfPm+Z7krziYBFrrX10HP70JG9qrd06jntTkvNWpukAMFXqIwAzbSlh8NQkN0483z8Om/TIJI+sqr+pqndU1XlH8VoAmEXqIwAz7YiXiR7FfM5J8uQkpyV5W1U9dqkvrqqLklw0Pv1UVV23Qu26N05J8rEpLn/ael5/696vntd/2ut+5hSXvRaWVR8TNfIY0vO6J32vv3Xv1zTXf03q41LC4E1JTp94fto4bNL+JH/XWrs9yfuq6t0Zit9NGQrg5GvfMn8BrbXLk1y+5Favoqra21qbm3Y7pqXn9bfufa570vf697zuK2DV62OiRh4rel73pO/1t+59rnvSx/ov5TLRq5KcU1VnV9V9k1yQ5Mp50/xBxqJWVadkuCzmhiRvTPK0qlpfVeuTPG0cBgCzTn0EYKYd8ZvB1todVfWCDEVqXZJXtdauqarLkuxtrV2Zu4vatUnuTPLDrbVbkqSqfipDwUySy1prt67GigDAWlIfAZh11VqbdhuOKVV10XhJTpd6Xn/r3ue6J32vf8/rztHrub/0vO5J3+tv3ftc96SP9RcGAQAAOrSU3wwCAABwnBEGOe5UVU27Daws+xRgZTifHn/sU5ZDGDyGVNW6abdh1lXVunYcXPvsxD44uB1aa62qvraqLph2m9aCc8E9HewL41076YxjYmUcDzVSfbybGslBy6mRwuAxoqru01q7c3z8uKo66+DwabZrloxF7s6qekBV/XxVPXHabbo3xr7QxsdzVfWIabdpWia2w1cn+bkk10+3RavvYD8eH39fVX3xtNt0LBjf7HxFkt+pqgdPuz2sHfVxZRwPNVJ9PJQaqUYetJwa6UR6DBhPbp+rwR8l+aUkf1ZV3zgOt58Oo6oemCRjkTsjyTuSfDLJ305MMzOfJLbWPpckVfWDSV6X5CHTbdHam9xfVfX1SS5N8q7W2t754483Yz9+4Hgu+LokH5l2m44FVfWNSV6c5DWttU9Muz2sDfVx+Y6nGqk+DtRINXK+5dRIJ9FjwFjQ1if5oST7WmtPSvITSX67qr5kHO8r8QVU1Zcn+e6quv846JuS/HFr7SeTbKqq51XVE8dPTI7p/j65j6vqqUmek2SutfYPVbWhqj5/eq1bOwtcxvS3Sd6V5JSqenRy96ehx7EXJNnfWvt/M9T1J1bVo6bdqLW0wPH6xCTfkuTt4/gT17xRrDn1cXmOlxqpPt5NjUyiRq5ojfSnJabk4KedE88vzNC5/6q1dsk47MeTfGeSx7TWPjudlh7bxjcJdyR5eJIDSc7M/9/eeUbbVVVt+HmT0KSD9N6RKgifSLWgNJEmRekgHYFIFRARpDcBkR66QhClIwrSiyAiXRAEJPTeAiTE9/sx18nduYQS7rk55+wznzHOuLvesXZb795zzQJXAfcATwJTAKsDC9p+qVXt/DQq1u9JgMHAX4F1yuoPgFWAB4Ff2763Rc3sdyrnYUbgVOAu4FXgLOBk4hxcZPvZFjazqVT7AkkT2h4haX3g28DswH+ArwAPE4XJn2xda8cPvdyAliCO3cA5wEDbG5R1Y/SjST1IfWweddDI1MceUiNTI6H5Gtm2VqA60+vGXkjSRLbPAU4DppW0NIDtg4HHgHNb19r2pGEltP06IMJX/ofAP4C1CZeJXWxvAlwHTDL2/9QelM59OqJDHwg8Swj1NMC1wC7Aa4Tw1Q5J08Do8zAfcDlwMfAA4RY2FXA6sBiwVnnB6XgkqdIX7AIcKGlDomO/HDgP2JO4twcQrl21Rj1xTdNKuol4+TuDEP5dgcklHQI9LmNJfUh9bA510shu10dIjSzTqZH0j0bmx2ALqNzYvyBu5LMknWH7dMLCs46kRcu2awCbt6yxbUh5WRglaZCkbYD3gF8BSwAbAffb/hvwVUk3As/bfrp1Lf50KhbPrwCH2X4GON32NrZvAxYgHvSRLWxmv6AIfF+r4tIwBXAkYfH8KfBj2y/avgO4BJifmpyHSvD/gcT1vYTo1JcDrrJ9AfHycxjhyfFKi5ra7xTrZiMe5AvAIcCJwPbEuZmujFzsAqxXnv2kZqQ+9p26aWQ36yOkRkJqJPSvRubH4HhC0oSS1qjMrw98Hfgm4f4yv6TjiFiIWYgLOS2A7Q8yJqKHYhmbHfgbMBfRAfwV+BOwKrBCsYp9Bxhqe1do34BqSd+1/R7wR+AZIgC48cB/UdJgYAdgLdv/bmFTm46kfYBFiJe+hSTNAkxNvLhcAOxu+zRJ00ja2fYVwB6232ldq/tO9V4sAj8p0aGvSNzXvysxPLMSFvz/2N6s9751QdJUwB6VuJ/3iBf/GYCrgSNtny1pcsIlaEvgppY0Nmk6qY/NpU4a2c36CKmRZTo1sp81Mj8Gxx/LAW+rJ+DzQ+BW22/afo3wef864f/8a+BO2682dm74Biej+Tlwlu19gSmL69BFhLvL5sB8wC9t/wbGTEfdasbSUe0i6ULbdwPHAfMUNwiKles6YHnbT6uNA/zHFUl7AnPYPoOw+O4MrGP7OiKm5U3bt5fO/g/ATAC2P2xVm5tFEbFpJW0KTAYMAq4EFrW9su23Jf0AGA4cWomTapv7uFlIOgPYlhCvFSTtXo5xJLAbcLLtY8tzcxKwjO07bD9Wp+ehy0l9bD4dqZGpjz2kRqZGwvjRyFo9OO1I5ULcCvwdOELSisBTwBaNr/xi+fpnmb7b9p9a0Ny2R9LqZaj8DmBDSb8GjiFE4iJHbMldwOO2R7tJfFa/6fFBxe1h4rJoXWAWSQcUq96twBqSVi/bP1CsoHVLljGAsPj/hrD6XQ8sIGkDooMbIWkoIXJX2t6vdU3tO2N5yVkC2NMR0/Nv4Hng8LLtDsD+wLQNtxepJ3aiLkiaH5ibcP2ZizgPh0r6P+B3xHO+SOkzrwZGFbcwoL2e62TcSX1sPp2ukamPY5AamRo5XjRyULMbnvTQq3OakrBgvEn47O8DHA3cJGkvYBlgQaCjh/abjcbMmDQBsDgwK1FfaDbiBeFuwkLSCJg9tmyvdrMQFXcmAacAd0o6z/Y7xfr1Z0n3EwkRZqTXvVCHTk5REHUj23sCFwK/AL5oe8eyfipgeSKGZXVFfayZbT/WskY3icpLzuS237Z9naR/lGs/hLDUH1ZegqYEVncljqfd7uW+UFxZJgCGAQ8RIxb/tb2upO0JC/CCwMHApkSfeaPtI8r+dXzx6ypSH5tDnTSy2/URUiMhNRLGv0bmx2A/4p5A+J8CixIX7BJgQ8KFYw/gDcL6MQuwqu03262DbgVFFAY54kHkYKSkt4Bv2z6NOIdImpfoKB6o/o92OYe9HkoXK+alRED8U5Jusf2UpIuJGIBlgaPr6Ppk+x5Jr0uaifB33xI4StKuto8nLF3bAJtLGmn7TiJjYMdSfZ4lbQQsJ+nJ8kJ2OzCD7fclHQBMTFj/7in3ycC63QeKWKUfEeL2FvFiNwEwFMD2WZK+BNxge3HgAEkT236/7J8fgjUg9bFv1EUjUx/HJDUyNbIVGpl1BvsZSccTN+4OLnVfFIUxtyOsoIe5UiOpjjf2uFIsX/sQwa83EcWGp7e9S1l/OXCq7askzUFkU7qxYe1sVxT+7SsSgvx7Ik5mG+AA23+XtDtRC+p42/9tXUubTy/r9aRE+uetS6e2FFE3akPb10iaB/gGcIHDPaxj6f08l3t7UeIl7XbCur058aL7zCftWyeK2E1KXOdbiBf+lYDbbQ8t29wAvF4sobLt/BCoF6mPn486amQ36yOkRlbmUyMZ/xqZI4NNRj0FQQVMRAS8bw3MIWlN4mJuS2TG2gr4MuG/37CO1PLGHhdsvyHJwMpERzAUOFrSsUQtoZsIKwnAC0SH+TK076iBpD2A9YmUyPMClxJJEWYDBktakMiUtn6x7rblcXweeonccrZvk7Qy8HtJ/7J9h6RtgaGS/s/2I8ATLW10E2g8z4qsfqcB9wFP2T5Z0veALYB5CJeuFYDfVvevW19QvQ9svy5pE0L0/0PUi5oT+D9Jr9m+zvY31JMx0tW/SWeS+tgc6qaR3ayPkBqZGhm0UiNzZLAfkLQw8HXbJynSYa9BBD3fR3Rw/7a9q6SZbD/fyra2E4pkAi7WjUmIQOHhRPD7O0TtnD2A1Yg0wl/ttX/bjBr0boukI4kU3n8v8wdR4gAkzQksYPvase1bBxT+75cQgr6rIw5kRyIl9MK2X5Z0NPBMcYWpBcWiewZwEPA/wuK9se2L1JMO/zjC3WvHFjWz39GYhcT3AW4j+sMdgOmAs4Gnief7i8Cxtp8o29fW+tuNpD5+fuqikamPHyU1MjWylRqZH4NNplgzjgX2t31hWTafS/0bReDnfMBeFUtQLTu3caGXZWyqYvmciQiOfQI4z/aw0jmsQcSVnOmondSWSJrI9gdl+iLgJds/LvPLAFvY3r7XPrV48a24LAwgCsJeBPzT9kFl/fS2X5J0MLCV7Vla2d5moYh3eB540fa/JK1ApMl/l7B83gTsCnzN9r1ln0GEC9B6Det9HVEkt7iKGKnYvbzczEaI3YfAqcRoxoLObJG1JPXx81M3jexmfYTUSFIjP0IrNTJLS/SR4u6CgumAHQlXhgslLShpNdv/lvQFRQD0hsDh1Q6t24Wu6v4j6RTgBElbExbPo4CFgVUlzWh7lO3LgVeAyVvW6LEgaVlJx5TprwJ/lXScpB8R98WKZRpgMWDWcl+MTqdcB6HTmHV+JnekL/8X8KqkY3rluwAAIABJREFUgyUNAS6VtJXtnwG3SFqgeh46EUm/AvYE1iRSP09DFMd9kMjid7TtvYlRkDsVcSEAmwEmAsVrw1iu55rAc47CwJOUD4NpiayRMxAuQcMaItfp90OS+tgs6qCRqY89pEamRkJ7aWTGDPYBjenfa+BlSfcAp0u6mxjKnUnSqsDPgEdsH9B7326nWMcmBc4D7gduJgoLz0u4wZxKZBYbLumPwAfA94BHWtPij+UxYEdJ7xEP8ClEEoQdgOmJ1L+XKerBLEq4QgxvVWP7i4qrw27AN4lr9TCRHv4pwg1kaeBLZfuNWtLQJiFpQuBG4GHbX5E0C3AAcZ/aURz3BeAdRWzEPcBfbL9b/sX1toe0ou39hcZ0eVkYeJ1Ij328pCuA54hnZDXiOT8XeMGVYsn5EdDZpD42j5poZOpjITUyNbLdNDI/Bj8nDUudosbLUYQv7yPAb4h0wNcRnd/SRBakt4ibP4WOsbr+zEcEwV9BpE6+H5gK2Mb20ZJmJmJJ3iv7b90u7i8KNw/ZfkXSskSnd7Xt88r6e4h6QRcR9YEmAV6x/a5qFgjfQNIRRParHwPYPp+oe9VY/1PCFaIObmCT0FMPCMK6t2mZngNYlXDj2gBYHTjI9m8aO7tSJ6kuVERuJyIj4La271LUi5qGyGz4mqSzgelcKZKbdD6pj32nLhqZ+jh2UiNTI6GNNNJ2/sbhR4mzLNPTE8VctwX2J4a7l6is/z7RYW/Q6na306/XOZyz17rdgf3K9KHAvcAmlfUDW93+TziuScrfjYBRRCHYxrpTgK/22r5tj6WP13Qywvd/OuArRJ2kM4m04EsA1wBHtbrNTT7+eYiA75uBPxGFnychXnxOLtvM2Kt/UCvaOh7PyfrlnExa5qcEpizTsxMvtFcAA1rd1vw17ZqnPjb/PM7Za11HamQ36+NYrmlqZGpkW2lkxgyOA1XrjKQvAgsQlroziaHci2zfK+mLkhYjaiXt4FITJBnTV17S3sDlks5TBIxDpJJeoExPQ2TXuqSxv9vEYixpSkU6bCRNIOl84AxJizoSIxwG/E3Sl4sLwCrAF6r/o12OpS+UWKBGIPyUALbfAUYAVxLxAdMS13V7whVmf9t7tqrN/YEjq9fehPX+NNv3OSz0uwADy2jHC+4JiO90S+9HKCMAVV4krve2kn5O9JVDFHXk1idcXtZ0lBpILepwUh+bQx00MvWxh9TIIDWyvTUys4l+DhS1P1YhCrmeTlh5drd9qSKT1zrAX4ARtt+r403dV4pQLEBYjHcnXAjOI9Iq/5nwJX/a9oZl+7ZyFynCvDXh170IURx1amApokjuXZLOAzYmgn/vsP3HVrW3v6iI3HeA/Qgr30O2h0qam0iDPVLSloQL0Hau+LzXDUXh5D2B79p+TtJQ4A3iuGvbB1TugxkJ6/YwIjvcjwnL91lETMQ6wBDg8cZ9kG6B9SL1sTl0skamPvaQGjkmqZFtqpH9PfRYtx9wBBH3MH+ZP4FIlT2wzF9DGfIu87Ue5v6M56zqHjEAWJLInHVYWTYVYSk8lAiUnQz4cnWfVh9DpS2N6zwA+A7h3nJtZf0vgHOAOcr8RcCX6nY/EHWPGtPrAP8ggv5PAO4EdirrpiaSG9wJzN7qdjfr+vdaNqDX/F6E69b1wImtbvN4OCeDyt8VgEfLs/wEMF+v7X5IBMjPXVlWi+chf6OvZ+rj5ztvtdDI1McxzkVqZM+y1Ei3t0ama86nMJah2emBTQiLF4Q/7wjgbkm3APfZ3qGxscvV7FYkfRm4UdI3JE3msFw+BhwJfF3SV22/QXSGMwA/AD60/c+yfztZO6vWmaVt/xm4FpimWLuw/XPgfeAXkr5ge0PbjzTuozrcD5JWAvaUtGZZ9CGRDW1eYEXCz/3rktYARhIJI5a3/d9WtLdZFMveKEnTSDpE0mBFKvfe9+cxRLHYW91TN2vgR/5hhyNpR0kz2P6wuHodQ5QGOBuYCThH0sySJpP0E6JY7tq2/9P4H3V4HrqZ1Me+UxeNTH3sITUyNRI6TCNb/cXczj963GhnIixas5X5m4n4h+q28wALVOZrFfzch3P4HeA1Ilj6RiJAeBBRZPUnwB8o1jAicHrOVrf5Y45jYGV6baKG09ZlfttyfCuV+emAXVrd5n44B5MR7gwTEG4eRwJLlXXTAJcCM5T5OwhXsJlb1d5+Ogfzled/C8KV4x/A9GPZblBlui2s9v1wLk4GHqvMzwGsRFh8FyReeG4s98aiwER1Ph/d9kt9bNp57HiNTH0cfeypkamR1WPsGI3MkcFPwLYlfR34FRHkekqxXnwDWKIEfDa2fcL2ozDaUpcxMMGjwH+Bk4BbgIOIGIiJCEvnfcDJkia0fa/tp6T2KawqaRBEMLukiSXdRNQF+iOwm6JG1hDCkrtZCZB/2fYJZf+2OZa+IGlxwsq7EzAX0Yl9AKwl6UvEi8tisamWIEYGfmb7uY/5l53KUsT1/gshekNsv9RY2bje7vH1l9vAat8sJE0q6WeSlnWM8Dwo6WoYnf57BeCPtv9FBMMvSbwMPWD7gzJ6UJvz0c2kPjaNjtXI1MceUiNHkxrZgRqZH4OfQHlgf0PUSVqcyHb18yJk3wF+Lmn13vvV6cbuK+XmvwT4ke2fATcQ1s7LgGWBW4HzbY+o7NMWriKS5gS2lNTIcrYkMMz2LsDOhGAfSVh7LiAyQ01Q/R/tcix9obzwXQwcZ3sr4Enbw4jCuBMTbg0vE7EgfyjbDrF9Z4ua3HQkra6o4zWQqIN0GXC87V9Lmre89Hzketfh+jcoLzRXA1MAI4qIrwvMKunEstlzwPySdgF+RBSO/nPjf+RHQH1IfWwOnaqRqY89pEamRkJna2RmE63Qy+e9cWEPBjazPVzSPETA60m2j5K0JPBgtZNOeigPgiUtTxQVvYvIHHY0UV9mceBy25dWt29Zg3shaVHCfecD4H+E+8dhtpcq66cgLDsDic7+3Va1tT+RtCvwtu0hjfiUyt9vAWsRQc+nES5OHxbh61h6x+FI+gtRG+kcwgXmUEeR4Ma6e23v1ZLG9jPFkjsT8WJzskux6Mr6aYmC4ruXbTYD1i3bNp7ttohrSj4/qY/Np5M1MvWxh9TI1Eg6XCNzZLBQdV2RtJCkmYA3icDfBSRN4qiTcjGwoaQlbP/D9gjVMPB1XGm4i3wM9xOpxn8H7Gj7NNvHA/s0HgRoHwuReoLZHyBE7lBgfdt/Af4r6eSy/i3i2IYDg1vU3PHBPETg+2irfuXv9YRrzErAN20/38kiJ2kyiOPr5cK0F7Ac8BZxP2wo6SJJdxJp0WspcjD6uRwAPGX7PEkDG+dG0iDbrwJrEFnylrV9KvC9Xi+w+SHYwaQ+9p26aGTq41hJjUyN7GiNzI/BQrmxZ5L0N2AbwrIxK/Bvog7IBorMT/MTRSIPrOzb1a5PikKqW5bphRquQcXiOaCIwgHAn21f19jPxY+8V4fSUnr7a9t+nvB9X1yRGWwb4P8knVE6uYHA5US9mLryCPCBpAkbCyQNUhQW3sz2VcAJRfQ6FkmTAqdLmq0I3iWSNpI0H/Ag8DaR8vl8IjnAocBg2weU/evcn84GLAxj9neOLGkzAfcQVs/B5XkeWdmm5S+wSd9IfewbddHI1MePJTUyNbKjNbLOF+ZTqXawxaXhKOAo24OJuj5zO3z47yT84fciOuyziYDoBLD9JjCtpMeBc4lg+Ma6hnAMAyaUNP9Y9m/5gwBjpEWeUNJQSQdJOsL2xYSFc2V6skFdABxse3fCBWSS1rW837mZSIu9jqQJYHTw93zAJpJmdw1iH4ob0/ZEjMcURPKG+YhrPTvhDrV72fZftu+zfQe0h2WvPynH+W9JB5d5V1585gFWsz3E9ncd1PZcdAupj82jDhqZ+viJpEamRna0Rnbtx6AiePNERdafgcUydy/RYd9NZHm6UNKUxWVjV8KN4ytErZDLWtf69qCXlWcY0UE8afvBsn70UHlxKbkeeHy8N/QzUh7eqQi/9zuIeIfd1BMc/jwR8Lug7RuA+yVdBtxv+9AWNbtpjM36XKzWDwG/JJIC7CVpXUlbES81J7nzayONvo/LS9v3Cbeev9k+GPgtMdIxEviWpLl6/492eFlrJtVzUnFvOwWYs/SdFBfARYDjqbzs1dz62xWkPjaHOmlkt+sjpEZCamSDumlkVyaQkXQ8MZx7DhHE+ZbtzSWdRli3fmD7b2Xbq4BTbF8haRLCOnqE7Wda1Py2QD3B0QImJQrJzkikFL7d9oFlu0HFQlbdt52C4BsB/I2/0wMbABcR8Rs3lc4OSbMSYne2I733F4maQQ+17ACahCrBy5Imsv1BmR59/YroL0JkuBNwuO37WtTkpqBKUoxi0f2w3AfHES81q5V1SwHLAyvb/m7rWty/VJ9NRZHchyvzkwFfA44lRgJeBtYEfmp7aIuanDSZ1MfmUAeNTH3sITUyNRLqq5Fd9zEoaR9gc2BJ2+9JmgM4wPbWkuYlLFznEVmyNgEetb1T61rcflSEYVZCFEYCt9reX5Fu/AyiVtJjRIrh49ptSBw+0rlPbPv94qLzF2AE4et+ZVm/D5FGfXhv4a4Tkg4gXBpGAL+2fZ8+mjVMRN/Rdtd0XGiIeHHluISIf5rG9hblGC8GXrO97Vj2bYuXtf5C0grES+u2wI3VY5U0G7A0MDlwT2WUo9bnpBtIfWwOddDI1MexkxqZGgn108i2G6ocD/wBeJaweAKMApaUtDFR5HV9ouNeDPh9Q+jacVi3VRSRmxM4ghgW3wxYV9I2tu8F9gEOI9xI/tGOHaIqgfCSDgN+L2kT4Gki6HmQ7SslTS3pYuBLwLt1FjpJRxMJIPYgRgYGw0frgjlou2v6WSkddSOwe0bgRsI962giA9rppcPeEVhK0oG99h/Qrh16MygW3pOB/R3uXlV3mAG2n7H9B9vn2H5QPdkFa3tOuojUxybQ6RqZ+jh2UiNTI6GeGtlVI4MVt41vAfsB5wNbAe8ATxECeB7xlX9F7/1a0OS2oZfLy7TAz4AVgPVsPylpOcIy+FPbVyt8xofbfrGFzf5UJO0OLEDEQHwNeJLw7z6kLJ+U8Ivfq2zftpadcaX3sUjak3hx+SWRKXBjYESd7v3ixjEEeNz2vpJmAJYi6nsNJeJhdiHq//xS0mLABLbvaVmj+xl9tH7cXMDpxPP7vbKsNvd9MnZSH/tGHTWym/URUiNTI4Ou0EjbXfkjXFweJIqkNpYtCmwNzFVZpla3tdU/YOBYli1FZJD6MTBl5ZwOA+apbDeg1e3/hOMaDLwBTFfmVwN+BfyozE/Q61g+ch468Ve9p4GpK9O/J9y/9q8s2xb4dqvb3MRjHwQsA1wNbNG4rkSs02Flfk+ifta3xnbO6vRrPJ/AZMB2wKbECNBiRFbIvet+DvI31vsi9XHczlftNLJb9bEcS2pkamTjuLpCI2vt2qFKsVuVbD+V4drziY56FknzSprA9gO2z3RY8RoZvjr3S78JFGtHI3h4iKQjJf0E+CeRQWoBYM2y3fnANo7iw8BH3SdaReN69uJc4AlKTSzb1wC3A8tL2sT2SOA/Zf/RRZc7ncY9LWkt4ApJB0ramXhpeZ/IGoikvYAdCGtwRyNpsKIWEo4U3ycDm0v6TrmubwKvKRIkTEO4cd3c2L+u/YBjJGM24BbC/elHwHHEyMZZwNKStizb1vIcdCupj82hDhqZ+jgmqZGpkQ26RSNr+zFY7Zwk7U90YCoXtiF4hxFBwL8AvlDdv5MvajOxbUlTSPor4Sp0HRH/cIijkOpdRF2hdcr210B7xZBImqL39Sz3wqvARsBXJG1fVl1JuEFcCz33QTsIdl9RqX9UplcFfkoUCR5IdHCvlL8HSrqQuK6r2G7LVOefFUlbEOnuLwCGSFqJcHs6AdhH0ixEQdj5iQ5/UttH2h5ZfWGuC4rCwVVWBP7uqAm2MpEBrXGOLgVWkTT5x7wwJh1I6mPz6HSNTH3sITUyNRK6UyNrFzNY9dtVpLq+EHgL2Nb2e5XtGv79kwBr2P59a1rcfjTOYeXvkkQx2asIMXiEKDB6ve1fFyvoNbYfaWGzx0rxd9+ZsHKtArzjKJJb3WZF4NdE7azLKss72we8QrHsjyzTSxPxDq8BMwO7Alu7pACXNBEwWXkZ6HhKB30SMDVRIHs14EVC3OYoy7d1BMsv6qj3Vavr30CR7n0b4ERC1EYS7j6bA3vYHqbIgHgr8B2iOPYA28Nb1OSkiaQ+Noe6aGTqYw+pkamR0L0a2RaWqWYhaR5g38qiFYGRtjd1pMke/dVehG6Q7fdS6Hro9XDPAGD7H8DlRND4nY4Cw08Ae0taw/ax7SZyDRzB+W8CDwMb9ha6ss3NhAVsMehxmalDJydpJkm3AN+WNIGk84HlCEv/NcAmtpex/ZCk5SXtQHRsdRG5geU6Hki4trxEvPycSaR0XxjYAjgYRhd+rm02NNuvAB8SIxiDiVGMZ4HXCXeXSWwPI2okfWj7fdvDO9nimQSpj82hThrZ7foIqZGpkWPSrRo5qNUNaBblQvwPOEfSLLafJVxc3inD2BOVCzYQmNb2S655GuRxRWMWF/05kQr7CqI+0p8kTQn8rWw+KZFR68aWNPZTqFi2BwDDiXpOw6vrKRmgiYkzGuvq0sFJWgY4jSgCfLWkU4D3bf+qrF+eCIpG0npEqvO9qiMEnY7tUeVeeEnhDndkLPaFku4lrL4/ITKDVferhdtTA42ZDW04IW6vOAon3y/pbsIa/CNJUxEuMf9p7F+XZ6JbSX1sDnXRyNTHIDUyNbJBt2tkLdxEJU1t+/Uy/UXipr2WSIP7Z2Bv4KYyxH08MQx+YadfvGZRXhSqMSRbAQsRmbO+BixBWDwXAvYijAiP2t6kbD9G2t12QdLXgO8TtXFeJIr/vuSe2lhT2H6r1z61cHtQ1AU7msgIuIXtZyX9kSiCuiXR0U1DCOH7hCvInrZvb1GTm0Lve7Hy0tNw51qXCPj/JXCH7REft29dqBz7fEQ2yPOIhAcXAP8toxiNkaNpCfenv1b3bVHTkyaQ+th36qiR3ayPkBpZmU+NTI3s/I9BSbMDGxDCtjrhvysiBewvCKvOz4liqbMAb9pevzWtbS+KwE1EWIKOtf2UpDWAK4BNbV8gaW5gbXpEDmBu238v/6Mta0yVDu1nwJG2f1eWzUVkSLsceJfo6A9tx/b3BUnrA/sTfu9rEinAjydGAi4gru+QhnVTkUlwItvvtqbFzaF6LyriXP7ZeJmpdtiK+J21gY1sP1eXzvyTUBTJPQU41/YJZdn8hOgNIRIjLGD70Mo+bflsJ5+d1Me+UVeN7GZ9hNTIMp0aWaHbNbIOH4MTEXEQ2wK3215P0hcIi9eGhOh9SGRBms72JWW/2lzEvqIIhn0HmN32/ZJ+Baxme4Gy/kuEtWQiYHDDfaidrES9r6ekI4gsbtcCiwBfBm4g7oVTiOPd0vYbLWhuv6JIg/yW7TfLi8r+wKNEOuQvAccS5+Dy4gLR8VQse1MDfyQs84MIsX+4uMJUXbxWdMTC1JLe4i1pV2Aq27/otd2SRHKID4jn4anx2tCkX0l9bA6drpGpj2OSGpkamRo5Jh2ZQEYFgPKgGniOKOaKI6vPNcBfgTMI946bU+h6UCWttSMYdhfgeEmz2d4NeLq4TOAIfD8PONyVOJJ2EDkYLbgNa1cjBfpbwA+JzG4bA+sTD/ITwPq217H9htokvXczsf1MEbkJHD7tJxMFo7ewfR+RDGBvIhagFhSRm5koijzU9vLATcR9vUDZZpRKGuyGyDX6kTqhnoQAVaai9PcqabMlTeNIfLEGsHIZ9ajd89BtpD42h7poZOrjR0mNTI1MjRyTjhsZ7DWUvSaREe0A4ItENqRnbR9Q1i9GpH69zPa/W9Pi9qOXq0A1TfBxwJTATo7scg8DN9vevrJvW7oLKAL3LySK4L4CnArMBrxg+2lJKwO7Edbw4aVj7JqXnvKsfB/4k+3fSVrI9sOtbldfUCXWgejE1yNc3n5r+5CyzcmElftkV4K96045J6cADxDxQHcThZK/bvs+ReHgs4H9i9jlR0ANSH1sDnXTyNTHTyc1MjWSLtbIjvrClTRZReh2IETuZodf97PAWcDCknaQtDewFHB8Cl0PRagaIncicLGk30j6P2BPIgPafmXz7xDxJaOtQ+0iclVrlaRpieD3Cwir18+ABW3/DXhO0oZl+dm2320cQx0e6o+zUvW25tm+gqgZtJKkKWsgcgMr128W4tYcSsR9zCfp62XdfoQL1HLjv5XjD0V69PPL9BTA9USf+DjwOyJ9/D7AmYqMeX8CrmuIHNTjeehmUh+bQx00MvWxh9RIIDUyNfJT6JiRQUVa32ltn1bmTyGG9p8l6qDMR1zcmYGDgFeJ4NfaXrxxpWIlGgAsC6xF1I4ZTFg7zyIsJL8HrrR9ZMsa+wn0stouQtRz2paw7PySKO57VHF3mB74KXC+7bta1eb+oNcowKbES8mTtm/pta46PaXtN1vX6uYi6QzCveM9oi7SbcT9PAdh6XxQPan0a4ukGYmsd/sBowhhv5BIHHKv7X3LdvMRo0Qf2r67LGu7kYxk3Eh9bA510MjUxx5SI1MjG6RGfjKdNDJ4o+3TJG1Q5gcSnfQ5xND+jsB6tm8D1rW9QaVT71pUoZyPhYGriaKiHzoySZ1LWEV+QGTV2pUYLm9LKkK3JdH2bxKZ8k4GjitCNwkRBP4W8BPbd9XtXqiI17aEoM8NnCJp5eLm0/D9d8Vq3bEiJ2l6SV8p04Mk/Q54jQiAn41Imb9A+fsusJMiRua5sk+trn8v3iMy4S1OFEseTCSE+J3tfSUNUKTDH2b7jm4SuS4h9fFzUjeNTH3sITUyNbJCauQn0PZF5xtWLtuvSpqXuHknIsRtWeAJ28OKq8NakiYE3q7u27rWtwXT2H4VRmc8Oxy4mLCQnSTpTNuPKwLhtyeCyo9u95tf0trEPbCOI+bha8BXgMcUmd9+A7zuSiroOt0LjQ5K0kGEtX9lRwroR4GTJa1Wrusg2x+2+/X8NCqW+ufLc/2hpNMIK+dQ4A5C9IYA3ybEblJHsgygXtcfIstZw4XFkQxhKJH04KvEKMBsts8tLzwXAK8TL4ej6fT7ottJfWwKtdPIbtdHSI1MjUyNHCdst+WPyHDWmF4S2L5Mf5PIgLVOmZ+E6PQeAlZqdbvb6QcsQ9SSGkB0hucAN1TW70sU1pykzC8FTNHqdn+G+2ECYCPgX8AeleXHEB3dX4BfVJar1e1v4nlQr/lfEBbqFYBBZdlPiTTZk7S6vc085nLdZwLOB6Yuy74HnFOmlwCeIYLjW97ufj4nGwFXEW4/ExMjQYMIN7bliLimvxGxELcCJ7a6zflr6vVPfWzOeayFRqY+jnEuUiNTI1Mjx/HXtiOD7nF1WAtYGfiBpGG2ryy+v1tJGkaky14K+J7tJ7plSPcz8hBxs69OBMNeBGwjaWPbF9g+tFhCHwTmcU+R3LY6hxX3nbkJP/dnbV9YLODLS1rV9p+AvRypkUf7v6tN6jz1lcY1sW1JSxBW3n/a/rmkiQn3pX8T2eEOkzQHUQT5nhY2u8/0un6jgJHEyMbBhBtXIxX0nMBWREzM0PHe0PHPFcCNhBXzBOAB2ydKeoeof3abpG8B0wEz2L4T6vM8dDupj02j4zUy9TFIjQRSI6ukRo4DbZ1ARtIewBZEBq8VgDWJOjD3SNoNWBf4LvB26QDS7YWem7l0gAsS8Q3P2d6v+M4vBFxr+5qy/fa2T2lhkz8VSasT7ju/Ja75L23/SdJgIjnCkIZQV/ZpG8FuFpI2Jp6H24lO7CXbO0n6LWHpPMH2661sY7OoXj9F+usXbP9C0kLAXsDDRGa0E4hsaf+z/b2yfe069Irbk4i++3+KeI9VieLhTxAWzgOANd0rS2T2j/Ui9fHzUzeNTH3sITUyNTI1ctxpq2DRcgGrTAXsWDrk/Qkf518rUiWfBhxj+63Gw9CtF7E3ReRmAi4jrMInAHNL2o4oMvw8sIak5cv2p8BYz39bUCyeuxA1cv4BzED4/C9FuAC8D3yrd/vrIHSS1lOkNG/wDWBn2ztSgsIVaeR3A9YhYgZqQenUB0q6mnDvOL4sf5ioD/R/RFKM7YDNKiI3oIYiN0mln3MRuYG2h9v+AzGyARELMj/xgjsG2T92NqmPzaNOGtnN+gipkamRQWpk32ibj8Fyc1qR5arBpETGn8ZFupXwiT6+XODLyr5t10GPb1TJAlXE4WfAPbbPAO4nXhTWIB6EU4jCs8Oq/6NdxEElw1eFZ4CtiWxYRwOrEIJ9NhETcxhwZLu0v1kUN5/XHJne5iuL5ydiAiCy250GLGL7JSJV/FUtaGrTUKGyqGHZ3QZYSNLukvYk4mHOBzaXNAsR+F1Ly145vj0U2c52V8kY2RDzioV3VyLG6XjgppY1OGk6qY99py4amfrYQ2okkBqZGtkE2uZjsHzFLwNcrijwui/RWU8q6dCy2eLALcDE6imY2RYddCtRpbhoGRKfmogdmF/SFOUhuAs4j6gxNQNwsO2nWtTkj6VhsZI0gaT9JW1B+HM/S7junGb7P8B/iTTB69h+ueEG1cKmN5VyTT+wfYOkzYiObjrgCOAQSXOV+34yYKpy7I+2ss19peHiUa7lCpI2IuqhLSjpJmBzYApgE+Abti8FNrX9bF1HP8rz8CyR3OJl4kX1z9VtGoJne6Tt520Ptv3WWF4akw4l9bFv1EUjUx97SI1MjYTUyGbRNglkFEHaZxFf7W8RN/XRxE19rSJt9nxEEdifA8M/5l91HRXrx+FE4eFtFEWHVwHWlPRb2+9IuoGotfKfdnURKC89sxBWzeuJgsk/V/jATwPMKWlnYFNgO5daMI19W9DkplLp7EcVS+cI4E5C6LcDTiJbPbrnAAARqUlEQVTSgl8r6WLC7WWPOhx7Q6wU8U5bAvvYHinpe0RmtEfL+ukJKzjF2ltLGhZcSRMADwBLA0/bfkPSBOXcfGzcT7s+48m4k/rYN+qikd2uj5AaCamRDVIjm0dLLUW9vsq/QBTO/SPwV2AHYC5gdmBFYCfC/3lDYB7gP+O3te2Nop7MAkTsCER68QeJVMLfA7D9iu0rHfVn2tlKuALh3nAGsChwge33iNTY/yRiPA5yT1HQdj6WcaLS2W9K1L1Z2PZjwDXA9ISV71eEENwNrG/76la1txn0ct+anwjsXhz4U+Ml1/ajkhZR1PqaCfh1i5o73igiNysR9zOKELqlJO1ne2TZrG0MeklzSX1sLjXSyK7VR0iNTI3sITWyeYz3kyRpELCv7YOKZafhvzwcWFvSGbbvAd5VFAedwvbbkt4D1gYWAb7V7V/0VWuHopDw5MAPbb+nCKR9T5E9axdgZUl32X6+sX+7WMk09oxWUxFB3tsDZ9s+VRHsP4/tEyrWoIaFsC2OpS9I+ibwoO2XFAWCdwTWsP1ysXrdSlhAt5C0I+EO9GELm9w0yrUcQLzcPgtMXu7d14EZgVUkrQO8C9xt+1CofTa0gcSxP0R5Bsr6TYCrJD0MLEa4BO3esgYnTSX1sXnUQSNTH3tIjUyNhNTI/mK8W4zKw7mLpCPK/P8kDbL9COHecoGkxSR9hSgMOaCy3/W2N6/bzT0uSBE4XB6GRSQtaHsE4S+9TVn3Xtl8OuBYIqHA82P9hy1ElYxWkvaU9MPi9nEZ4fd9QeMBB04k3HnUELePG/rvNBT1f2YvIjeAELSBwIqSDiTOx92EK8xdRKr4WoicIuBbRCKM3YBpiVpITwFn2F6PcI2bzfbtNRe5gY172vYoRxzESYQrIGX5I4Qr1HrAlwlXwaQmpD72nbpoZOpjD6mRqZGQGtmftKTOoKQZCP/efWwPKV/4crhm7E1YN+cFjrV98XhvYBtTsYpsChwIbGP7r4riw6sSrkQXSTqEcBta0/b7LWzyp6LIfLU2kRZ7KiIAfCbgV0TR0MWJ4rE7t6qN/UXF8t84D68C5xDXdiHgQiLr1Ynld3tdLL0Akia1/a4iJmoHIqX7GbZfLuu3Jax6PywjIrWk1yjGYCIL4F9s310swNPbXrmy/ZS23yzTtRP9bib1sW/UTSO7WR8hNTI1MkiN7F9aVnRe0nKEj/dqtm+rLF+ZsHi8YPudsqyWxVHHBUn7Viw+KxB1kda0PUzSZMTLwWzAccB9RIzJ922/26o2fxLF0jUBEf/ykO3tJE0DfJ+IgfkJcQyzEq5QV5f9avFQ9+rYZiLSmG9GWPsvsH19ZdtvEdd7A9sPtaK9/YGkRYBDgB1sP1f6hE2INO+XELE8BxEi90TrWjp+KC/9lxEJQl4kEkI8aPsoSbcC/7L9o1771C5NeJL6+Hmok0Z2uz5CaiSkRvYmNbL/aFlgpe3bytf9pZJmsT1C0qVE7aS1qx10Ch0Q1rAGzwE3EO5Ew4mMaI8StVOWASax/Qy0lzhIWsAl21W5piMk3QzsLOkntl+T9FfiAT8e2N32rZX9a1MotSJy6xAxK/vaPlNRMPr7kt4H7gG2KOu37nSRG8u9+BrxYnugpB1Kn7AAsDfhBnQRsFwZEaldh14ZwWi89MwJjLT9w7J+WWDT8kLwXeBxSUcDj1ZcZWp1TpIg9fFz0dEamfo4JqmRQGpkauR4oqVZpmyfSaTLflTSLcBw298uQ+JZKLeCI0j8TEmnFwvQ/UTmrBuAHxOFZydzZENrK5EDUIl1KB356OxYtvcFrgWuKvOPA38B7gXG8Pmv20NdOrL9ibTXd5TFxxCFctcEFiTOywqV9R1J9V6UtKmkr9t+DjiSiP34Zdn0CuAl4FXb79Rd5Mpso5D468ACklYv83cRSS+Wtv0GMIftf+XLf3eQ+jhudLJGpj6OndTI1MgymxrZz7Q85bDtvQjrzt2Vr/1B3X4xJX1T0uGSjlb4hAP8FFhd0pa2z7a9he2biPiBtYEx4h7aQeQqTAc8Z7thvR39MmN7fQBJZ5b5e4CjHcHitXnp0UdTfM8GPGz7HgUTlmt2NFEQeR7bz1TOWcfiyIw4k6TriAxfu5SXtmeJ2JcVJf2eqJ11hqNYbmPfuoncgIrVewfgXEl7EG5shxCZ4RZ3JEAYRbiLQXm+6/RMJJ9M6uPHUzON7Hp9hNTI1MggNXL80/KPQQDb37f9ExhtHalFFqjPi6TtiEDoYYSbwO6SDiXSBq8JHC9pxbLt9kSWuU1cKTDbDkgapMj0BdH2atruUZIWljRUUSdnDWBjSauW9bXKiKaedN+DFHVxIK7vCEnzOxghaUlgSmB725e0rsV9R9L8kuYp0wOBfYj6WAcDUxPX+xjbDxCFga8Hfmz7nLJPLTt09yRD2AVYDTiPcH/ZlEgQ8ShwiaQrgC8S9cRGv7jW5ZlIPhupjx+lDhqZ+jgmqZGpkQ1SI8c/LUsgMzbqONQ9rkg6mEgb/QPbT5ZlixGpg/9u+2hJmwNHEVnE/ge8U1yH2u78SXoV+A3h2rCj7S3K8mmJIf5DbA9pLKuDhe/jUPj6n0V0ZK8QRY83ItyX7iybHU5kEfxrSxrZJCTtBGxMHOOlth+WNB1h8b6UsOzeQYj9ju5Jkd7Yv9ZJMSStRmTE29L2VZJmB1YnMqIdVJ75CW3/vWzfds92Mn7JeyCok0amPo5JamRqZIPUyPFLW30MdjvFMnQp8IDtfYvLhG1b4SN9FFFk9SlJpxA+4/uVfdvyQZA0I5G57TngQUL0Hgf+TTzUT5TtqrWh2iKOoxk0OuzSkZ1CWPxGEnEsywATAt8AvglMDBxaA5E7EVgY2BoYZntkZd2XgJ/a3kzSpERGtFGENb927i4NKvfBIEeMxzREfaRpgVWLRXwVQvTX6rVvbZ6HJOkLddPIbtdHSI0kNRJIjWw1+THYJlRcJGYjhryvIPzC3688JNcBV9k+rrWtHTcUBZJvB54mhHwlYqj/NeCfwE/cUwS4FigSArxF6PdLkmYmCsU+ThSOPcn2eZImLtd4cuB/bsM05+NCseweSaT4/qCyfDfCxeVVwr3r78BawOmORBm1pGq9VZQF2IxwCTscmAzYHpjY9jZlNGNjYEPgjTpbfZNkXKmrRnajPkJqJKmRQGpku5Afg21ERey+BhxGpI++utFhSDoLONX2nb33aU2LPzuSNiCsPI006bMRpU3etP1aa1vXXCSdS1izpiACm/clMtudCswF7GT7DkmzEBnC9rH9Yqva20yKa8d2tteWNIhw0ToNWLZMLwcsCixPCPuRZb9aW/YkfYMQt4OAdQnL92XAy8DviBeAp4n06bWvF5Ukn4e6amQ36SOkRpIa+RFSI1tLWySQ6VYarh8NisjJkSL5N8BOwCJl28HAQoT/+Bj7jKfm9gnbQ4GzgYfL/DO2n3TUThrY0sY1CUnTSvojUQdnDcKCNYToyGYgUoI/Aswi6ctEjaBX6iJyhYeAGSQt4Uh0MRVwu+2FgMuBg23favvwisjVqj4WjJkVT9LywB5ERsirbG9NiNoajniHHYEngUtsP1FeEJKk6+kWjewGfYTUyEJqJKmR7Uae0BZRtVZK2h140vYfGsPetocWH/p9JY0iLGhr2X6hda3uG7b3lLSEpB/a/m1lecd3cqVju45wXVinLB5m+7RyHS+0vaik94jYh42A822f0qIm9xcvAH8C1pf0mu2nCbGHcP14s9e9r054WRtXKse3EuECdj+wqKSvOFLDHwPcV1ylbiFSyw+WdJ9L4ekk6Wa6TSPrrI+QGlkhNZLUyHYj3URbTBG5JYA9bT9fllV9qH8FTGJ7uzJfa1eBTkbSCkTMxxq275Q0UcV96QbgTNvnl/kpbL/Vwub2G5IWBLYFpiEyoj0LHEF05lvZfr2FzRtvSFqayIy3M5EkYj9gBHABUTD6fOD7tp9WBMvPX3VvS5IkNbJOpEYGqZFBamT7kB+DLaRYPP5OBMHvriioOqKs+0icQ51ErhPiOD4PkrYm/N4Xtf2CpEkdKc0vBi4u7kC1R9JchGvH1wif/7dtb1bW1fLaN2g8x5ImJmIffgRsQ8TG/BKYBXgHOM721a1raZK0N92qkXXuI1Mjg9TI1Mh2Ij8GxxON2AfbljRhmRwpaTMiEP7bjjozYxWzuncOdULSkcB3SwxAY9llRMd2Y8sa1gIkTQFMZPvlMl+Ll7UqkjYBngD+AcxHJANYxfbbkiYjRG4l4IfAl4k4mQca7k/VUY4k6VZSI7uH1MgeUiNTI9uBTCAznnBB0prAUOAMSQvZPhc4tsxPbXvU2ALGU+Q6B9t7AY9JGirpC5KuJ2Ijbmxx08Y7xc3nFahtEPxxRBr09wBsPwi8CJxe5t8BrgVmBIbYvh24B/iqIo02KXJJkhrZTaRG9pAamRrZDuTH4HhE0jaET/RgYjj895LmtX0McBdwNdQnYLybsb02sBjh6vAH2zu1uEkto9GR1+1lTdL+wKy2l7L9z0bsiyM5wvSSjimbvkZYRBuxDhcRcTPXj+82J0k7kxrZPaRG9pAamRrZatJNtB/p7bYiaWPgVmAFInj4WaKmzipETZWdgKPTClIPJE0JLGn7hla3JWk+ko4FHrZ9hqRlgJmBxYlMcfcDfybSiK9OpAs/tWWNTZI2JDWyu0mNrDepkZ1Dfgz2E+opjjsAmN32U2X5vET2pFVsD5f0KnAVsIXHTCWcFyZJ2hhJGwKbE5btWYGngEmJkj0nEIkvFgQmbLzs5LOdJEFqZJLUm9TIziHrDPYTReTmIoqpPlRiHH5JpM19EZhP0qSEyF1etY7mg5AkHcGVRBzEesA+wH9sD5O0HzC37WuB2xobp8glSQ+pkUlSe1IjO4QcGewHSla06YBTgV8TWZQeIFLo3ke4unyZ8JffxPZtH/OvkiTpMCT9EbjZ9nGtbkuStCOpkUnSvaRGth/5MdgkeqcDLulyBxPD4jsQxVTPLC4xEwGzA+/YfrYV7U2SpHmU531u4EjgOdtbtbhJSdJWpEYmSfeSGtneZDbRJlBNByxp5lI3ZhTwJWBPYN8ictMA5wIz2X7U9rNF+JIk6Wy+CGwF3NYQubGlv0+SbiQ1Mkm6ntTINiZHBvuIpEG2PyzTlxHBse8COxNuLlsQKXIfA44A/ml7t9a0NkmS/kLSFKVmVBbATpJCamSSJJAa2c7kx+DnpFgwR9h+R9JUwMaEW8tBwK7A2sBywKrAysAMhEXkxLJ/BsomSQ3JZztJUiOTJBk7+Wy3H/kx+DmQNAORLvd8289J+hkhdAfavrBscyowQymsiqTJbb9dptMikiRJktSS1MgkSZLOIT8GPyclGHZGYB7gL8BpwBtE4cw3S7a024Fbbe/ZsISkRSRJkiSpO6mRSZIknUF+DI4jkiawPbIEte8NzAmcDDwD/Ba4HDirFMudHBhle3jLGpwkSZIk44nUyCRJks4is3R9BiQNknQAQBG5gcWFZQjwX+CHwMTA7mV6nbLN20XwMmNSkiRJUktSI5MkSTqX/Bj8DJRMaLtIOqLMjyoxDS8ClwD/I1LmPknUUBlVradUnU6SJEmSOpEamSRJ0rmkm+hnpATEPwDsY3tIsWTK9oeSlgS2AR6zfVxLG5okSZIk45nUyCRJks5kUKsb0CnYflHSOsA1kh61fVtl9TTAP4ArW9O6JEmSJGkdqZFJkiSdSY4MjiOStgYOB2axPaIU0Z0QWK/EPmQmtCRJkqQrSY1MkiTpLHJkcByxfaakBYBHJQ0DnrG9FmQhzSRJkqS7SY1MkiTpLHJk8HMi6ffAf23/pMwPKkH0SZIkSdLVpEYmSZJ0Bvkx2ARKiuzMhpYkSZIkvUiNTJIkaV+ytEQfKemzU+SSJEmSpBepkUmSJO1NjgwmSZIkSZIkSZJ0ITkymCRJkiRJkiRJ0oXkx2CSJEmSJEmSJEkXkh+DSZIkSZIkSZIkXUh+DCZJkiRJkiRJknQh+TGYJEmSJEmSJEnSheTHYJIkSZIkSZIkSRfy/5ib7VEnmItzAAAAAElFTkSuQmCC\n","text/plain":["<Figure size 1080x720 with 2 Axes>"]},"metadata":{"needs_background":"light"}}]},{"cell_type":"markdown","metadata":{"id":"XDeFa8mQSM_X"},"source":["# 3) Overfitting and Underfitting"]},{"cell_type":"markdown","metadata":{"id":"kdQxGDfuSM_a"},"source":["- **Overfitting**:  \n","A model suffers from Overfitting when it has learned too much from the training data, and does not perform well in practice as a result. This is usually caused by the model having too much exposure to the training data. For the number classification example, if the model is overfit in this way, it may be picking up on tiny details that are misleading, like stray marks as an indication of a specific number\n","\n","- **Underfitting**:  \n","A model suffers from Underfitting when it has not learned enough from the training data, and does not perform well in practice as a result. As a direct contrast to the previous idea, this issue is caused by not letting the model learn enough from training data. In the number classification example, if the training set is too small or the model has not had enough attempts to learn from it, then it will not be able to pick out key features of the numbers. (4)\n","\n","<a href=\"https://arxiv.org/pdf/1811.12808.pdf\">\n","  <img src=\"images/over_and_under_fitting.png\" alt=\"over_under_fitting\" class=\"center\"  height=\"400\" width=\"1000\" >\n","</a>\n","\n","*How to detect overfitting?*  \n","A key challenge with overfitting, and with machine learning in general, is that we **can’t know how well our model will perform on new data until we actually test it.** (5)\n","\n","To address this, we can split our initial dataset into separate training and test subsets. This method can approximate of how well our model will perform on new data. If our model does much better on the training set than on the test set, then we’re likely overfitting.\n","For example, it would be a big red flag if our model saw 99% accuracy on the training set but only 55% accuracy on the test set.\n","\n","*How to prevent overfitting?*  \n","- **Cross-validation**  \n","The idea is clever: Use your initial training data to generate multiple mini train-test splits. Use these splits to tune your model.\n","In standard k-fold cross-validation, we partition the data into k subsets, called folds. Then, we iteratively train the algorithm on k-1 folds while using the remaining fold as the test set (called the “holdout fold”). Cross-validation allows you to tune hyperparameters with only your original training set. This allows you to keep your test set as a truly unseen dataset for selecting your final model.\n","- **Train with more data**  \n","It won’t work every time, but training with more data can help algorithms detect the signal better. Of course, that’s not always the case. If we just add more noisy data, this technique won’t help. That’s why you should always ensure your data is clean and relevant.\n","- **Remove features**  \n","Some algorithms have built-in feature selection. For those that don’t, you can manually improve their generalizability by removing irrelevant input features.\n","- **Regularization**  \n","Regularization refers to a broad range of techniques for artificially forcing your model to be simpler. The method will depend on the type of learner you’re using. For example, you could prune a decision tree, use dropout on a neural network, or add a penalty parameter to the cost function in regression.\n","- **Also, Ensembling, early stopping ...**"]},{"cell_type":"markdown","metadata":{"id":"F9falgMUSM_b"},"source":["# 4) Other hyperparameters optimization"]},{"cell_type":"markdown","metadata":{"id":"-zRJ5ypiSM_d"},"source":["Wikipedia states that \"hyperparameter tuning/optimizization is choosing a set of optimal hyperparameters for a a learning algorithm\". \n","So what is a hyperparameter? [3]\n","<div align=\"center\">\n","    <i> a hyperparameter is a parameter whose value is set before the learning process begins </i>\n","</div>\n","\n","Some examples of hyperparameters include penalty in logistic regression \n","In sklearn, hyperparameters are passed in as arguments of the constructor of the models classes.\n","   \n","**Tuning Strategies:**\n"," - Grid Search  \n"," Also known as an exhaustive search, Grid search looks through each combination of hyperparameters. This means that every combination of specified hyperparameter values will be tried.\n"," - Random Search  \n"," As its names suggests, Random Search uses random combinations of hyperparameters. This means that not all of the parameter values are tried, and instead, parameters will be sampled with fixed numbers of iterations.\n","\n","<a href=\"http://www.jmlr.org/papers/volume13/bergstra12a/bergstra12a.pdf\">\n","  <img src=\"images/random_grid_search.png\" alt=\"random_grid_search class=\"center\"  height=\"400\" width=\"700\" >\n","</a>"]},{"cell_type":"markdown","metadata":{"id":"nLSSI_hFSM_f"},"source":["### Grid Search"]},{"cell_type":"markdown","metadata":{"id":"X8BEr4BvSM_h"},"source":["We define a space for the Grid Search, we will work with z-score preprocessing in the pipeline"]},{"cell_type":"code","metadata":{"id":"yL7AzvrTSM_j","executionInfo":{"status":"ok","timestamp":1636139388563,"user_tz":-60,"elapsed":357,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["\"\"\"\n","Create a dictionary with classifier name as a key and it's hyper parameters options as a value for grid search\n","\"\"\"\n","\n","import numpy as np\n","\n","# Logistic Regression Params\n","C = [x for x in np.arange(0.1, 3, 0.2)]\n","penalty = [\"l1\", \"l2\"]\n","fit_intercept = [True, False]\n","solver = [\"saga\"]\n","lr_params = {'C': C,\n","             'penalty': penalty,\n","             'fit_intercept': fit_intercept,\n","             'solver': solver\n","             }\n","\n","# DecisionTreeClassifier PARAMS\n","criterion = ['gini', 'entropy']\n","splitter = ['best', 'random']\n","class_weight = [None, \"balanced\"]\n","max_depth = [int(x) for x in np.linspace(10, 110, num=11)]\n","max_depth.append(None)\n","min_samples_split = [2, 5, 10]\n","min_samples_leaf = [1, 2, 4]\n","max_features = [\"auto\", \"sqrt\", \"log2\"]\n","dtc_params = {'criterion': criterion,\n","              'splitter': splitter,\n","              'class_weight': class_weight,\n","              'max_depth': max_depth,\n","              'min_samples_split': min_samples_split,\n","              'min_samples_leaf': min_samples_leaf,\n","              'max_features': max_features\n","              }\n","\n","# KNN PARAMS\n","n_neighbors = [int(x) for x in np.linspace(start=1, stop=20, num=2)]\n","weights = [\"uniform\", \"distance\"]\n","algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n","leaf_size = [int(x) for x in np.linspace(start=5, stop=50, num=2)]\n","p = [int(x) for x in np.linspace(start=1, stop=4, num=1)]\n","knn_params = {'n_neighbors': n_neighbors,\n","              'weights': weights,\n","              'algorithm': algorithm,\n","              'leaf_size': leaf_size,\n","              'p': p,\n","              }\n","\n","# LDA PARAMS\n","solver = [\"lsqr\"]\n","shrinkage = [\"auto\", None, 0.1, 0.3, 0.5, 0.7, 0.9]\n","lda_params = {'solver': solver,\n","              'shrinkage': shrinkage\n","              }\n","\n","# GaussianNB PARAMS\n","var_smoothing = [1e-9, 1e-8, 1e-7, 1e-6, 1e-5]\n","gnb_params = {'var_smoothing': var_smoothing,\n","              }\n","\n","# SVC PARAMS\n","C = [x for x in np.arange(0.1, 2, 0.2)]\n","gamma = [\"auto\"]\n","kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n","degree = [1, 2, 3, 4, 5, 6]\n","svc_params = {'C': C,\n","              'gamma': gamma,\n","              'kernel': kernel,\n","              'degree': degree,\n","              }\n","\n","hypertuned_params_gs = {\"Logistic Regression\": lr_params,\n","                     \"Decision Tree Classifier\": dtc_params,\n","                     \"K-nearest neighbours\": knn_params,\n","                     \"Linear Discriminant Analysis\": lda_params,\n","                     \"Gaussian Naive Bayes\": gnb_params,\n","                     \"Support Vector Classifier\": svc_params,\n","                     }\n"],"execution_count":38,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"PUf6YNEHSM_r"},"source":["**Exercice:** Complete the following script to:\n","- find the numbers of combination from hypertuned_params that Grid Search will evaluate for each models (\"LR\",\"DTC\", \"KNN\" \"LDA\", \"GNB\", \"SVC\")"]},{"cell_type":"code","metadata":{"id":"qTMdsTAfSM_t"},"source":["#@title Exercise 6 {display-mode: \"form\"}\n","# %load solutions_exercices/find_combination.py\n","\n","\n","def find_combination_from_grid(grid):\n","    com = 1\n","    for x in grid.values():\n","        com *= len('''CompleteHere''')\n","    return '''CompleteHere'''\n","\n","\n","for key, value in hypertuned_params_gs.items():\n","    nb_combination = find_combination_from_grid(value)\n","    print(\"%s: %i combinations \" % (key, nb_combination))\n"],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"XmncOCfcSM_8","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636139399559,"user_tz":-60,"elapsed":358,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"f275b39e-4e23-424e-a98d-4669361838c1"},"source":["#@title Solution 6 {display-mode: \"form\"}\n","# %load solutions_exercices/find_combination.py\n","\n","\n","def find_combination_from_grid(grid):\n","    com = 1\n","    for x in grid.values():\n","        com *= len(x)\n","    return com\n","\n","\n","for key, value in hypertuned_params_gs.items():\n","    nb_combination = find_combination_from_grid(value)\n","    print(\"%s: %i combinations \" % (key, nb_combination))\n","    "],"execution_count":39,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression: 60 combinations \n","Decision Tree Classifier: 2592 combinations \n","K-nearest neighbours: 32 combinations \n","Linear Discriminant Analysis: 7 combinations \n","Gaussian Naive Bayes: 5 combinations \n","Support Vector Classifier: 240 combinations \n"]}]},{"cell_type":"code","metadata":{"id":"gNDsw331SNAD","executionInfo":{"status":"ok","timestamp":1636139403583,"user_tz":-60,"elapsed":595,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["# grid search function\n","import numpy as np\n","import warnings\n","warnings.filterwarnings('ignore') \n","\n","def grid_search(X, y, name, model, param_grid, verbose=False):\n","    names = []\n","    names.append(name)\n","    result_gs, max_val_mean_test = [], 0 \n","    for i, params in enumerate(param_grid):\n","        model = model.set_params(**params)\n","        results_cv_train_w_std, results_cv_test_w_std = cross_validation_with_standadization(X=X, y=y, model=model, num_folds=5)\n","        mean_train, std_train = np.mean(results_cv_train_w_std), np.std(results_cv_train_w_std)\n","        mean_test, std_test = np.mean(results_cv_test_w_std), np.std(results_cv_test_w_std)\n","        if verbose:\n","            print(\"%s - iteration %i: %f (%f)\" % (name, i, mean_test, std_test))\n","        if mean_test > max_val_mean_test:\n","            max_val_mean_test = mean_test\n","            max_val_std_test = std_test\n","            max_val_mean_train = mean_train\n","            max_val_std_train = std_train\n","            max_i = i\n","            best_params = param_grid[i] \n","    msg = \"%s: Maximum value on test = %.3f (+/- %.3f) with train = %.3f (+/- %.3f) for iteration %i with params: %s\" % (name, max_val_mean_test, max_val_std_test, max_val_mean_train, max_val_std_train, max_i, best_params)\n","    print(msg)"],"execution_count":40,"outputs":[]},{"cell_type":"code","metadata":{"id":"XCjYOyxWSNAJ","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636139533787,"user_tz":-60,"elapsed":120583,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"6f651952-84fe-4bf5-bab1-526a460d7b2e"},"source":["from sklearn.model_selection import ParameterGrid\n","\n","for name, model in models:\n","    param_grid = list(ParameterGrid(hypertuned_params_gs[name]))\n","    grid_search(X=x_train, y=y_train, name=name, model=model, param_grid=param_grid, verbose=False)  # you can set verbose to True to see each iteration"],"execution_count":41,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression: Maximum value on test = 0.876 (+/- 0.057) with train = 0.951 (+/- 0.014) for iteration 16 with params: {'C': 0.9000000000000001, 'fit_intercept': True, 'penalty': 'l1', 'solver': 'saga'}\n","Linear Discriminant Analysis: Maximum value on test = 0.876 (+/- 0.060) with train = 0.901 (+/- 0.019) for iteration 6 with params: {'shrinkage': 0.9, 'solver': 'lsqr'}\n","K-nearest neighbours: Maximum value on test = 0.824 (+/- 0.105) with train = 0.851 (+/- 0.022) for iteration 2 with params: {'algorithm': 'auto', 'leaf_size': 5, 'n_neighbors': 20, 'p': 1, 'weights': 'uniform'}\n","Decision Tree Classifier: Maximum value on test = 0.865 (+/- 0.024) with train = 0.944 (+/- 0.012) for iteration 10 with params: {'class_weight': None, 'criterion': 'gini', 'max_depth': 10, 'max_features': 'auto', 'min_samples_leaf': 2, 'min_samples_split': 10, 'splitter': 'best'}\n","Gaussian Naive Bayes: Maximum value on test = 0.847 (+/- 0.034) with train = 0.876 (+/- 0.018) for iteration 0 with params: {'var_smoothing': 1e-09}\n","Support Vector Classifier: Maximum value on test = 0.888 (+/- 0.043) with train = 0.910 (+/- 0.020) for iteration 121 with params: {'C': 1.1000000000000003, 'degree': 1, 'gamma': 'auto', 'kernel': 'poly'}\n"]}]},{"cell_type":"markdown","metadata":{"id":"fRxWVXo0SNAS"},"source":["You see that we have improve the performance with the grid search hyperparameters tunings."]},{"cell_type":"markdown","metadata":{"id":"XyaTmT2aSNAT"},"source":["### Random Search"]},{"cell_type":"markdown","metadata":{"id":"3CNO78j2SNAX"},"source":["We define a space for the Random Search, we will works with z-score preprocessing in the pipeline"]},{"cell_type":"code","metadata":{"id":"ew_T4PGgSNAZ","executionInfo":{"status":"ok","timestamp":1636139537611,"user_tz":-60,"elapsed":357,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["\"\"\"\n","Create a dictionary with classifier name as a key and it's hyper parameters options as a value for Random search\n","\"\"\"\n","\n","import numpy as np\n","from scipy.stats import uniform\n","\n","# Logistic Regression Params\n","# Create regularization hyperparameter distribution using uniform distribution\n","C = uniform(loc=0, scale=4)\n","penalty = [\"l1\", \"l2\"]\n","fit_intercept = [True, False]\n","solver = [\"saga\"]\n","lr_params = {'C': C,\n","             'penalty': penalty,\n","             'fit_intercept': fit_intercept,\n","             'solver': solver\n","             }\n","\n","# DecisionTreeClassifier PARAMS\n","criterion = ['gini', 'entropy']\n","splitter = ['best', 'random']\n","class_weight = [None, \"balanced\"]\n","max_depth = list(range(10, 501))\n","max_depth.append(None)\n","min_samples_split = list(range(2, 101))\n","min_samples_leaf = list(range(1, 50))\n","max_features = [\"auto\", \"sqrt\", \"log2\"]\n","dtc_params = {'criterion': criterion,\n","              'splitter': splitter,\n","              'class_weight': class_weight,\n","              'max_depth': max_depth,\n","              'min_samples_split': min_samples_split,\n","              'min_samples_leaf': min_samples_leaf,\n","              'max_features': max_features\n","              }\n","\n","# KNN PARAMS\n","n_neighbors = list(range(1, 101))\n","weights = [\"uniform\", \"distance\"]\n","algorithm = [\"auto\", \"ball_tree\", \"kd_tree\", \"brute\"]\n","leaf_size = list(range(2, 101))\n","p = list(range(1, 11))\n","knn_params = {'n_neighbors': n_neighbors,\n","              'weights': weights,\n","              'algorithm': algorithm,\n","              'leaf_size': leaf_size,\n","              'p': p,\n","              }\n","\n","# LDA PARAMS\n","solver = [\"lsqr\"]\n","shrinkage = [\"auto\", None, 0.1, 0.3, 0.5, 0.7, 0.9]\n","lda_params = {'solver': solver,\n","              'shrinkage': shrinkage\n","              }\n","\n","# GaussianNB PARAMS\n","var_smoothing = uniform(loc=0, scale=0.1)\n","gnb_params = {'var_smoothing': var_smoothing,\n","              }\n","\n","# SVC PARAMS\n","C =  uniform(loc=0, scale=2)\n","gamma = [\"auto\"]\n","kernel = [\"linear\", \"poly\", \"rbf\", \"sigmoid\"]\n","degree = list(range(1,11))\n","svc_params = {'C': C,\n","              'gamma': gamma,\n","              'kernel': kernel,\n","              'degree': degree,\n","              }\n","\n","hypertuned_params_rs = {\"Logistic Regression\": lr_params,\n","                     \"Decision Tree Classifier\": dtc_params,\n","                     \"K-nearest neighbours\": knn_params,\n","                     \"Linear Discriminant Analysis\": lda_params,\n","                     \"Gaussian Naive Bayes\": gnb_params,\n","                     \"Support Vector Classifier\": svc_params,\n","                     }"],"execution_count":42,"outputs":[]},{"cell_type":"code","metadata":{"id":"11OdRu3WSNAg","executionInfo":{"status":"ok","timestamp":1636139537612,"user_tz":-60,"elapsed":7,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}}},"source":["# random search function\n","import random\n","import warnings\n","warnings.filterwarnings('ignore') \n","\n","def random_search(X, y, name, model, param_grid, nb_iterations, verbose=False):\n","    best_params = []\n","    names = []\n","    names.append(name)\n","    result_rs, max_val_mean_test = [], 0 \n","    for i in range(nb_iterations):\n","        # create random param from the grid dict\n","        params = {key: value.rvs() if isinstance(value, type(uniform())) else random.choice(value) for key, value in param_grid.items()}\n","        model = model.set_params(**params)\n","        results_cv_train_w_std, results_cv_test_w_std = cross_validation_with_standadization(X=X, y=y, model=model, num_folds=5)\n","        mean_train, std_train = np.mean(results_cv_train_w_std), np.std(results_cv_train_w_std)\n","        mean_test, std_test = np.mean(results_cv_test_w_std), np.std(results_cv_test_w_std)\n","        if verbose:\n","            print(\"%s - iteration %i: %f (%f)\" % (name, i, mean_test, std_test))\n","        if mean_test > max_val_mean_test:\n","            max_val_mean_test = mean_test\n","            max_val_std_test = std_test\n","            max_val_mean_train = mean_train\n","            max_val_std_train = std_train\n","            max_i = i\n","            best_params = params\n","    msg = \"%s: Maximum value on test = %.3f (+/- %.3f) with train = %.3f (+/- %.3f) for iteration %i with params: %s\" % (name, max_val_mean_test, max_val_std_test, max_val_mean_train, max_val_std_train, max_i, best_params)\n","    print(msg)"],"execution_count":43,"outputs":[]},{"cell_type":"code","metadata":{"id":"W_eGlkYCSNAm","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1636139897415,"user_tz":-60,"elapsed":359809,"user":{"displayName":"Rahimeh Rouhi","photoUrl":"https://lh3.googleusercontent.com/a-/AOh14GhymHo6F2su6_0hefT8nLzjN01z9ThxYIUOeUQCrg=s64","userId":"09868697787543330984"}},"outputId":"014f9b04-0782-4bba-afcf-46d4763be757"},"source":["for name, model in models:\n","    dic_grid = hypertuned_params_rs[name]\n","    random_search(X=x_train, y=y_train, name=name, model=model, param_grid=dic_grid, nb_iterations=100, verbose=False)  # you can set verbose to True to see each iteration"],"execution_count":44,"outputs":[{"output_type":"stream","name":"stdout","text":["Logistic Regression: Maximum value on test = 0.876 (+/- 0.057) with train = 0.951 (+/- 0.014) for iteration 3 with params: {'C': 0.8683354519823312, 'penalty': 'l1', 'fit_intercept': True, 'solver': 'saga'}\n","Linear Discriminant Analysis: Maximum value on test = 0.876 (+/- 0.060) with train = 0.901 (+/- 0.019) for iteration 0 with params: {'solver': 'lsqr', 'shrinkage': 0.9}\n","K-nearest neighbours: Maximum value on test = 0.847 (+/- 0.022) with train = 0.869 (+/- 0.016) for iteration 20 with params: {'n_neighbors': 8, 'weights': 'uniform', 'algorithm': 'auto', 'leaf_size': 27, 'p': 5}\n","Decision Tree Classifier: Maximum value on test = 0.876 (+/- 0.047) with train = 0.876 (+/- 0.012) for iteration 9 with params: {'criterion': 'entropy', 'splitter': 'best', 'class_weight': 'balanced', 'max_depth': 426, 'min_samples_split': 22, 'min_samples_leaf': 41, 'max_features': 'auto'}\n","Gaussian Naive Bayes: Maximum value on test = 0.847 (+/- 0.034) with train = 0.876 (+/- 0.017) for iteration 1 with params: {'var_smoothing': 0.014011186228611174}\n","Support Vector Classifier: Maximum value on test = 0.882 (+/- 0.049) with train = 0.904 (+/- 0.021) for iteration 23 with params: {'C': 0.8190266594469529, 'gamma': 'auto', 'kernel': 'poly', 'degree': 1}\n"]}]},{"cell_type":"markdown","metadata":{"id":"HOuy4k5wSNA1"},"source":["Here, we have implemented our own Grid Search and Random Search function to understand the mechanism. Sklearn directly implements the functions, [Random Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html) and [Grid Search](https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html)"]},{"cell_type":"markdown","metadata":{"id":"aFy8XnzzSNBP"},"source":["# 5) Ensembling"]},{"cell_type":"markdown","metadata":{"id":"ukGBPl70SNBR"},"source":["No free lunch theorem states that no machine learning algorithm is universally better than the others in all domains. The goal of ensembling is to combine multiple learner to improve the applicability and get better performance. But always remember, if two models have comparable performance, then you should usually pick the simpler one. [Occam's razor](https://simple.wikipedia.org/wiki/Occam%27s_razor)"]},{"cell_type":"markdown","metadata":{"id":"lG8LeKCESNBU"},"source":["### Voting"]},{"cell_type":"markdown","metadata":{"id":"WpuYrDetSNBX"},"source":["Voting is arguably the most straightforward way to combine multiple learners $d^{(j)}(\\cdot)$. The idea is to take a linear combination of the predictions made by the learners. For example, in multiclass classification, we have\n","$$\\tilde{y}_k =\\sum_j^L w_j d^{(j)}_k(\\boldsymbol{x}), \\text{ where }w_j\\geq 0\\text{ and }\\sum_j w_j=1,$$<p>for any class $k$, where $L$ is the number of voters. This can be simplified to the <strong>plurarity vote</strong> where each voter has the same weight:</p>\n","$$\\tilde{y}_k =\\sum_j \\frac{1}{L} d^{(j)}_k(\\boldsymbol{x}).$$<p>Let's use the <code>VotingClassifier</code> from Scikit-learn to combine <code>KNeighborsClassifer</code> ,<code>DecisionTreeClassifier</code> and <code>SVC</code>  together and train on our datasets</p>\n","\n","We will use the Sklearn <code>Pipeline</code> tools which allow to combine all the steps we've seen previously\n"]},{"cell_type":"code","metadata":{"id":"LICaKWRcSNBY"},"source":["from sklearn.pipeline import Pipeline\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.discriminant_analysis import LinearDiscriminantAnalysis\n","from sklearn.naive_bayes import GaussianNB\n","from sklearn.svm import SVC\n","\n","pipe1 = Pipeline([['sc', StandardScaler()], ['clf', LogisticRegression(**{'C': 0.1, 'fit_intercept': True, 'penalty': 'l1', 'solver': 'saga'}, random_state=random_state)]])\n","pipe2 = Pipeline([['clf', DecisionTreeClassifier(**{'class_weight': None, 'criterion': 'gini', 'max_depth': 80, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 10, 'splitter': 'best'}, random_state=random_state)]])\n","pipe3 = Pipeline([['sc', StandardScaler()], ['clf', KNeighborsClassifier(**{'algorithm': 'auto', 'leaf_size': 5, 'n_neighbors': 20, 'p': 1, 'weights': 'distance'})]])\n","pipe4 = Pipeline([['sc', StandardScaler()], ['clf', LinearDiscriminantAnalysis(**{'shrinkage': 0.7, 'solver': 'lsqr'})]])\n","pipe5 = Pipeline([['sc', StandardScaler()], ['clf', GaussianNB(**{'var_smoothing': 1e-09})]])\n","pipe6 = Pipeline([['sc', StandardScaler()], ['clf', SVC(**{'C': 0.5000000000000001, 'degree': 1, 'gamma': 'auto', 'kernel': 'sigmoid', 'probability': True}, random_state=random_state)]])"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"0ARQMF5DSNBe"},"source":["We can estimate the performance of individual classifiers via the 5-fold CV:"]},{"cell_type":"code","metadata":{"id":"GPDX2-OASNBj"},"source":["from sklearn.model_selection import cross_validate\n","\n","clf_labels = ['LR', 'DTC', 'KNN', 'LDA', 'GNB', 'SVC']\n","print('[Individual]')\n","for pipe, label in zip([pipe1, pipe2, pipe3, pipe4, pipe5, pipe6], clf_labels):\n","    results = cross_validate(estimator=pipe, X=x_train, y=y_train, cv=5, scoring='accuracy', return_train_score=True)\n","    scores_test = results['test_score']\n","    scores_train = results['train_score']\n","    print('%s: train = %.3f (+/- %.3f), test = %.3f (+/- %.3f)' % (label, scores_train.mean(), scores_train.std(), scores_test.mean(), scores_test.std()))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"VJotcstcSNBo"},"source":["Let's combined the classifiers by <code>VotingClassifer</code> from Scikit-learn and experiment some weight combinations:"]},{"cell_type":"code","metadata":{"id":"Zlu_2z9WSNBp"},"source":["from sklearn.ensemble import VotingClassifier\n","import itertools\n","\n","print('[Voting]')\n","best_vt, best_w, best_test_score, best_train_score = None, (), -1, 1\n","for a, b, c in list(itertools.permutations(range(0,3))): # try some weight combination\n","    clf = VotingClassifier(estimators=[('dtc', pipe2), ('knn', pipe3), ('svc', pipe6)], \n","                           voting='soft', weights=[a,b,c])\n","    results = cross_validate(estimator=clf, X=x_train, y=y_train, cv=5, scoring='accuracy', return_train_score=True)\n","    scores_test = results['test_score']\n","    scores_train = results['train_score']\n","    print('%s: train = %.3f (+/- %.3f), test = %.3f (+/- %.3f)' % ((a,b,c), scores_train.mean(), scores_train.std(), scores_test.mean(), scores_test.std()))\n","    if best_test_score < scores_test.mean() and best_train_score > scores_train.mean():\n","        best_vt, best_w, best_test_score = clf, (a, b, c), scores_test.mean()\n","\n","print('\\nBest %s: %.3f' % (best_w, best_test_score))"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"gNd7Zji6SNBu"},"source":["The best ensemble combines the <code>DecisionTreeClassifier</code> and <code>SVC</code>. This is a reasonable choice because these two models \"complement\" each other in design."]},{"cell_type":"markdown","metadata":{"id":"gabgxFJFSNBv"},"source":["# 6) Final prediction and Evaluation metrics"]},{"cell_type":"markdown","metadata":{"id":"UIoe_ZlYSNBw"},"source":["This is the moment you've been waiting for! We will finally be able to evaluate our test set. Our final model will be an ensemble and combines the <code>DecisionTreeClassifier</code> and <code>SVC</code> with a z-score features preprocessing. A decision tree does not need a z-score preprocessing because by nature the algorithm is scale invariant. Let's fit the model on all the training data."]},{"cell_type":"code","metadata":{"id":"KannU_dNSNBx"},"source":["from sklearn.ensemble import VotingClassifier\n","from sklearn.pipeline import Pipeline\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.svm import SVC\n","from sklearn.metrics import roc_curve, confusion_matrix, auc\n","\n","pipe2 = Pipeline([['clf', DecisionTreeClassifier(**{'class_weight': None, 'criterion': 'gini', 'max_depth': 80, 'max_features': 'log2', 'min_samples_leaf': 1, 'min_samples_split': 10, 'splitter': 'best'}, random_state=0)]])\n","pipe6 = Pipeline([['sc', StandardScaler()], ['clf', SVC(**{'C': 0.5000000000000001, 'degree': 1, 'gamma': 'auto', 'kernel': 'sigmoid', 'probability': True})]])\n","\n","clf = VotingClassifier(estimators=[('dtc', pipe2), ('svc', pipe6)], voting='soft', weights=[1,2])\n","clf.fit(x_train, y_train)\n","classes = clf.classes_\n","\n","# Here we have the probabilty associate to each classes\n","proba_test = clf.predict_proba(x_test)[:,1] # [:,1] referes to the second class HGG\n","y_pred = np.where(proba_test>0.5, 1, 0) # Here we have the prediction\n","\n","\n","# 1 -- Confusion matrix\n","cm = confusion_matrix(y_test, y_pred)\n","\n","# 2 -- ROC curve \n","fp_rates, tp_rates, _ = roc_curve(y_test, proba_test, pos_label=1)\n","roc_auc = auc(fp_rates, tp_rates)\n","\n","tn, fp, fn, tp = [i for i in cm.ravel()]\n","\n","# 3 -- Calculate each metrics\n","precision = tp / (tp + fp)\n","recall = tp / (tp + fn)\n","F1 = 2 * (precision * recall) / (precision + recall)\n","accuracy = (tn + tp) / (tn + fp + fn + tp)\n","\n","printout = (\n","        f'Precision: {round(precision, 3)} | '\n","        f'Recall: {round(recall, 3)} | '\n","        f'F1 Score: {round(F1, 3)} | '\n","        f'Accuracy Score: {round(accuracy, 3)} | '\n","        f'ROC auc: {round(roc_auc, 3)} | '\n","\n","    )\n","print(printout)"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Un6M8dcISNB4"},"source":["Now let's go plot the confusion matrix ! "]},{"cell_type":"code","metadata":{"id":"tJlo201HSNB5"},"source":["import matplotlib.pyplot as plt\n","\n","\n","# 1 -- Confusion matrix\n","# https://stackoverflow.com/questions/48817300/sklearn-plot-confusion-matrix-combined-across-trainingtest-sets\n","def plot_confusion_matrix(cm, classes,\n","                          normalize=False,\n","                          title='Confusion matrix',\n","                          cmap=plt.cm.Blues):\n","    \"\"\"\n","    This function prints and plots the confusion matrix.\n","    Normalization can be applied by setting `normalize=True`.\n","    \"\"\"\n","    if normalize:\n","        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n","        print(\"Normalized confusion matrix\")\n","    else:\n","        print('Confusion matrix, without normalization')\n","\n","    print(cm)\n","\n","    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n","    plt.title(title)\n","    tick_marks = np.arange(len(classes))\n","    plt.xticks(tick_marks, classes, rotation=45)\n","    plt.yticks(tick_marks, classes)\n","\n","    fmt = '.2f' if normalize else 'd'\n","    thresh = cm.max() / 2.\n","    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n","        plt.text(j, i, format(cm[i, j], fmt),\n","                 horizontalalignment=\"center\", verticalalignment=\"center\",\n","                 color=\"white\" if cm[i, j] > thresh else \"black\")\n","\n","    plt.tight_layout()\n","    plt.ylabel('True label')\n","    plt.xlabel('Predicted label')\n","    \n","plt.figure(figsize=(10,5))\n","plot_confusion_matrix(cm, classes=['HGG', 'LGG'],\n","                      title='Confusion matrix')\n","\n","# display of predicted value is not center in the middle of confusion matrix. This is actually a bug of the last version of matplotlib"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"4B9nnED_SNB8"},"source":["Now lets plot the roc curve !"]},{"cell_type":"code","metadata":{"id":"NMp0kS25SNB-"},"source":["plt.figure(figsize=(10,5))\n","plt.plot(fp_rates, tp_rates, color='green',\n","             lw=1, label='ROC curve (area = %0.2f)' % roc_auc)\n","plt.plot([0, 1], [0, 1], lw=1, linestyle='--', color='grey')\n","plt.xlim([0.0, 1.0])\n","plt.ylim([0.0, 1.05])\n","plt.xlabel('False Positive Rate', size=13)\n","plt.ylabel('True Positive Rate', size=13)\n","plt.title('ROC Curve', size=15)\n","plt.legend(loc=\"lower right\")\n","plt.subplots_adjust(wspace=.3)\n","plt.tight_layout()\n","plt.show()"],"execution_count":null,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"f3ExnZBKSNCC"},"source":["Distribution of predicted probabilty for each class "]},{"cell_type":"code","metadata":{"id":"5BSCoEchSNCD"},"source":["df = pd.DataFrame({'probPos': proba_test, 'target': y_test})\n","plt.figure(figsize=(10,5))\n","plt.hist(df[df.target == 0].probPos, density=True, bins=25,\n","             alpha=.5, color='green', label='LGG')\n","plt.hist(df[df.target == 1].probPos, density=True, bins=25,\n","             alpha=.5, color='red', label='HGG')\n","plt.axvline(.5, color='blue', linestyle='--', label='Boundary')\n","plt.xlim([0, 1])\n","plt.title('Distributions of Predictions', size=15)\n","plt.xlabel('Positive Probability (predicted)', size=13)\n","plt.ylabel('Samples (normalized scale)', size=13)\n","plt.legend(loc=\"upper right\")\n","plt.show();"],"execution_count":null,"outputs":[]}]}